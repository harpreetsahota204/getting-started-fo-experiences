{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Keypoint Estimation: Loading Keypoint Data\n",
    "\n",
    "\n",
    "\n",
    "### Loading Keypoint Data in Common Format\n",
    "\n",
    "Start by downloading the dataset from [this website](https://zenodo.org/records/10057090). Alternatively, you can use your favorite method for programmatically downloading the dataset, for example by running the following command in your terminal (Note: This is a 6GB download):\n",
    "\n",
    "```bash\n",
    "wget https://zenodo.org/records/10057090/files/tampar.zip?download=1\n",
    "```\n",
    "\n",
    "\n",
    "This dataset is in [COCO format](https://beta-docs.voxel51.com/fiftyone_concepts/dataset_creation/datasets/#cocodetectiondataset) and we can [automatically load it FiftyOne format](https://beta-docs.voxel51.com/fiftyone_concepts/dataset_creation/#common-formats). For this we will need to install `pycocotools`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates how to load TAMPAR dataset using FiftyOne's [data loaders for common formats](https://beta-docs.voxel51.com/fiftyone_concepts/dataset_creation/datasets/#supported-import-formats). Here's what's happening:\n",
    "\n",
    "\n",
    "- **[`fo.Dataset.from_dir()`](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#from_dir)**: This method imports the TAMPAR dataset in COCO format directly from disk. It accepts several important parameters:\n",
    "  - `dataset_type=fo.types.COCODetectionDataset`: Specifies we're loading data in [COCO object detection format](https://beta-docs.voxel51.com/api/fiftyone.types.dataset_types.COCODetectionDataset.html)\n",
    "  - `data_path` and `labels_path`: Point to the directories containing images and annotation JSON file\n",
    "  - `include_id=True`: Preserves the original COCO IDs\n",
    "  - `name=\"TAMPAR\"`: Gives our dataset a [descriptive name](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#name) in FiftyOne\n",
    "  - `persistent=True`: [Persists](https://beta-docs.voxel51.com/fiftyone_concepts/using_datasets/#dataset-persistence) the dataset to disk for future sessions\n",
    "\n",
    "- **[`compute_metadata()`](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#compute_metadata)**: After loading, this method analyzes all images to extract and store metadata like dimensions, file sizes, and other properties that enhance the dataset's functionality within FiftyOne.\n",
    "\n",
    "\n",
    "**Note:** If your annotations are in VOC format, you can use the [`VOCDetectionDataset`](https://beta-docs.voxel51.com/fiftyone_concepts/export_datasets/#vocdetectiondataset) type to import your dataset by passing [`fo.types.VOCDetectionDataset`](https://beta-docs.voxel51.com/api/fiftyone.types.html#fiftyone.types.VOCDetectionDataset) into the `dataset_type` argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "tampar_dataset = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    data_path=\"tampar\",\n",
    "    labels_path=\"tampar/tampar_test.json\",\n",
    "    include_id=True,\n",
    "    name=\"TAMPAR\",\n",
    "    persistent=True\n",
    ")\n",
    "\n",
    "tampar_dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keypoints are automatically parsed as [Keypoint labels](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) in the Dataset. \n",
    "\n",
    "Keypoints in FiftyOne represents a collection of coordinate points that mark specific locations in an image. These can be used for localizing important features like facial landmarks, human pose joints, or in our case, the corners of parcel boxes. Each [Keypoint label](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) contains:\n",
    "\n",
    "* A set of points as (x,y) coordinates normalized to [0,1] range\n",
    "* Optional confidence scores for each point\n",
    "* A semantic label describing what these points represent\n",
    "* Optional attributes for storing additional metadata\n",
    "\n",
    "This dataset has 8 keypoints (24 values total), all with `visibility=2` (which means \"visible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tampar_dataset.first()['keypoints']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Dataset is loaded, we can [launch the app](https://beta-docs.voxel51.com/getting_started/basic/application_tour/) and inspect it.\n",
    "\n",
    "```python\n",
    "fo.launch_app(tampar_dataset)\n",
    "```\n",
    "\n",
    "<img src=\"assets/tampar.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting Keypoint Edges in FiftyOne\n",
    "\n",
    "When combined with a [`KeypointSkeleton`](https://beta-docs.voxel51.com/api/fiftyone.core.odm.dataset.KeypointSkeleton.html), these individual points form a connected structure (like our box wireframe) that visually represents the spatial relationships between points.\n",
    "\n",
    "In our parcel detection dataset, we use keypoints to mark the 8 corners of each box, allowing us to reconstruct the 3D geometry of parcels from 2D images.\n",
    "\n",
    "## The Skeleton Structure\n",
    "\n",
    "All [Dataset](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset) instances have [`skeletons`](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#skeletons) and [`default_skeleton`](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#default_skeleton) properties that you can use to store keypoint skeletons for Keypoint field(s) of a dataset.\n",
    "\n",
    "To visualize these boxes correctly in FiftyOne, we define a skeleton that connects these keypoints with edges. Our [`KeypointSkeleton`](https://beta-docs.voxel51.com/api/fiftyone.core.odm.dataset.KeypointSkeleton.html) defines:\n",
    "\n",
    "1. **Labels**: Semantic names for each keypoint (e.g., \"front_top_right\", \"back_bottom_left\")\n",
    "2. **Edges**: Connections between pairs of keypoints that form the wireframe of the 3D box\n",
    "\n",
    "## Edge Connections\n",
    "\n",
    "The edges are organized to represent the physical structure of the keypoints:\n",
    "\n",
    "- **Front Face**: Connects the four front corners in a clockwise or counter-clockwise order\n",
    "\n",
    "- **Top Face**: Connects the four top corners (front-top-left → back-top-left → back-top-right → front-top-right)\n",
    "\n",
    "- **Right Side**: Connects the front and back edges on the right side of the box\n",
    "\n",
    "- **Additional Edges**: Completes the remaining connections to form a full box\n",
    "\n",
    "## Implementation\n",
    "\n",
    "In FiftyOne, we implement this by creating a [`KeypointSkeleton`](https://beta-docs.voxel51.com/api/fiftyone.core.odm.dataset.KeypointSkeleton.html) object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "labels = [\n",
    "    \"front_upper_right\",    # 0\n",
    "    \"front_lower_left\",     # 1\n",
    "    \"front_upper_left\",     # 2\n",
    "    \"front_lower_right\",    # 3\n",
    "    \"back_upper_right\",     # 4\n",
    "    \"back_upper_left\",      # 5 \n",
    "    \"back_upper_left\",      # 6 \n",
    "    \"back_lower_right\"      # 7\n",
    "]\n",
    "\n",
    "# Complete set of edges to create a full 3D box\n",
    "edges = [\n",
    "    # Front face - complete square\n",
    "    [2, 0],  # front upper-left to front upper-right\n",
    "    [0, 3],  # front upper-right to front lower-right\n",
    "    [3, 1],  # front lower-right to front lower-left\n",
    "    [1, 2],  # front lower-left to front upper-left\n",
    "    \n",
    "    # Top face\n",
    "    [2, 6],  # front upper-left to back upper-left\n",
    "    [6, 4],  # back upper-left to back upper-right\n",
    "    [4, 0],  # back upper-right to front upper-right\n",
    "    \n",
    "    # Right side face\n",
    "    [0, 4],  # front upper-right to back upper-right\n",
    "    [4, 7],  # back upper-right to back lower-right\n",
    "    [7, 3],  # back lower-right to front lower-right\n",
    "    \n",
    "    # Back face \n",
    "    [6, 4],  # back upper-left to back upper-right\n",
    "    [4, 7],  # back upper-right to back lower-right\n",
    "    \n",
    "    # Left side face \n",
    "    [2, 6],  # front upper-left to back upper-left\n",
    "    \n",
    "    # Needed for complete box\n",
    "    [1, 5],  # front lower-left to back lower-left\n",
    "    [5, 6],  # back lower-left to back upper-left\n",
    "    [5, 7]   # back lower-left to back lower-right\n",
    "]\n",
    "\n",
    "\n",
    "tampar_dataset.default_skeleton = fo.KeypointSkeleton(\n",
    "    labels=labels,\n",
    "    edges=edges\n",
    ")\n",
    "\n",
    "tampar_dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This skeleton definition ensures that when our 3D box keypoints are visualized in FiftyOne, the lines connecting them accurately represent the box's structure, making it easy to interpret the detection results at a glance.\n",
    "\n",
    "\n",
    "```python\n",
    "fo.launch_app(tampar_dataset)\n",
    "```\n",
    "\n",
    "<img src=\"assets/tampar-skeletons.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Format\n",
    "\n",
    "http://domedb.perception.cs.cmu.edu/handdb.html\n",
    "\n",
    "This code will demonstrate how to parse a Keypoints datset with a custom annotation format. The specifics of your dataset may be different, but the core will remain the same:\n",
    "\n",
    "- You will need to use the Keypoint lable type\n",
    "- You will need to normalize the points to the [0,1] range\n",
    "\n",
    "You can download the dataset from here: http://domedb.perception.cs.cmu.edu/panopticDB/hands/hand_labels.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import fiftyone as fo\n",
    "\n",
    "# Input data path for labeled hand images\n",
    "dataset_path = 'hand_labels/manual_test/'\n",
    "\n",
    "# Get all annotation JSON files\n",
    "annotation_files = glob.glob(os.path.join(dataset_path, \"*.json\"))\n",
    "\n",
    "# Create a FiftyOne dataset with overwrite option to ensure clean slate\n",
    "hand_dataset = fo.Dataset(\"hand_keypoints\", overwrite=True)\n",
    "\n",
    "# Create a list to collect all samples\n",
    "samples_to_add = []\n",
    "\n",
    "# Process each annotation file\n",
    "for annotation_file in annotation_files:\n",
    "    # Get corresponding image file\n",
    "    image_file = annotation_file.replace(\".json\", \".jpg\")\n",
    "    \n",
    "    # Load the annotation data\n",
    "    with open(annotation_file, 'r') as file_handle:\n",
    "        annotation_data = json.load(file_handle)\n",
    "    \n",
    "    # Create a sample for this image\n",
    "    sample = fo.Sample(filepath=image_file)\n",
    "\n",
    "    # Extract hand joint coordinates and hand type\n",
    "    joint_coordinates = np.array(annotation_data['hand_pts'])\n",
    "    is_left_hand = annotation_data['is_left']\n",
    "    \n",
    "    # Get image dimensions for normalization\n",
    "    image = cv2.imread(image_file)\n",
    "    image_height, image_width = image.shape[:2]\n",
    "    \n",
    "    # Create a list of all normalized keypoints for the hand\n",
    "    normalized_joint_points = []\n",
    "    \n",
    "    for joint_index in range(joint_coordinates.shape[0]):\n",
    "        # Normalize coordinates to [0, 1] range\n",
    "        normalized_x = joint_coordinates[joint_index, 0] / image_width\n",
    "        normalized_y = joint_coordinates[joint_index, 1] / image_height\n",
    "        \n",
    "        normalized_joint_points.append([normalized_x, normalized_y])\n",
    "    \n",
    "    # Determine field name and label based on hand type\n",
    "    hand_field_name = \"left_hand\" if is_left_hand else \"right_hand\"\n",
    "    hand_label = \"left hand\" if is_left_hand else \"right hand\"\n",
    "    \n",
    "    # Create a keypoint object containing all joint points\n",
    "    hand_keypoint = fo.Keypoint(\n",
    "        label=hand_label,\n",
    "        points=normalized_joint_points,\n",
    "        num_keypoints=len(normalized_joint_points),\n",
    "    )\n",
    "    \n",
    "    # Create keypoints collection and assign the skeleton\n",
    "    hand_keypoints_collection = fo.Keypoints(keypoints=[hand_keypoint])\n",
    "    \n",
    "    # Add keypoints to the sample\n",
    "    sample[hand_field_name] = hand_keypoints_collection\n",
    "    \n",
    "    # Add sample to our list instead of directly to dataset\n",
    "    samples_to_add.append(sample)\n",
    "\n",
    "# Add all samples to the dataset in a single batch operation\n",
    "hand_dataset.add_samples(samples_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keypoint labels for each joint in the hand\n",
    "hand_joint_labels = [\n",
    "    \"WRIST\", \"THUMB_CMC\", \"THUMB_MCP\", \"THUMB_IP\", \"THUMB_TIP\",\n",
    "    \"INDEX_MCP\", \"INDEX_PIP\", \"INDEX_DIP\", \"INDEX_TIP\",\n",
    "    \"MIDDLE_MCP\", \"MIDDLE_PIP\", \"MIDDLE_DIP\", \"MIDDLE_TIP\",\n",
    "    \"RING_MCP\", \"RING_PIP\", \"RING_DIP\", \"RING_TIP\",\n",
    "    \"PINKY_MCP\", \"PINKY_PIP\", \"PINKY_DIP\", \"PINKY_TIP\"\n",
    "]\n",
    "\n",
    "# Define the connections between joints to visualize the hand structure\n",
    "hand_joint_connections = [\n",
    "    [0, 1], [1, 2], [2, 3], [3, 4],       # Thumb\n",
    "    [0, 5], [5, 6], [6, 7], [7, 8],       # Index finger\n",
    "    [0, 9], [9, 10], [10, 11], [11, 12],  # Middle finger\n",
    "    [0, 13], [13, 14], [14, 15], [15, 16], # Ring finger\n",
    "    [0, 17], [17, 18], [18, 19], [19, 20]  # Pinky finger\n",
    "]\n",
    "\n",
    "# Create the hand skeleton definition for visualization\n",
    "hand_skeleton = fo.KeypointSkeleton(\n",
    "    labels=hand_joint_labels,\n",
    "    edges=hand_joint_connections\n",
    ")\n",
    "\n",
    "hand_dataset.default_skeleton = hand_skeleton\n",
    "\n",
    "# Save the dataset\n",
    "hand_dataset.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(hand_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Next steps\n",
    "\n",
    "* Checkout these poset estimation datasets on the Hugging Face Hub:\n",
    "  * [DensePose-COCO](https://huggingface.co/datasets/Voxel51/DensePose-COCO)\n",
    "  * [MPII_Human_Pose_Dataset](https://huggingface.co/datasets/Voxel51/MPII_Human_Pose_Dataset)\n",
    "\n",
    "* Learn about working with [Detections](https://beta-docs.voxel51.com/how_do_i/recipes/adding_detections/)\n",
    "\n",
    "* Learn about working with [Segmentations](https://beta-docs.voxel51.com/fiftyone_concepts/using_datasets/#semantic-segmentation)\n",
    "\n",
    "* Join the [Discord community](https://community.voxel51.com/)\n",
    "\n",
    "* Follow us on [LinkedIn](https://www.linkedin.com/company/voxel51/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
