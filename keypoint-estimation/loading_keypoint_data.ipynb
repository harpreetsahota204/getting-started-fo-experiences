{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Keypoint Estimation: Loading Keypoint Data\n",
    "\n",
    "## Who this is for\n",
    "This tutorial is designed for:\n",
    "- ML engineers working with keypoint detection/pose estimation\n",
    "- Those new to FiftyOne or with basic familiarity\n",
    "- Practitioners looking to organize and visualize keypoint datasets\n",
    "- Anyone building systems for keypoint estimation\n",
    "\n",
    "## Assumed Knowledge\n",
    "You should be familiar with:\n",
    "- Basic computer vision concepts (keypoints, pose estimation)\n",
    "- Common dataset formats (COCO)\n",
    "- Python programming fundamentals\n",
    "- Basic FiftyOne concepts\n",
    "\n",
    "## Time to complete\n",
    "- 20-30 minutes to work through the examples\n",
    "- Additional time if downloading the full datasets\n",
    "\n",
    "## Required packages\n",
    "We recommend using a virtual environment with FiftyOne already installed. If you haven't installed FiftyOne yet, follow the [installation guide](https://beta-docs.voxel51.com/getting_started/basic/install/).\n",
    "\n",
    "Additional required packages:\n",
    "```python\n",
    "pip install pycocotools opencv-python numpy\n",
    "```\n",
    "\n",
    "## Content Overview\n",
    "\n",
    "This tutorial contains several key sections:\n",
    "\n",
    "- Loading Keypoint Data in Common Format: Working with standardized COCO-format keypoint data\n",
    "\n",
    "- Connecting Keypoint Edges: Creating skeleton structures to visualize keypoint relationships\n",
    "\n",
    "- Loading Keypoints with Custom Format: Converting custom keypoint formats into FiftyOne's structure\n",
    "\n",
    "### Loading Keypoint Data in Common Format\n",
    "\n",
    "Start by downloading the dataset from [this website](https://zenodo.org/records/10057090). Alternatively, you can use your favorite method for programmatically downloading the dataset, for example by running the following command in your terminal (Note: This is a 6GB download):\n",
    "\n",
    "```bash\n",
    "wget https://zenodo.org/records/10057090/files/tampar.zip?download=1\n",
    "```\n",
    "\n",
    "\n",
    "This dataset is in [COCO format](https://beta-docs.voxel51.com/fiftyone_concepts/dataset_creation/datasets/#cocodetectiondataset) and we can [automatically load it FiftyOne format](https://beta-docs.voxel51.com/fiftyone_concepts/dataset_creation/#common-formats). For this we will need to install `pycocotools`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates how to load TAMPAR dataset using FiftyOne's [data loaders for common formats](https://beta-docs.voxel51.com/fiftyone_concepts/dataset_creation/datasets/#supported-import-formats). Here's what's happening:\n",
    "\n",
    "\n",
    "- **[`fo.Dataset.from_dir()`](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#from_dir)**: This method imports the TAMPAR dataset in COCO format directly from disk. It accepts several important parameters:\n",
    "  - `dataset_type=fo.types.COCODetectionDataset`: Specifies we're loading data in [COCO object detection format](https://beta-docs.voxel51.com/api/fiftyone.types.dataset_types.COCODetectionDataset.html)\n",
    "  - `data_path` and `labels_path`: Point to the directories containing images and annotation JSON file\n",
    "  - `include_id=True`: Preserves the original COCO IDs\n",
    "  - `name=\"TAMPAR\"`: Gives our dataset a [descriptive name](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#name) in FiftyOne\n",
    "  - `persistent=True`: [Persists](https://beta-docs.voxel51.com/fiftyone_concepts/using_datasets/#dataset-persistence) the dataset to disk for future sessions\n",
    "\n",
    "- **[`compute_metadata()`](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#compute_metadata)**: After loading, this method analyzes all images to extract and store metadata like dimensions, file sizes, and other properties that enhance the dataset's functionality within FiftyOne.\n",
    "\n",
    "\n",
    "**Note:** If your annotations are in VOC format, you can use the [`VOCDetectionDataset`](https://beta-docs.voxel51.com/fiftyone_concepts/export_datasets/#vocdetectiondataset) type to import your dataset by passing [`fo.types.VOCDetectionDataset`](https://beta-docs.voxel51.com/api/fiftyone.types.html#fiftyone.types.VOCDetectionDataset) into the `dataset_type` argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 485/485 [26.2s elapsed, 0s remaining, 20.4 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "tampar_dataset = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    data_path=\"tampar\",\n",
    "    labels_path=\"tampar/tampar_test.json\",\n",
    "    include_id=True,\n",
    "    name=\"TAMPAR\",\n",
    "    persistent=True,\n",
    ")\n",
    "\n",
    "tampar_dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `keypoints` field is automatically parsed as [Keypoint labels](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) in the Dataset. \n",
    "\n",
    "[Keypoints](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoints.html) in FiftyOne represents a **collection of coordinate points that mark specific locations in an image**. These can be used for localizing important features like facial landmarks, human pose joints, or in our case, the corners of parcel boxes. Each [Keypoint label](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) contains:\n",
    "\n",
    "* A list of points as (x,y) coordinates normalized to [0,1] range\n",
    "* Optional confidence scores for each point\n",
    "* A semantic label describing what these points represent\n",
    "* Optional attributes for storing additional metadata\n",
    "\n",
    "This dataset has 8 keypoints (24 values total), all with `visibility=2` (which means \"visible\").\n",
    "\n",
    "Notice that the field we parsed (which is named `keypoints`) is a FiftyOne [Keypoints](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoints.html) label and the entire collection of points which make up the box is parsed as as a [Keypoint label](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html). This is useful when Keypoints are semantically meaningful and should be parsed together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Keypoints: {\n",
       "    'keypoints': [\n",
       "        <Keypoint: {\n",
       "            'id': '67d452a8a7449f0b9f1841c9',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'normal box',\n",
       "            'points': [\n",
       "                [0.5589583333333333, 0.6946957671957673],\n",
       "                [0.14292162698412697, 0.4266931216931217],\n",
       "                [0.09013640873015873, 0.21361772486772487],\n",
       "                [0.5546006944444445, 0.9186904761904762],\n",
       "                [0.898640873015873, 0.31390873015873016],\n",
       "                [0.4302876984126984, 0.27409391534391536],\n",
       "                [0.43548611111111113, 0.06962632275132276],\n",
       "                [0.8419171626984128, 0.5316666666666666],\n",
       "            ],\n",
       "            'confidence': None,\n",
       "            'index': None,\n",
       "            'visible': [2, 2, 2, 2, 2, 2, 2, 2],\n",
       "            'supercategory': 'box',\n",
       "            'iscrowd': 0,\n",
       "            'num_keypoints': 8,\n",
       "            'occluded': False,\n",
       "        }>,\n",
       "    ],\n",
       "}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tampar_dataset.first()['keypoints']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Dataset is loaded, we can [launch the app](https://beta-docs.voxel51.com/getting_started/basic/application_tour/) and inspect it.\n",
    "\n",
    "```python\n",
    "fo.launch_app(tampar_dataset)\n",
    "```\n",
    "\n",
    "<img src=\"assets/tampar.webp\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting Keypoint Edges in FiftyOne\n",
    "\n",
    "When combined with a [`KeypointSkeleton`](https://beta-docs.voxel51.com/api/fiftyone.core.odm.dataset.KeypointSkeleton.html), these individual points form a connected structure (like our box wireframe) that visually represents the spatial relationships between points.\n",
    "\n",
    "In our parcel detection dataset, we use keypoints to mark the 8 corners of each box, allowing us to reconstruct the 3D geometry of parcels from 2D images.\n",
    "\n",
    "## The Skeleton Structure\n",
    "\n",
    "All [Dataset](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset) instances have [`skeletons`](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#skeletons) and [`default_skeleton`](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#default_skeleton) properties that you can use to store keypoint skeletons for Keypoint field(s) of a dataset.\n",
    "\n",
    "To visualize these boxes correctly in FiftyOne, we define a skeleton that connects these keypoints with edges. Our [`KeypointSkeleton`](https://beta-docs.voxel51.com/api/fiftyone.core.odm.dataset.KeypointSkeleton.html) defines:\n",
    "\n",
    "1. **Labels**: Semantic names for each keypoint (e.g., \"front_top_right\", \"back_bottom_left\")\n",
    "2. **Edges**: Connections between pairs of keypoints that form the wireframe of the 3D box\n",
    "\n",
    "## Edge Connections\n",
    "\n",
    "The edges are organized to represent the physical structure of the keypoints:\n",
    "\n",
    "- **Front Face**: Connects the four front corners in a clockwise or counter-clockwise order\n",
    "\n",
    "- **Top Face**: Connects the four top corners (front-top-left → back-top-left → back-top-right → front-top-right)\n",
    "\n",
    "- **Right Side**: Connects the front and back edges on the right side of the box\n",
    "\n",
    "- **Additional Edges**: Completes the remaining connections to form a full box\n",
    "\n",
    "## Implementation\n",
    "\n",
    "In FiftyOne, we implement this by creating a [`KeypointSkeleton`](https://beta-docs.voxel51.com/api/fiftyone.core.odm.dataset.KeypointSkeleton.html) object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "labels = [\n",
    "    \"front_upper_right\",    # 0\n",
    "    \"front_lower_left\",     # 1\n",
    "    \"front_upper_left\",     # 2\n",
    "    \"front_lower_right\",    # 3\n",
    "    \"back_upper_right\",     # 4\n",
    "    \"back_upper_left\",      # 5 \n",
    "    \"back_upper_left\",      # 6 \n",
    "    \"back_lower_right\"      # 7\n",
    "]\n",
    "\n",
    "# Complete set of edges to create a full 3D box\n",
    "edges = [\n",
    "    # Front face - complete square\n",
    "    [2, 0],  # front upper-left to front upper-right\n",
    "    [0, 3],  # front upper-right to front lower-right\n",
    "    [3, 1],  # front lower-right to front lower-left\n",
    "    [1, 2],  # front lower-left to front upper-left\n",
    "    \n",
    "    # Top face\n",
    "    [2, 6],  # front upper-left to back upper-left\n",
    "    [6, 4],  # back upper-left to back upper-right\n",
    "    [4, 0],  # back upper-right to front upper-right\n",
    "    \n",
    "    # Right side face\n",
    "    [0, 4],  # front upper-right to back upper-right\n",
    "    [4, 7],  # back upper-right to back lower-right\n",
    "    [7, 3],  # back lower-right to front lower-right\n",
    "    \n",
    "    # Back face \n",
    "    [6, 4],  # back upper-left to back upper-right\n",
    "    [4, 7],  # back upper-right to back lower-right\n",
    "    \n",
    "    # Left side face \n",
    "    [2, 6],  # front upper-left to back upper-left\n",
    "    \n",
    "    # Needed for complete box\n",
    "    [1, 5],  # front lower-left to back lower-left\n",
    "    [5, 6],  # back lower-left to back upper-left\n",
    "    [5, 7]   # back lower-left to back lower-right\n",
    "]\n",
    "\n",
    "\n",
    "tampar_dataset.default_skeleton = fo.KeypointSkeleton(\n",
    "    labels=labels,\n",
    "    edges=edges\n",
    ")\n",
    "\n",
    "tampar_dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This skeleton definition ensures that when our 3D box keypoints are visualized in FiftyOne, the lines connecting them accurately represent the box's structure, making it easy to interpret the detection results at a glance.\n",
    "\n",
    "\n",
    "```python\n",
    "fo.launch_app(tampar_dataset)\n",
    "```\n",
    "\n",
    "<img src=\"assets/tampar-skeletons.webp\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Keypoints with Custom Format\n",
    "\n",
    "We'll use the [_Hand Keypoint Detection in Single Images\n",
    "using Multiview Bootstrapping_](http://domedb.perception.cs.cmu.edu/handdb.html) dataset. You can download the dataset from here: http://domedb.perception.cs.cmu.edu/panopticDB/hands/hand_labels.zip\n",
    "\n",
    "Regardless of your dataset, whose specifics may be different, the core pattern for parsing them will remain the same:\n",
    "\n",
    "### 1. Create an empty Dataset\n",
    "\n",
    "The [`fo.Dataset()`](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html) constructor creates a new FiftyOne dataset named \"hand_keypoints\". The `overwrite=True` parameter ensures any existing dataset with the same name is replaced.\n",
    "\n",
    "### 2. Create Samples to populate the Dataset with\n",
    "The [`fo.Sample()`](https://beta-docs.voxel51.com/api/fiftyone.core.sample.Sample.html) class is the fundamental unit in FiftyOne. Each sample represents one data point (in this case, an image) along with its metadata and annotations. Here we create samples pointing to image files on disk.\n",
    "\n",
    "### 3. Parse Keypoints for each Sample\n",
    "\n",
    "The [`fo.Keypoint()`](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) class represents a single labeled point or a collection of points in an image. In this code, we're using it to represent all joints of a hand with:\n",
    "- `label`: Identifies this as a \"left hand\" or \"right hand\"\n",
    "- `points`: The normalized (0-1) coordinates of each joint\n",
    "- `num_keypoints`: The total number of joints in the hand\n",
    "\n",
    "### 4. Add the parsed Keypoint to a Keypoints Field\n",
    "The [`fo.Keypoints()`](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoints.html) class is a collection that can contain multiple `Keypoint` objects. In our case, we're creating a collection with just one keypoint (which itself contains all the joint points).\n",
    "\n",
    "### 5. Adding Fields to Samples\n",
    "FiftyOne samples are dynamic - we can add custom fields to them. Here we add a field named either \"left_hand\" or \"right_hand\" containing the keypoints collection.\n",
    "\n",
    "### 6. Add Samples to Dataset\n",
    "Rather than adding samples one at a time, which would be inefficient, we collect all samples in a list and add them to the dataset in a single batch operation using [`add_samples()`](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#add_samples).\n",
    "\n",
    "\n",
    "The FiftyOne Keypoints structure has an important hierarchical design that's reflected in our code:\n",
    "\n",
    "- A [`Keypoints`](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoints.html) object is a collection container that holds multiple `Keypoint` objects.\n",
    "\n",
    "- Each [`Keypoint`](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) object represents a semantically meaningful group of points (like a human pose or hand), not a single point.\n",
    "\n",
    "In our implementation, we store all joints of a hand/body as a single [`Keypoint`](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) object with multiple coordinate points rather than creating separate [`Keypoint`](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html)  objects for each individual joint. \n",
    "\n",
    "This approach is ideal when:\n",
    "\n",
    "1. The points collectively represent a unified structure (like a hand or full body pose)\n",
    "2. The relationships between points matter (which we visualize using a skeleton)\n",
    "3. All points share a common label (e.g., \"left hand\" or \"body\")\n",
    "\n",
    "For different use cases where points aren't semantically related (such as general point tracking or scattered interest points), you would create individual [`Keypoint`](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html)  objects for each independent point instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import fiftyone as fo\n",
    "\n",
    "# Input data path for labeled hand images\n",
    "dataset_path = 'hand_labels/manual_test/'\n",
    "\n",
    "# Get all annotation JSON files\n",
    "annotation_files = glob.glob(os.path.join(dataset_path, \"*.json\"))\n",
    "\n",
    "# Create a FiftyOne dataset\n",
    "hand_dataset = fo.Dataset(\n",
    "    \"hand_keypoints\",\n",
    "    overwrite=True,\n",
    "    persistent=True\n",
    "    )\n",
    "\n",
    "# Create a list to collect all samples\n",
    "samples_to_add = []\n",
    "\n",
    "# Process each annotation file\n",
    "for annotation_file in annotation_files:\n",
    "    # Get corresponding image file\n",
    "    image_file = annotation_file.replace(\".json\", \".jpg\")\n",
    "    \n",
    "    # Load the annotation data\n",
    "    with open(annotation_file, 'r') as file_handle:\n",
    "        annotation_data = json.load(file_handle)\n",
    "    \n",
    "    # Create a sample for this image\n",
    "    sample = fo.Sample(filepath=image_file)\n",
    "\n",
    "    # Get image dimensions for normalization\n",
    "    image = cv2.imread(image_file)\n",
    "    image_height, image_width = image.shape[:2]\n",
    "    \n",
    "    # Process hand keypoints if present\n",
    "    if 'hand_pts' in annotation_data:\n",
    "        # Extract hand joint coordinates and hand type\n",
    "        joint_coordinates = np.array(annotation_data['hand_pts'])\n",
    "        is_left_hand = annotation_data['is_left']\n",
    "        \n",
    "        # Create a list of all normalized keypoints for the hand\n",
    "        normalized_joint_points = []\n",
    "        \n",
    "        for joint_index in range(joint_coordinates.shape[0]):\n",
    "            # Normalize coordinates to [0, 1] range\n",
    "            normalized_x = joint_coordinates[joint_index, 0] / image_width\n",
    "            normalized_y = joint_coordinates[joint_index, 1] / image_height\n",
    "            \n",
    "            normalized_joint_points.append([normalized_x, normalized_y])\n",
    "        \n",
    "        # Determine field name and label based on hand type\n",
    "        hand_field_name = \"left_hand\" if is_left_hand else \"right_hand\"\n",
    "        hand_label = \"left hand\" if is_left_hand else \"right hand\"\n",
    "        \n",
    "        # Create a keypoint object containing all joint points\n",
    "        hand_keypoint = fo.Keypoint(\n",
    "            label=hand_label,\n",
    "            points=normalized_joint_points,\n",
    "            num_keypoints=len(normalized_joint_points),\n",
    "        )\n",
    "        \n",
    "        # Create keypoints collection\n",
    "        hand_keypoints_collection = fo.Keypoints(keypoints=[hand_keypoint])\n",
    "        \n",
    "        # Add keypoints to the sample\n",
    "        sample[hand_field_name] = hand_keypoints_collection\n",
    "    \n",
    "    # Process body keypoints if present\n",
    "    if 'mpii_body_pts' in annotation_data:\n",
    "        # Extract body joint coordinates\n",
    "        body_coordinates = np.array(annotation_data['mpii_body_pts'])\n",
    "        \n",
    "        # Create a list of all normalized keypoints for the body\n",
    "        normalized_body_points = []\n",
    "        \n",
    "        for joint_index in range(body_coordinates.shape[0]):\n",
    "            # Normalize coordinates to [0, 1] range\n",
    "            normalized_x = body_coordinates[joint_index, 0] / image_width\n",
    "            normalized_y = body_coordinates[joint_index, 1] / image_height\n",
    "            \n",
    "            normalized_body_points.append([normalized_x, normalized_y])\n",
    "        \n",
    "        # Create a keypoint object containing all body joint points\n",
    "        body_keypoint = fo.Keypoint(\n",
    "            label=\"body\",\n",
    "            points=normalized_body_points,\n",
    "            num_keypoints=len(normalized_body_points),\n",
    "        )\n",
    "        \n",
    "        # Create keypoints collection \n",
    "        body_keypoints_collection = fo.Keypoints(keypoints=[body_keypoint])\n",
    "        \n",
    "        # Add keypoints to the sample\n",
    "        sample[\"body\"] = body_keypoints_collection\n",
    "    \n",
    "    # Add sample to our list instead of directly to dataset\n",
    "    samples_to_add.append(sample)\n",
    "\n",
    "# Add all samples to the dataset in a single batch operation\n",
    "hand_dataset.add_samples(samples_to_add)\n",
    "\n",
    "# Save the dataset\n",
    "hand_dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recapping the code above, we:\n",
    "\n",
    "1. Locatee all JSON annotation files in the specified directory\n",
    "2. Created a new FiftyOne dataset\n",
    "3. For each annotation file:\n",
    "   - Loaded the corresponding image and JSON data\n",
    "   - Extracted hand joint coordinates\n",
    "   - Normalized the coordinates to [0,1] range\n",
    "   - Created a FiftyOne Keypoint object with all the joint points\n",
    "   - Created a Keypoints collection containing this keypoint\n",
    "   - Added this collection to the sample as either (\"left_hand\" or \"right_hand\", and \"body\")\n",
    "   - Added the sample to a list\n",
    "4. Added all samples to the dataset in one efficient batch operation\n",
    "\n",
    "Let's inspect the the keypoints in the `body` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Keypoints: {\n",
       "    'keypoints': [\n",
       "        <Keypoint: {\n",
       "            'id': '67d45301a7449f0b9f18495d',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'body',\n",
       "            'points': [\n",
       "                [0.46510416666666665, 0.7435185185185185],\n",
       "                [0.45729166666666665, 0.35555555555555557],\n",
       "                [0.4548357963562012, 0.2400996172869647],\n",
       "                [0.45037253697713214, 0.03027074248702438],\n",
       "                [0.5109375, 0.8972222222222223],\n",
       "                [0.3125, 0.7851851851851852],\n",
       "                [0.4125, 0.7472222222222222],\n",
       "                [0.5177083333333333, 0.7388888888888889],\n",
       "                [0.6838541666666667, 0.7472222222222222],\n",
       "                [0.46927083333333336, 0.8796296296296297],\n",
       "                [0.42239583333333336, 0.3398148148148148],\n",
       "                [0.4078125, 0.5583333333333333],\n",
       "                [0.365625, 0.3425925925925926],\n",
       "                [0.5484375, 0.3685185185185185],\n",
       "                [0.553125, 0.6212962962962963],\n",
       "                [0.50625, 0.7564814814814815],\n",
       "            ],\n",
       "            'confidence': None,\n",
       "            'index': None,\n",
       "            'num_keypoints': 16,\n",
       "        }>,\n",
       "    ],\n",
       "}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand_dataset.first()['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen before, we can define the connectivity between Keypoints. In this example, I will show you how to set the [`skeleton`](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#skeletons) for the semantic labels and point connectivity of the hands, while leaving the body connectivity empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keypoint labels for hand joints\n",
    "hand_joint_labels = [\n",
    "    \"WRIST\", \"THUMB_CMC\", \"THUMB_MCP\", \"THUMB_IP\", \"THUMB_TIP\",\n",
    "    \"INDEX_MCP\", \"INDEX_PIP\", \"INDEX_DIP\", \"INDEX_TIP\",\n",
    "    \"MIDDLE_MCP\", \"MIDDLE_PIP\", \"MIDDLE_DIP\", \"MIDDLE_TIP\",\n",
    "    \"RING_MCP\", \"RING_PIP\", \"RING_DIP\", \"RING_TIP\",\n",
    "    \"PINKY_MCP\", \"PINKY_PIP\", \"PINKY_DIP\", \"PINKY_TIP\"\n",
    "]\n",
    "\n",
    "# Define the hand skeleton connections\n",
    "hand_joint_connections = [\n",
    "    # Original finger connections\n",
    "    [0, 1], [1, 2], [2, 3], [3, 4],       # Thumb\n",
    "    [0, 5], [5, 6], [6, 7], [7, 8],       # Index finger\n",
    "    [0, 9], [9, 10], [10, 11], [11, 12],  # Middle finger\n",
    "    [0, 13], [13, 14], [14, 15], [15, 16], # Ring finger\n",
    "    [0, 17], [17, 18], [18, 19], [19, 20], # Pinky finger\n",
    "    \n",
    "    # Knuckle connections (MCP joints)\n",
    "    [2, 5],    # Thumb MCP to Index MCP\n",
    "    [5, 9],    # Index MCP to Middle MCP\n",
    "    [9, 13],   # Middle MCP to Ring MCP\n",
    "    [13, 17]   # Ring MCP to Pinky MCP\n",
    "]\n",
    "\n",
    "\n",
    "# Create the hand skeleton definition\n",
    "hand_skeleton = fo.KeypointSkeleton(\n",
    "    labels=hand_joint_labels,\n",
    "    edges=hand_joint_connections\n",
    ")\n",
    "\n",
    "hand_dataset.skeletons = {\n",
    "    \"right_hand\": hand_skeleton,\n",
    "    \"left_hand\": hand_skeleton,\n",
    "}\n",
    "\n",
    "# Save the dataset\n",
    "hand_dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the parsed Dataset in the FiftyOne App:\n",
    "\n",
    "```python\n",
    "fo.launch_app(hand_dataset)\n",
    "```\n",
    "\n",
    "<img src=\"assets/hands-dataset.webp\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to work with keypoint data in FiftyOne through three main approaches:\n",
    "\n",
    "1. **Loading Standard Format Data**\n",
    "   - Used[ FiftyOne's built-in COCO format support](https://beta-docs.voxel51.com/fiftyone_concepts/dataset_creation/) to load the TAMPAR dataset\n",
    "   - Learned how keypoints are automatically parsed into FiftyOne's data structure\n",
    "   - Explored the relationship between [Keypoints](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoints.html#keypoints) collections and individual [Keypoint](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) labels\n",
    "\n",
    "2. **Creating Keypoint Skeletons**\n",
    "   - Defined semantic relationships between keypoints using [KeypointSkeleton](https://beta-docs.voxel51.com/api/fiftyone.core.odm.dataset.KeypointSkeleton.html)\n",
    "   - Created edge connections to visualize 3D structures from 2D keypoints\n",
    "   - Learned how to customize skeleton visualization for different use cases\n",
    "\n",
    "3. **Working with Custom Formats**\n",
    "   - Implemented a complete workflow for loading custom keypoint data\n",
    "   - Normalized coordinate systems for consistent representation\n",
    "   - Organized related keypoints into meaningful structures\n",
    "   - Handled multiple keypoint types (hands and body) in the same dataset\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- FiftyOne provides flexible tools for working with keypoint data, whether in standard formats or custom annotations\n",
    "\n",
    "- The hierarchical Keypoints → Keypoint structure helps organize related points meaningfully\n",
    "\n",
    "- Skeleton definitions turn abstract point collections into interpretable visualizations\n",
    "\n",
    "- Batch operations (like [`add_samples()`](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#add_samples)) help maintain efficiency when working with large datasets\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "You're now equipped to:\n",
    "- Load and visualize your own keypoint datasets\n",
    "- Create custom skeleton definitions for your specific use cases\n",
    "- Convert various keypoint formats into FiftyOne's structure\n",
    "- Build more complex applications using FiftyOne's keypoint capabilities\n",
    "\n",
    "\n",
    "### Next steps\n",
    "\n",
    "Check out the additional resources in the Next Steps section to continue your journey with FiftyOne and computer vision!\n",
    "\n",
    "* Checkout these poset estimation datasets on the Hugging Face Hub:\n",
    "  * [DensePose-COCO](https://huggingface.co/datasets/Voxel51/DensePose-COCO)\n",
    "  * [MPII_Human_Pose_Dataset](https://huggingface.co/datasets/Voxel51/MPII_Human_Pose_Dataset)\n",
    "\n",
    "* Read more about [Creating Pose Skeletons from Scratch](https://voxel51.com/blog/creating-pose-skeletons-from-scratch-fiftyone-tips-and-tricks-sep-15-2023/)\n",
    "\n",
    "* Learn about working with [Detections](https://beta-docs.voxel51.com/how_do_i/recipes/adding_detections/)\n",
    "\n",
    "* Learn about working with [Segmentations](https://beta-docs.voxel51.com/fiftyone_concepts/using_datasets/#semantic-segmentation)\n",
    "\n",
    "* Join the [Discord community](https://community.voxel51.com/)\n",
    "\n",
    "* Follow us on [LinkedIn](https://www.linkedin.com/company/voxel51/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
