{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keypoint Estimation with FiftyOne\n",
    "\n",
    "## Who this is for\n",
    "This tutorial is designed for:\n",
    "- Computer vision engineers with basic FiftyOne experience ([familiar with Datasets](https://beta-docs.voxel51.com/getting_started/basic/datasets_samples_fields/) and [the FiftyOne App](https://beta-docs.voxel51.com/getting_started/basic/application_tour/))\n",
    "- Intermediate understanding of computer vision and keypoint detection\n",
    "- Those looking to implement or evaluate keypoint detection models in their workflow\n",
    "\n",
    "## Assumed Knowledge\n",
    "### Computer Vision Concepts\n",
    "- Basic understanding of keypoint detection and pose estimation\n",
    "- Familiarity with common CV models (YOLO, R-CNN)\n",
    "- Understanding of confidence scores and model evaluation metrics\n",
    "\n",
    "### Technical Requirements\n",
    "- Python programming (intermediate level)\n",
    "- Basic understanding of PyTorch or similar deep learning frameworks\n",
    "\n",
    "### FiftyOne Concepts\n",
    "You should be familiar with:\n",
    "- [Datasets and Samples](https://beta-docs.voxel51.com/getting_started/basic/datasets_samples_fields/)\n",
    "- [The FiftyOne App](https://beta-docs.voxel51.com/getting_started/basic/application_tour/)\n",
    "- [Model Zoo](https://beta-docs.voxel51.com/models/model_zoo/models/)\n",
    "- [Plugins](https://beta-docs.voxel51.com/plugins/)\n",
    "\n",
    "## Time to Complete\n",
    "Estimated time: 30 minutes\n",
    "\n",
    "## Required Packages\n",
    "First, ensure you have a virtual environment with FiftyOne installed. Then install the following packages:\n",
    "\n",
    "```python\n",
    "# Install required packages\n",
    "pip install fiftyone\n",
    "pip install ultralytics\n",
    "pip install torch torchvision\n",
    "pip install transformers\n",
    "pip install Pillow\n",
    "pip install opencv-python\n",
    "```\n",
    "\n",
    "## Content Overview\n",
    "This tutorial covers:\n",
    "1. **Dataset Download**: Loading a hand keypoints dataset from Hugging Face\n",
    "2. **Model Zoo Integration**: Using FiftyOne's built-in Keypoint R-CNN model\n",
    "3. **Ultralytics Integration**: Implementing YOLO pose estimation\n",
    "4. **Plugin Usage**: Leveraging the community-contributed ViTPose plugin\n",
    "5. **Custom Integration**: Implementing an arbitrary keypoint detection model (SuperPoint)\n",
    "6. **Evaluation**: Assessing keypoint detection performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Datset\n",
    "\n",
    "Let's start by downloading a dataset from Voxel51's [Hugging Face org](https://huggingface.co/Voxel51). In this tutorial, we'll use the the [Hands Keypoint](https://huggingface.co/datasets/Voxel51/hand-keypoints) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config file fiftyone.yml from voxel51/hand-keypoints\n",
      "Loading dataset\n",
      "Importing samples...\n",
      " 100% |█████████████████| 846/846 [29.1ms elapsed, 0s remaining, 29.1K samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "keypoint_dataset = load_from_hub(\"voxel51/hand-keypoints\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Model Zoo\n",
    "\n",
    "The FiftyOne [Model Zoo](https://beta-docs.voxel51.com/models/model_zoo/models/) has several models you can use for common tasks, including one for pose estimation, [**Keypoint R-CNN ResNet50 FPN COCO Torch**](https://beta-docs.voxel51.com/models/model_zoo/models/#keypoint-rcnn-resnet50-fpn-coco-torch).\n",
    "\n",
    "This model is a variant of the R-CNN architecture designed for keypoint detection, utilizing a ResNet50 backbone with Feature Pyramid Network (FPN) enhancements. This model is pre-trained on the COCO dataset and is capable of detecting keypoints for objects, particularly humans, with high accuracy, achieving a keypoint AP of around 61.1% on COCO-val2017.\n",
    "\n",
    "We'll [load the model from the FiftyOne Model Zoo via `load_zoo_model`](https://beta-docs.voxel51.com/api/fiftyone.zoo.models.html#load_zoo_model), and then run inference on our dataset [using the `apply_model` method](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#apply_model) of the [Dataset](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html) and store the results in a [Field](https://beta-docs.voxel51.com/getting_started/basic/datasets_samples_fields/) named `rcnn_keypoints`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 846/846 [21.1s elapsed, 0s remaining, 38.1 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "rcnn_pose_model = foz.load_zoo_model(\"keypoint-rcnn-resnet50-fpn-coco-torch\")\n",
    "\n",
    "keypoint_dataset.apply_model(rcnn_pose_model, label_field=\"rcnn_keypoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the contents of the [first](https://beta-docs.voxel51.com/tutorials/pandas_comparison/#first-and-last) [Sample](https://beta-docs.voxel51.com/api/fiftyone.core.sample.Sample.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Keypoints: {\n",
       "    'keypoints': [\n",
       "        <Keypoint: {\n",
       "            'id': '67d9a06b1b6600364313edd9',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'person',\n",
       "            'points': [\n",
       "                [0.4506858825683594, 0.16730889214409722],\n",
       "                [0.4701820373535156, 0.1366334561948423],\n",
       "                [0.43118969599405926, 0.13796715912995516],\n",
       "                [0.4919277826944987, 0.16730889214409722],\n",
       "                [0.40719436009724935, 0.165975175080476],\n",
       "                [0.5429178237915039, 0.3540289984809028],\n",
       "                [0.36445271174112953, 0.3513615360966435],\n",
       "                [0.5556653340657552, 0.6127696849681713],\n",
       "                [0.4101937929789225, 0.5860954002097801],\n",
       "                [0.5114240010579427, 0.7581446329752605],\n",
       "                [0.4236911137898763, 0.3433592619719329],\n",
       "                [0.5181727091471354, 0.7514759770146122],\n",
       "                [0.41169347763061526, 0.7568109017831308],\n",
       "                [0.6576454162597656, 0.7741492377387152],\n",
       "                [0.312712828318278, 0.7874863236038773],\n",
       "                [0.47843046188354493, 0.8861812450267651],\n",
       "                [0.5399184544881185, 0.8941835191514756],\n",
       "            ],\n",
       "            'confidence': [\n",
       "                1.0,\n",
       "                1.0,\n",
       "                1.0,\n",
       "                0.9999998807907104,\n",
       "                1.0,\n",
       "                0.9999606609344482,\n",
       "                0.9999843835830688,\n",
       "                0.9999891519546509,\n",
       "                0.9999998807907104,\n",
       "                0.9999996423721313,\n",
       "                1.0,\n",
       "                0.9999794960021973,\n",
       "                0.9999942779541016,\n",
       "                0.9892560243606567,\n",
       "                0.9996325969696045,\n",
       "                0.6467458009719849,\n",
       "                0.999906063079834,\n",
       "            ],\n",
       "            'index': None,\n",
       "        }>,\n",
       "    ],\n",
       "}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoint_dataset.first()['rcnn_keypoints_keypoints']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Ultralytics\n",
    "\n",
    "FiftyOne has an [integration with Ultralytics](https://beta-docs.voxel51.com/integrations/ultralytics/) which makes it easy for you to use one of their [Keypoint estimation](https://beta-docs.voxel51.com/integrations/ultralytics/#keypoints) models. All you have to do is instantiate an Ultralytics model for keypoint estimation and pass that into the [`apply_model` method](https://beta-docs.voxel51.com/api/fiftyone.core.models.html#apply_model) of your [Dataset](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 846/846 [23.5s elapsed, 0s remaining, 36.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "ul_pose_model = YOLO(\"yolo11n-pose.pt\") \n",
    "\n",
    "keypoint_dataset.apply_model(ul_pose_model, label_field=\"ul_pose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the [first Sample](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#first) of your Dataset as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Keypoints: {\n",
       "    'keypoints': [\n",
       "        <Keypoint: {\n",
       "            'id': '67d9a07f1b660036431403ca',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'person',\n",
       "            'points': [\n",
       "                [0.44621869921684265, 0.16390232741832733],\n",
       "                [0.4681428074836731, 0.13291257619857788],\n",
       "                [0.43007001280784607, 0.13300056755542755],\n",
       "                [0.4991542100906372, 0.16258731484413147],\n",
       "                [0.4017489552497864, 0.16290490329265594],\n",
       "                [0.5392971038818359, 0.33804500102996826],\n",
       "                [0.3559896945953369, 0.3735353946685791],\n",
       "                [0.567536473274231, 0.6175299882888794],\n",
       "                [0.3908997178077698, 0.548811674118042],\n",
       "                [0.513115406036377, 0.7681204676628113],\n",
       "                [0.4212362468242645, 0.30758777260780334],\n",
       "                [0.5325959324836731, 0.7181503176689148],\n",
       "                [0.4006675183773041, 0.7379183173179626],\n",
       "                [0.5545814633369446, 0.7615246176719666],\n",
       "                [0.3563954830169678, 0.8105442523956299],\n",
       "                [0.6064387559890747, 0.8693596720695496],\n",
       "                [0.4452235698699951, 0.8603571057319641],\n",
       "            ],\n",
       "            'confidence': [\n",
       "                0.9964215755462646,\n",
       "                0.988978922367096,\n",
       "                0.9839149713516235,\n",
       "                0.924457311630249,\n",
       "                0.7615193724632263,\n",
       "                0.9967668056488037,\n",
       "                0.9995267391204834,\n",
       "                0.9808858036994934,\n",
       "                0.9971755743026733,\n",
       "                0.9727884531021118,\n",
       "                0.991881787776947,\n",
       "                0.9970002770423889,\n",
       "                0.9986845850944519,\n",
       "                0.9738444089889526,\n",
       "                0.9871912002563477,\n",
       "                0.819236695766449,\n",
       "                0.8919721245765686,\n",
       "            ],\n",
       "            'index': None,\n",
       "        }>,\n",
       "    ],\n",
       "}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoint_dataset.first()['ul_pose']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Plugins\n",
    "\n",
    "FiftyOne provides a powerful [Plugin framework](https://beta-docs.voxel51.com/plugins/) that allows for extending and customizing the functionality of the tool to suit your specific needs. Check out the [FiftyOne plugins repository](https://github.com/voxel51/fiftyone-plugins) for a growing collection of plugins that you can easily [download](https://beta-docs.voxel51.com/plugins/using_plugins/#plugins-download) and use locally.\n",
    "\n",
    "One plugin that's been contributed by a community member is the [ViTPose plugin](https://github.com/harpreetsahota204/vitpose-plugin). You can learn more about the plugin by visit it's GitHub repo.\n",
    "\n",
    "Let's start by setting a require enviornment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['FIFTYONE_ALLOW_LEGACY_ORCHESTRATORS'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, [download the plugin](https://beta-docs.voxel51.com/plugins/using_plugins/#downloading-plugins):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/vitpose-plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that you've [installed all the requirements](https://beta-docs.voxel51.com/plugins/using_plugins/#installing-plugin-requirements) for the plugin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins requirements @harpreetsahota/vitpose --install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plugin requires that we have some [Metadata about our dataset](https://beta-docs.voxel51.com/fiftyone_concepts/using_datasets/#metadata). To ensure you have the required metadata, use the [`compute_metadata` method](https://beta-docs.voxel51.com/api/fiftyone.core.metadata.html#compute_metadata) of the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metadata...\n",
      " 100% |█████████████████| 846/846 [242.9ms elapsed, 0s remaining, 3.5K samples/s]      \n"
     ]
    }
   ],
   "source": [
    "keypoint_dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the Plugin via the [FiftyOne App](https://beta-docs.voxel51.com/getting_started/basic/application_tour/) or [via the SDK](https://beta-docs.voxel51.com/plugins/using_plugins/#calling-operators) by instantiating the operator [using `get_operator`](https://beta-docs.voxel51.com/api/fiftyone.operators.registry.html#get_operator) as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "vitpose_operator = foo.get_operator(\"@harpreetsahota/vitpose/vitpose_keypoint_estimator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model requires that we have bounding boxes. Luckily, we have already obtained these when we applied `keypoint-rcnn-resnet50-fpn-coco-torch` to our Dataset above.\n",
    "\n",
    "You'll need to start a [Delegated Service](https://beta-docs.voxel51.com/plugins/developing_plugins/#delegated-execution_1). To do so, open your terminal and run: `fiftyone delegated launch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the operator on your dataset\n",
    "await vitpose_operator(\n",
    "    keypoint_dataset,\n",
    "    model_name=\"usyd-community/vitpose-plus-small\",  # Select from one of the supported models\n",
    "    bbox_field=\"rcnn_keypoints_detections\", # Name of the field where your bounding box detections are stored.\n",
    "    output_field=\"vitpose_estimates\",  # Name of the field to store the Keypoints in.\n",
    "    confidence_threshold= 0.55, #Confidence threshold for keypoint detection\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running the Operator in a notebook, you will need to [Save your Dataset](https://beta-docs.voxel51.com/faq/#why-didnt-changes-to-my-dataset-save):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can monitor the progress of the execution in your terminal. You'll see something to the effect of `Operation 67d88b7a7fe8205cb39fcf8b complete`  upon successful execution of the [Operator](https://beta-docs.voxel51.com/api/fiftyone.operators.operator.Operator.html). Like before, you can inspect the first element of your Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Keypoints: {\n",
       "    'keypoints': [\n",
       "        <Keypoint: {\n",
       "            'id': '67d9a0c46b8b76864b1b1714',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'person',\n",
       "            'points': [\n",
       "                [0.44923893610636395, 0.1704833984375],\n",
       "                [0.4699263572692871, 0.13364246509693287],\n",
       "                [0.4285169919331869, 0.13664460358796296],\n",
       "                [0.49417511622111004, 0.1592803955078125],\n",
       "                [0.40818579991658527, 0.1666087962962963],\n",
       "                [0.538152567545573, 0.35329295970775465],\n",
       "                [0.37299680709838867, 0.3453253286856192],\n",
       "                [0.5570560455322265, 0.6124801070601852],\n",
       "                [0.4096650759379069, 0.5843711570457176],\n",
       "                [0.5075816154479981, 0.763127757884838],\n",
       "                [0.4215219179789225, 0.34310212311921295],\n",
       "                [0.5223502159118653, 0.7390630651403356],\n",
       "                [0.4158531506856283, 0.7545021339699074],\n",
       "                [0.6923196156819661, 0.7509071632667824],\n",
       "                [0.30827372868855796, 0.787372617368345],\n",
       "                [0.5190111478169759, 0.9034255416304977],\n",
       "            ],\n",
       "            'confidence': [\n",
       "                0.9434729814529419,\n",
       "                0.9817294478416443,\n",
       "                0.9361246824264526,\n",
       "                0.8964086174964905,\n",
       "                0.9384421110153198,\n",
       "                0.9214484095573425,\n",
       "                0.9293968677520752,\n",
       "                0.9615890979766846,\n",
       "                0.9260848760604858,\n",
       "                0.9617094397544861,\n",
       "                0.9044341444969177,\n",
       "                0.7140032649040222,\n",
       "                0.7526169419288635,\n",
       "                0.8668050765991211,\n",
       "                0.9065258502960205,\n",
       "                0.8299188017845154,\n",
       "            ],\n",
       "            'index': None,\n",
       "        }>,\n",
       "    ],\n",
       "}>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoint_dataset.first()['vitpose_estimates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbitrary Keypoint Estimation Model\n",
    "\n",
    "The following example shows how to run inference on your dataset using an arbitrary keypoint estimation model.\n",
    "\n",
    "When integrating a Keypoint detection model with FiftyOne, you'll typically follow a two-phase process: first setting up the model, then processing its outputs into FiftyOne format.\n",
    "\n",
    "#### Setting Up Your Model\n",
    "\n",
    "Before running inference, you need to configure your model and any preprocessing components. This setup phase typically involves:\n",
    "\n",
    "1. **Loading Model Weights**: Import your pretrained model from a local path or model hub\n",
    "2. **Configuring the Model**: Set any specific parameters like confidence thresholds or detection modes\n",
    "3. **Preparing Preprocessing**: Set up any image transformations required before inference\n",
    "4. **Device Placement**: Move the model to the appropriate device (CPU/GPU)\n",
    "\n",
    "This preparation ensures your model is ready to process images efficiently. For a keypoint detection model, the setup might include configuring the number of keypoints to detect, how confidence scores are calculated, and any specific detection thresholds.\n",
    "\n",
    "Once your model is configured, you can proceed to the inference phase where you'll run the model on each sample and process its outputs into FiftyOne's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from transformers import AutoImageProcessor, SuperPointForKeypointDetection\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "superpoint_processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superpoint\")\n",
    "\n",
    "superpoint_model = SuperPointForKeypointDetection.from_pretrained(\"magic-leap-community/superpoint\", device_map=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with keypoint detection models like SuperPoint, processing their outputs for storage in FiftyOne follows a consistent pattern:\n",
    "\n",
    "### 1. Process Each Dataset Sample\n",
    "\n",
    "The workflow starts by [iterating through](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#iter_samples) each [sample in the Dataset](https://beta-docs.voxel51.com/getting_started/basic/datasets_samples_fields/). For each image, the model is run to detect keypoints, using the sample's actual dimensions to ensure proper scaling.\n",
    "\n",
    "### 2. Extract Raw Keypoint Data\n",
    "\n",
    "The model output typically provides two key pieces of information:\n",
    "- Keypoint coordinates in pixel space (x, y positions)\n",
    "- Confidence scores for each detected point\n",
    "\n",
    "### 3. Normalize Coordinates\n",
    "\n",
    "FiftyOne requires [Keypoint](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) coordinates to be normalized to [0,1] range rather than pixel coordinates. This normalization makes the keypoints resolution-independent and ensures they work correctly across different image sizes.\n",
    "\n",
    "### 4. Create Structured Keypoint Objects\n",
    "\n",
    "FiftyOne's [`Keypoint`](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) structure has an important hierarchical design that affects how we process model outputs:\n",
    "\n",
    "- A [`Keypoints` object](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoints.html) is a collection container that holds multiple `Keypoint` objects\n",
    "- Each [`Keypoint` object](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) can represent either a single point or a semantically meaningful group of points\n",
    "\n",
    "### 5. Choose the Appropriate Representation\n",
    "\n",
    "The way we organize points depends on their semantic relationships:\n",
    "\n",
    "**For semantically unrelated points (as in SuperPoint)**:\n",
    "- Each interest point becomes its own individual [`Keypoint`](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) object\n",
    "- All these individual [`Keypoint`](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html)  objects are gathered into a single [`Keypoints` object](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoints.html) collection\n",
    "\n",
    "**For semantically related points (like hand or body poses)**:\n",
    "- All related joints would be stored as multiple coordinates within a single `Keypoint` object\n",
    "- The `Keypoint` object would represent the entire structure (e.g., \"left hand\" or \"body\")\n",
    "- We might have multiple such `Keypoint` objects in one `Keypoints` collection (e.g., for multiple people)\n",
    "\n",
    "### 6. Store in the Dataset\n",
    "\n",
    "The final step adds this structured data to each sample in the dataset, creating a new field that holds the keypoints collection.\n",
    "\n",
    "### Key Representation Differences\n",
    "\n",
    "The distinction between these two approaches is crucial:\n",
    "\n",
    "1. **Interest Point Models**: Each detected point stands alone functionally, without inherent relationships to other points. We create separate `Keypoint` objects for each independent point.\n",
    "\n",
    "2. **Structural Keypoint Models (Pose/Hand)**: The points collectively represent a unified structure where relationships between points matter. We'd store all joints of a hand/body as a single `Keypoint` object with multiple coordinate points.\n",
    "\n",
    "This flexible hierarchical structure allows FiftyOne to appropriately represent both scattered interest points from models like SuperPoint and structured anatomical features from pose estimation models, while maintaining the proper semantic relationships in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each sample in the dataset\n",
    "for sample in keypoint_dataset.iter_samples(autosave=True):\n",
    "    # Get image dimensions from metadata\n",
    "    img_height = sample.metadata.height\n",
    "    img_width = sample.metadata.width\n",
    "    \n",
    "    # Load image from file path\n",
    "    file_path = sample.filepath\n",
    "    sample_media = Image.open(file_path)\n",
    "    \n",
    "    # Process image through SuperPoint model\n",
    "    inputs = superpoint_processor(sample_media, return_tensors=\"pt\").to(device, superpoint_model.dtype)  # Prepare inputs for model\n",
    "    outputs = superpoint_model(**inputs)  # Run inference\n",
    "    \n",
    "    # Post-process model outputs to get keypoints in image coordinates\n",
    "    processed_outputs = superpoint_processor.post_process_keypoint_detection(\n",
    "        outputs, \n",
    "        [(img_height, img_width)]  # Provide original image dimensions for coordinate scaling\n",
    "    )\n",
    "    \n",
    "    # Extract keypoint coordinates and confidence scores\n",
    "    keypoints = processed_outputs[0]['keypoints'].tolist()  # List of [x,y] coordinates\n",
    "    scores = processed_outputs[0]['scores'].tolist()  # Confidence score for each keypoint\n",
    "    \n",
    "    # Create a list of individual Keypoint objects\n",
    "    keypoint_objects = []\n",
    "    for idx, (keypoint, score) in enumerate(zip(keypoints, scores)):\n",
    "        # Normalize coordinates to [0, 1] range for FiftyOne format\n",
    "        normalized_x = keypoint[0] / img_width\n",
    "        normalized_y = keypoint[1] / img_height\n",
    "        \n",
    "        # Create individual keypoint object with normalized coordinates\n",
    "        kp = fo.Keypoint(\n",
    "            label=f\"point_{idx}\",  # Unique label for each keypoint\n",
    "            points=[(normalized_x, normalized_y)],  # Points expects a list of coordinates\n",
    "            confidence=[score]  # Confidence score from model, expected as a list\n",
    "        )\n",
    "        keypoint_objects.append(kp)\n",
    "    \n",
    "    # Create Keypoints collection containing all keypoints for this image\n",
    "    keypoints_collection = fo.Keypoints(keypoints=keypoint_objects)\n",
    "    \n",
    "    # Add keypoints to the sample and save\n",
    "    # sample.set_field(\"superpoint_keypoints\", keypoints_collection, create=True)  # create=True allows new field creation\n",
    "    sample[\"superpoint_keypoints\"] = keypoints_collection\n",
    "    # sample.save()  # Persist changes to database\n",
    "\n",
    "# Reload dataset to ensure changes are reflected in memory\n",
    "keypoint_dataset.reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the line below (excluded here, because there are over 700 Keypoint objects) and notice the difference between how these are parsed and how the outputs for pose estimation models are parsed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_dataset.first()['superpoint_keypoints']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the outputs from the various keypoint estimation models. Start by launching the app:\n",
    "\n",
    "```python\n",
    "fo.launch_app(keypoint_dataset)\n",
    "```\n",
    "\n",
    "<img src =\"assets/keypoint-estimation-output.webp\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Keypoints\n",
    "\n",
    "You can use FiftyOne's [Evaluation API](https://beta-docs.voxel51.com/fiftyone_concepts/evaluation/) to evaluate the output of keypoint estimation models.\n",
    "\n",
    "[Keypoints](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) are treated as [Detections](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detections.html) for the purposes of evaluation in FiftyOne. We can use the [`evaluate_detections`](https://beta-docs.voxel51.com/api/fiftyone.utils.eval.detection.html#evaluate_detections) method of the Dataset to perform evaluation. Note that when evaluating keypoints, “IoUs” are computed via [object keypoint similarity](https://cocodataset.org/#keypoints-eval).\n",
    "\n",
    "For illustrative purposes we will consider `ul_pose` as the ground truth and `vitpose_estimates` as the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_eval_results = keypoint_dataset.evaluate_detections(\n",
    "    pred_field=\"vitpose_estimates\",\n",
    "    gt_field=\"ul_pose\",\n",
    "    eval_key=\"ul_vs_vitpose\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of [`evaluate_detections`](https://beta-docs.voxel51.com/api/fiftyone.utils.eval.detection.html#evaluate_detections) is an [EvaluationResults](https://beta-docs.voxel51.com/api/fiftyone.core.evaluation.EvaluationResults.html) object. You can use the [`print_report`](https://beta-docs.voxel51.com/api/fiftyone.utils.eval.base.BaseClassificationResults.html#print_report) or [`print_metrics`](https://beta-docs.voxel51.com/api/fiftyone.utils.eval.base.BaseEvaluationResults.html#print_metrics) methods of the [EvaluationResults](https://beta-docs.voxel51.com/api/fiftyone.core.evaluation.EvaluationResults.html) object to see high-level performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      person       0.51      0.97      0.67      1477\n",
      "\n",
      "   micro avg       0.51      0.97      0.67      1477\n",
      "   macro avg       0.51      0.97      0.67      1477\n",
      "weighted avg       0.51      0.97      0.67      1477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kp_eval_results.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy   0.5\n",
      "precision  0.51\n",
      "recall     0.97\n",
      "fscore     0.67\n",
      "support    1477\n"
     ]
    }
   ],
   "source": [
    "kp_eval_results.print_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation routine also populated some new fields on our dataset that contain helpful information that we can use to evaluate our predictions at the sample-level.\n",
    "\n",
    "In particular, each sample now contains new fields:\n",
    "\n",
    "- `ul_vs_vitpose_tp`: the number of true positive (TP) predictions in the sample\n",
    "- `ul_vs_vitpose_fp`: the number of false positive (FP) predictions in the sample\n",
    "- `ul_vs_vitpose_fn`: the number of false negative (FN) predictions in the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        voxel51/hand-keypoints\n",
       "Media type:  image\n",
       "Num samples: 846\n",
       "Persistent:  False\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:                        fiftyone.core.fields.ObjectIdField\n",
       "    filepath:                  fiftyone.core.fields.StringField\n",
       "    tags:                      fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:                  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:                fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at:          fiftyone.core.fields.DateTimeField\n",
       "    right_hand:                fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Keypoints)\n",
       "    body:                      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Keypoints)\n",
       "    left_hand:                 fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Keypoints)\n",
       "    rcnn_keypoints_detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    rcnn_keypoints_keypoints:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Keypoints)\n",
       "    ul_pose:                   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Keypoints)\n",
       "    vitpose_estimates:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Keypoints)\n",
       "    superpoint_keypoints:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Keypoints)\n",
       "    ul_vs_vitpose_tp:          fiftyone.core.fields.IntField\n",
       "    ul_vs_vitpose_fp:          fiftyone.core.fields.IntField\n",
       "    ul_vs_vitpose_fn:          fiftyone.core.fields.IntField"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoint_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also obtain information [aggregate information](https://beta-docs.voxel51.com/fiftyone_concepts/using_aggregations/) about the metrics, for example the [upper and lower bounds](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#bounds) of the IOU values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5010650611528887, 0.9999724480963939)\n"
     ]
    }
   ],
   "source": [
    "print(keypoint_dataset.bounds(\"vitpose_estimates.keypoints.ul_vs_vitpose_iou\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the [count](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#count_values) of true positives and false negatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 1437, 'fp': 1372}\n"
     ]
    }
   ],
   "source": [
    "print(keypoint_dataset.count_values(\"vitpose_estimates.keypoints.ul_vs_vitpose\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this tutorial, we've explored several approaches to working with keypoint estimation in FiftyOne:\n",
    "\n",
    "- Used the Model Zoo's Keypoint R-CNN for out-of-the-box pose estimation\n",
    "- Integrated Ultralytics' YOLO pose estimation model\n",
    "- Leveraged community plugins with ViTPose\n",
    "- Implemented a custom keypoint detection model (SuperPoint)\n",
    "- Evaluated keypoint detection performance using FiftyOne's evaluation tools\n",
    "\n",
    "## Key Takeaways\n",
    "- FiftyOne provides multiple paths for keypoint detection, from pre-built solutions to custom implementations\n",
    "- The framework's flexible data structures can handle both structured pose keypoints and unstructured interest points\n",
    "- Built-in evaluation tools make it easy to compare and assess different keypoint detection approaches\n",
    "\n",
    "## Next Steps\n",
    "To build upon what you've learned:\n",
    "\n",
    "- Explore other keypoint detection models in the [Model Zoo](https://beta-docs.voxel51.com/models/model_zoo/models/)\n",
    "\n",
    "- Try implementing your own custom keypoint detection models\n",
    "\n",
    "- Check out the [FiftyOne plugins repository](https://github.com/voxel51/fiftyone-plugins) for more community-contributed tools\n",
    "\n",
    "- Learn more about [evaluating detections](https://beta-docs.voxel51.com/tutorials/evaluate_detections/) in FiftyOne\n",
    "\n",
    "- Read [this blog post](https://voxel51.com/blog/cotracker3-a-point-tracker-using-real-videos/) about using CoTracker3 with FiftyOne\n",
    "\n",
    "For questions or to share your implementations, join the [FiftyOne Discord community](https://community.voxel51.com/) and [follow us on LinkedIn](https://www.linkedin.com/company/voxel51/posts/?feedView=all)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
