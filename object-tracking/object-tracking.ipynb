{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Tracking in FiftyOne: Adding Tracking Results to a FiftyOne Dataset\n",
    "\n",
    "## Who this is for\n",
    "\n",
    "This tutorial is designed for:\n",
    "- Computer vision practitioners with [basic FiftyOne experience](https://beta-docs.voxel51.com/getting_started/) (can load datasets and use the App)\n",
    "- Computer vision practitioners interested in implementing object tracking workflows\n",
    "- Anyone looking to integrate tracking results into their FiftyOne datasets for visualization and analysis\n",
    "\n",
    "## Assumed Knowledge\n",
    "\n",
    "### Computer Vision Concepts\n",
    "- Basic understanding of object detection and tracking\n",
    "- Familiarity with bounding box coordinates and confidence scores\n",
    "- Understanding of video sequences and frame-based processing\n",
    "\n",
    "### Python Skills\n",
    "- Intermediate Python programming\n",
    "- Basic understanding of PyTorch\n",
    "- Experience working with Jupyter notebooks\n",
    "\n",
    "### FiftyOne Concepts\n",
    "- [Dataset basics and samples](https://beta-docs.voxel51.com/getting_started/basic/datasets_samples_fields/)\n",
    "- [Detection fields and labels](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html)\n",
    "- [Dataset views and filtering](https://beta-docs.voxel51.com/how_do_i/cheat_sheets/filtering_cheat_sheet/)\n",
    "\n",
    "## Time to Complete\n",
    "Estimated time: 30-45 minutes\n",
    "\n",
    "## Required Packages\n",
    "\n",
    "We recommend using a virtual environment with [FiftyOne already installed](https://beta-docs.voxel51.com/getting_started/basic/install/). Additional required packages:\n",
    "\n",
    "```bash\n",
    "# Assuming you have fiftyone installed\n",
    "pip install ultralytics torch\n",
    "```\n",
    "\n",
    "## Content Overview\n",
    "\n",
    "This notebook contains several key sections:\n",
    "1. Dataset Loading - Downloading and preparing the VisDrone-MOT dataset\n",
    "2. Implementation Pattern - Detailed breakdown of the tracking integration approach\n",
    "3. Tracking Implementation - Step-by-step code for [adding tracking results to FiftyOne](https://beta-docs.voxel51.com/how_do_i/recipes/adding_detections/)\n",
    "4. Results Visualization - Exploring tracking results in the [FiftyOne App](https://beta-docs.voxel51.com/getting_started/basic/application_tour/)\n",
    "\n",
    "#### Object Tracking with Ultralytics\n",
    "\n",
    "This code demonstrates a pattern for integrating object tracking into a FiftyOne workflow using the [Ultralytics integration](https://beta-docs.voxel51.com/integrations/ultralytics/). This is for **illustration purposes only** and focuses on the implementation pattern rather than prediction quality. \n",
    "\n",
    "The example illustrates a common object tracking pipeline:\n",
    "\n",
    "1. Scene-based Processing: Videos are processed as logical scenes, maintaining tracking continuity within each scene\n",
    "\n",
    "2. Object Tracking: Using YOLO's built-in tracking capabilities to assign consistent IDs to objects across frames\n",
    "\n",
    "3. FiftyOne Integration: Storing tracking results as [FiftyOne Detection](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html)\n",
    "\n",
    "Let's begin by downloading the VisDrone-MOT dataset from the [Voxel51 Hugging Face org](https://huggingface.co/datasets/Voxel51/visdrone-mot). [Refer to these docs](https://beta-docs.voxel51.com/integrations/huggingface/) for more information about how FiftyOne integrates with Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "visdrone = fouh.load_from_hub(\n",
    "    \"Voxel51/VisDrone2019-DET\",\n",
    "    name=\"visdrone-mot\",\n",
    "    persistent=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Implementation pattern\n",
    "\n",
    "We'll use the open-vocabulary detetcion model YOLO-World from Ultralytics due to its ease of implementation. Any arbitrary model will follow the same FiftyOne specific logic for iterating through the sequences and adding the [Detections](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html) as a [Field](https://beta-docs.voxel51.com/api/fiftyone.core.fields.Field.html). For each scene in dataset:\n",
    "\n",
    "\n",
    "#### 1. Initialize tracking model\n",
    "- Grab object classes using FiftyOne's [`distinct()`](https://beta-docs.voxel51.com/fiftyone_concepts/using_aggregations/#distinct-values) method\n",
    "- Create a fresh YOLO model for each scene to avoid cross-contamination\n",
    "- Configure model to detect only the filtered classes\n",
    "\n",
    "#### 2. Load all frames in sequence order\n",
    "- Use FiftyOne's [`match()`](https://beta-docs.voxel51.com/how_do_i/cheat_sheets/filtering_cheat_sheet/#built-in-filter-and-match-functions) with [`ViewField`](https://beta-docs.voxel51.com/api/fiftyone.core.expressions.ViewField.html) to filter frames by scene ID\n",
    "- Sort frames by frame number with [`sort_by()`](https://beta-docs.voxel51.com/tutorials/pandas_comparison/#sorting)\n",
    "- Extract frame filepaths to create a properly sequenced input\n",
    "\n",
    "#### 3. Run tracking across the entire sequence\n",
    "- Process all frames in a single batch with YOLO's `track()` method\n",
    "- Uses BoTSORT tracker to maintain object identity across frames\n",
    "- Returns detection boxes with consistent ID numbers for the same object\n",
    "\n",
    "#### 4. Store tracking results with persistent IDs\n",
    "- Create fresh [`fo.Detections()`](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detections.html) object for each frame\n",
    "- Convert YOLO's detection format to FiftyOne's normalized coordinates\n",
    "- Store track IDs in the [`index` field of each `fo.Detection`](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html) object\n",
    "- Save changes to the database with [`video_frame.save()`](https://beta-docs.voxel51.com/faq/#why-didnt-changes-to-my-dataset-save)\n",
    "\n",
    "#### 5. Clean up resources before next scene\n",
    "- Delete model and tracking results\n",
    "- Clear CUDA memory cache\n",
    "- Force garbage collection to prevent memory leaks\n",
    "\n",
    "The key advantage of this approach is processing each scene as a complete sequence, allowing the tracker to maintain consistent object identities across all frames in a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import gc\n",
    "import fiftyone as fo\n",
    "from ultralytics import YOLO\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Get all classes\n",
    "all_classes = visdrone.distinct(\"detections.detections.label\")\n",
    "\n",
    "# Remove unwanted classes\n",
    "detection_classes = [c for c in all_classes if c not in [\"ignored_region\", \"others\"]]\n",
    "\n",
    "# Use this to group samples by scene_id\n",
    "scene_ids = visdrone.distinct(\"scene_id\")\n",
    "\n",
    "for scene_id in scene_ids:\n",
    "    # Create new model instance for each scene\n",
    "    tracker_model = YOLO(\"yolov8x-world.pt\")\n",
    "    tracker_model.set_classes(detection_classes)  # Classes to predict\n",
    "    \n",
    "    # Get all frames for this scene and sort by frame_number\n",
    "    frames_in_scene = visdrone.match(F(\"scene_id\") == scene_id).sort_by(\"frame_number\")\n",
    "    print(f\"Processing scene {scene_id} with {len(frames_in_scene)} frames\")\n",
    "    \n",
    "    # Get the image paths in sequence order\n",
    "    frame_filepaths = [frame.filepath for frame in frames_in_scene]\n",
    "    \n",
    "    # Run tracking on the sequence of images\n",
    "    tracking_results = tracker_model.track(\n",
    "        source=frame_filepaths, \n",
    "        show=False,\n",
    "        persist=True,\n",
    "        half=True,\n",
    "        tracker=\"botsort.yaml\",  # or use bytetrack.yaml\n",
    "    )\n",
    "    \n",
    "    # Update the dataset with tracking results\n",
    "    for i, (video_frame, result) in enumerate(zip(frames_in_scene, tracking_results)):\n",
    "        # Get image dimensions for normalization\n",
    "        image_width = video_frame.metadata.width\n",
    "        image_height = video_frame.metadata.height\n",
    "        \n",
    "        # Always create a fresh Detections object for tracked_objects\n",
    "        video_frame[\"tracked_objects\"] = fo.Detections()\n",
    "        \n",
    "        # Get boxes from this result\n",
    "        boxes = result.boxes\n",
    "        \n",
    "        # Skip if no boxes or if boxes is empty\n",
    "        if boxes is None or len(boxes) == 0:\n",
    "            video_frame.save()\n",
    "            continue\n",
    "        \n",
    "        # Get all data at once to avoid repeated GPU->CPU transfers\n",
    "        xyxy_coords = boxes.xyxy.cpu().numpy()\n",
    "        class_indices = boxes.cls.cpu().numpy()\n",
    "        confidences = boxes.conf.cpu().numpy()\n",
    "        \n",
    "        # Safely get track IDs if they exist, \n",
    "        track_ids = None\n",
    "        if hasattr(boxes, 'id') and boxes.id is not None:\n",
    "            track_ids = boxes.id.cpu().numpy()\n",
    "        \n",
    "        # Get class name mapping\n",
    "        class_mapping = result.names\n",
    "        \n",
    "        for j in range(len(boxes)):\n",
    "            # Get box coordinates\n",
    "            x1, y1, x2, y2 = xyxy_coords[j]\n",
    "            \n",
    "            # Convert to [x, y, width, height] and normalize\n",
    "            normalized_bbox = [\n",
    "                x1/image_width, \n",
    "                y1/image_height, \n",
    "                (x2-x1)/image_width, \n",
    "                (y2-y1)/image_height\n",
    "            ]\n",
    "            \n",
    "            # Get class name, confidence, and track ID\n",
    "            class_idx = int(class_indices[j])\n",
    "            class_name = class_mapping[class_idx]\n",
    "            confidence = float(confidences[j])\n",
    "            track_id = int(track_ids[j]) if track_ids is not None else None\n",
    "            \n",
    "            # Create detection with track ID\n",
    "            tracked_detection = fo.Detection(\n",
    "                label=class_name,\n",
    "                bounding_box=normalized_bbox,\n",
    "                confidence=confidence,\n",
    "                index=track_id\n",
    "            )\n",
    "            \n",
    "            video_frame.tracked_objects.detections.append(tracked_detection)\n",
    "        \n",
    "        # Save each frame after processing\n",
    "        video_frame.save()\n",
    "        \n",
    "        \n",
    "    print(f\"Completed processing scene {scene_id}\")\n",
    "    \n",
    "    # Clean up resources before next scene\n",
    "    del tracker_model, tracking_results\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now launch the app and view the results:\n",
    "\n",
    "```python\n",
    "fo.launch_app(visdrone)\n",
    "```\n",
    "\n",
    "<img src=\"assets/object-tracking-yolo.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking on Video Dataset using SAM2\n",
    "\n",
    "The next example will show you how to perform tracking on a video dataset using the SAM2 model. Let's start by downloading a subset of the [WEBUOT-1M dataset](https://voxel51.com/blog/webuot-1m-a-dataset-for-underwater-object-tracking/) from the Hugging Face Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "webuot = load_from_hub(\n",
    "    \"Voxel51/WebUOT-238-Test\",\n",
    "    name=\"webuot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration purposed, we will randomly select 3 samples from the `webuot` dataset using FiftyOne's [`take()`](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#take) method. This creates a lightweight [View](https://beta-docs.voxel51.com/api/fiftyone.core.view.html) without modifying the original dataset\n",
    "\n",
    "#### Select specific frames for modification\n",
    "- We'll use [`match_frames()`](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#match_frames) with a [`ViewField`](https://beta-docs.voxel51.com/api/fiftyone.core.expressions.ViewField.html) condition to target only frames with frame number greater than 1\n",
    "- This selects all frames except the first frame in each video sample\n",
    "\n",
    "#### Clear ground truth annotations\n",
    "- The [`set_field(\"frames.gt\", None)`](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#set_field) operation removes all ground truth annotations from the matched frames\n",
    "- Sets the `gt` field to None, effectively clearing any existing data\n",
    "\n",
    "#### Persist changes\n",
    "- The [`save()`](https://beta-docs.voxel51.com/faq/#why-didnt-changes-to-my-dataset-save) method commits these changes back to the underlying dataset. Without this call, changes would only exist in the view and not affect the actual data\n",
    "- The [`clone()`](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#clone) method will create a deep copy of the dataset, however the source media will not be copied.\n",
    "\n",
    "This will remove annotations from everything except the first frame, which is useful for creating partial annotation scenarios or preparing data for specific evaluation workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "smol_view = webuot.take(3, seed=51)\n",
    "\n",
    "smol_view.match_frames(F(\"frame_number\") > 1).set_field(\"frames.gt\", None).save()\n",
    "\n",
    "smol_view= smol_view.clone(\"smol_webout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the subset and verify that only the annotations from the first frame is kept:\n",
    "\n",
    "<img src=\"assets/webuot-smol.gif\">\n",
    "\n",
    "We'll use [FiftyOne's integration with SAM2](https://voxel51.com/blog/sam-2-is-now-available-in-fiftyone/) via the [FiftyOne Model Zoo](https://beta-docs.voxel51.com/models/model_zoo/api/). Prior to using the model, you will need to install SAM2. Follow the [installation instructions from the SAM2 GitHub](https://github.com/facebookresearch/sam2).\n",
    "\n",
    "[`load_zoo_model()`](https://beta-docs.voxel51.com/api/fiftyone.zoo.models.html#load_zoo_model) will download and initialize SAM2 (Segment Anything Model 2). In this example we are using the `hiera-tiny-video` variant optimized for video processing. Visit the [documentation for the Model Zoo](https://beta-docs.voxel51.com/models/model_zoo/models/) and search for \"SAM2\" to see all supported checkpoints\n",
    "\n",
    "We'll use the [`apply_model`](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#apply_model) and pass the existing ground truth boxes from the first frame (in `frames.gt`) as prompts to guide segmentation and creates new boxes and [Segmentations](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Segmentation.html) in the `sam_predictions` field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "sam2_model = foz.load_zoo_model(\"segment-anything-2-hiera-tiny-video-torch\")\n",
    "\n",
    "smol_view.apply_model(\n",
    "    sam2_model,\n",
    "    label_field=\"sam_predictions\",\n",
    "    prompt_field=\"frames.gt\", # Can be a detections or a keypoint field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can launch the app and view the results:\n",
    "\n",
    "<img src=\"assets/webuot-sam-tracking.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "\n",
    "1. **Implement Object Tracking in FiftyOne**\n",
    "   - Set up tracking workflows using both YOLO-World and SAM2\n",
    "   - Process videos as logical scenes to maintain tracking continuity\n",
    "   - Store tracking results with persistent object IDs in FiftyOne's format\n",
    "\n",
    "2. **Handle Common Technical Challenges**\n",
    "   - Manage GPU memory effectively across long sequences\n",
    "   - Convert between different coordinate systems (YOLO to FiftyOne)\n",
    "   - Use proper cleanup procedures to prevent memory leaks\n",
    "\n",
    "3. **Leverage FiftyOne's Features**\n",
    "   - Use `distinct()` to extract unique classes and scene IDs\n",
    "   - Filter and sort frames using `match()` and `sort_by()`\n",
    "   - Visualize tracking results in the FiftyOne App\n",
    "\n",
    "The patterns demonstrated here can be adapted for:\n",
    "- Different tracking models or algorithms\n",
    "- Custom video datasets\n",
    "- Various object tracking scenarios (multi-object, single-object)\n",
    "\n",
    "For more information about working with videos in FiftyOne, check out the [video documentation](https://beta-docs.voxel51.com/api/fiftyone.core.video.html). \n",
    "\n",
    "### Next steps\n",
    "\n",
    "* Check out the [documentation for evaluating detections](https://beta-docs.voxel51.com/tutorials/evaluate_detections/) in FiftyOne\n",
    "\n",
    "* Check out [this blog](https://voxel51.com/blog/tracking-datasets-in-fiftyone/) for an end-to-end walk through of loading, predicting, and evaluating tracking results with the MOT17 dataset.\n",
    "\n",
    "* Join the [Discord community](https://community.voxel51.com/)\n",
    "\n",
    "* Follow us on [LinkedIn](https://www.linkedin.com/company/voxel51/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
