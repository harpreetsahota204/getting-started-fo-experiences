{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Object Tracking\n",
    "\n",
    "## Who this is for\n",
    "\n",
    "This guide is designed for computer vision practitioners who are new to FiftyOne but have experience working with object tracking datasets. You should be comfortable with Python and basic computer vision concepts. We assume you're trying to:\n",
    "\n",
    "- Load and organize multi-object tracking data in a structured way\n",
    "\n",
    "- Visualize tracking results across video frames\n",
    "\n",
    "- Prepare tracking data for model training or evaluation\n",
    "\n",
    "## Assumed Knowledge\n",
    "\n",
    "**CV Concepts:**\n",
    "\n",
    "- Understanding of bounding boxes and object detection\n",
    "\n",
    "- Familiarity with multi-object tracking concepts (object IDs, frame sequences)\n",
    "\n",
    "- Basic knowledge of video data representation (frames, sequences)\n",
    "\n",
    "**Data Formats:**\n",
    "\n",
    "- Experience with common annotation formats\n",
    "\n",
    "- Understanding of image file handling\n",
    "\n",
    "- Familiarity with dataset directory structures\n",
    "\n",
    "**Python Skills:**\n",
    "\n",
    "- Intermediate Python programming\n",
    "\n",
    "- Experience with pandas DataFrames\n",
    "\n",
    "- Basic file system operations\n",
    "\n",
    "**FiftyOne Concepts:**\n",
    "\n",
    "- [Datasets and Samples](https://beta-docs.voxel51.com/getting_started/basic/datasets_samples_fields/)\n",
    "\n",
    "- [Labels and Label Types](https://beta-docs.voxel51.com/api/fiftyone.core.labels.html)\n",
    "\n",
    "- [Dataset Views](https://beta-docs.voxel51.com/how_do_i/recipes/creating_views/)\n",
    "\n",
    "- [The FiftyOne App](https://beta-docs.voxel51.com/getting_started/basic/application_tour/)\n",
    "\n",
    "## Time to complete\n",
    "\n",
    "20-30 minutes (including dataset download time)\n",
    "\n",
    "## Required packages\n",
    "\n",
    "We recommend using a virtual environment with FiftyOne already installed. If you need to install FiftyOne, follow the [installation guide](https://beta-docs.voxel51.com/getting_started/basic/install/).\n",
    "\n",
    "Additional required packages:\n",
    "```bash\n",
    "pip install gdown pandas pillow\n",
    "```\n",
    "\n",
    "## Content\n",
    "\n",
    "1. **Loading Object Tracking Data**: Learn how to structure and load multi-object tracking data into FiftyOne, including handling frame sequences, annotations, and metadata.\n",
    "\n",
    "2. **Working with Scene Attributes**: Understand how to incorporate scene-level attributes and natural language descriptions into your tracking dataset.\n",
    "\n",
    "3. **Visualizing Tracking Data**: Explore how to use FiftyOne's grouping functionality to visualize tracking sequences as videos without actual video conversion.\n",
    "\n",
    "# Loading Object Tracking Data\n",
    "\n",
    "The directory structure of an multi-object tracking (MOT) dataset varies depending on the dataset. However, most datasets follow a similar organization. Generally, here's what a typical object tracking dataset structure might entail:\n",
    "\n",
    "- **Image Data:** This is typically sequential images corresponding to frames in a video\n",
    "\n",
    "- **Annotations:** These come in various formats such as JSON, XML, or text files. Annotations typically include bounding box coordinates, object IDs, and sometimes additional metadata like occlusion or truncation levels.\n",
    "\n",
    "- **Attributes Files:** While not universally included, some datasets might provide additional attributes or metadata at the scene level. \n",
    "\n",
    "- **Language:**  As part of a shift towards integrating tasks like Vision-Language Multi-Object Tracking, an emerging trend is including natural language descriptions for each scene in the datasets. This is useful for models that are designed to track objects based on human language commands or descriptions.\n",
    "\n",
    "#### Parsing the VisDrone dataset into FiftyOne\n",
    "\n",
    "In this guide, we will work with the VisDrone dataset, which was introduced in the 2020 paper [*Detection and Tracking Meet Drones Challenge*](https://arxiv.org/abs/2001.06303). This dataset contains object detection and multi-object tracking data from drone-captured imagery. Refer to dataset's [GitHub repo](https://github.com/VisDrone/VisDrone-Dataset) for more information.\n",
    "\n",
    "Start by downloading the validation set of the VisDrone for multi-object tracking. The dataset is located in a Google drive folder, which you can download from [here](https://drive.google.com/file/d/1rqnKe9IgU_crMaxRoel9_nuUsMEBBVQu/view?usp=sharing).\n",
    "\n",
    "Alternatively, you can download using `gdown` and extract the folder:\n",
    "\n",
    "```bash\n",
    "\n",
    "> pip install gdown\n",
    "> gdown 1rqnKe9IgU_crMaxRoel9_nuUsMEBBVQu\n",
    "> unzip VisDrone2019-MOT-val.zip\n",
    "```\n",
    "\n",
    "This datset contains **sequences of frames and annotations for each frame**, it does not contain scene level attributes. \n",
    "\n",
    "To demonstrate how we can parse an attributes or language as part of a MOT dataset, I'll generate dictionaries for attributes and language for each scene in the validation set. In a \"real-world\" scenario you might have these in `attributes` or `language` directories as part of the dataset. Whatever the case may be, it's just a matter of writing some logic to parse those files.  \n",
    "\n",
    "What matters for this guide is how those values are parsed as part of a FiftyOne dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_attributes = {\n",
    "    \"uav0000086_00000_v\": {\n",
    "        \"scene_type\": \"sporting event\",\n",
    "        \"time_of_day\": \"daytime\",\n",
    "        \"pedestrian_density\": \"high\"\n",
    "    },\n",
    "    \"uav0000117_02622_v\": {\n",
    "        \"scene_type\": \"intersection\",\n",
    "        \"time_of_day\": \"night\",\n",
    "        \"pedestrian_density\": \"medium\"\n",
    "    },\n",
    "    \"uav0000137_00458_v\": {\n",
    "        \"scene_type\": \"intersection\",\n",
    "        \"time_of_day\": \"daytime\",\n",
    "        \"pedestrian_density\": \"high\"\n",
    "    },\n",
    "    \"uav0000182_00000_v\": {\n",
    "        \"scene_type\": \"road\",\n",
    "        \"time_of_day\": \"daytime\",\n",
    "        \"pedestrian_density\": \"low\"\n",
    "    },\n",
    "    \"uav0000268_05773_v\": {\n",
    "        \"scene_type\": \"road\",\n",
    "        \"time_of_day\": \"daytime\",\n",
    "        \"pedestrian_density\": \"low\"\n",
    "    },\n",
    "    \"uav0000305_00000_v\": {\n",
    "        \"scene_type\": \"intersection\",\n",
    "        \"time_of_day\": \"daytime\",\n",
    "        \"pedestrian_density\": \"low\"\n",
    "    },\n",
    "    \"uav0000339_00001_v\": {\n",
    "        \"scene_type\": \"intersection\",\n",
    "        \"time_of_day\": \"dusk\",\n",
    "        \"pedestrian_density\": \"low\"\n",
    "    }\n",
    "}\n",
    "\n",
    "scene_language = {\n",
    "    \"uav0000086_00000_v\": \"A drone flies over a large crowd of people at a sporting complex where people are playing basketball.\",\n",
    "    \"uav0000117_02622_v\": \"This scene shows a busy intersection at night with cars and pedestrians moving around. There seems to be a festial going on.\",\n",
    "    \"uav0000137_00458_v\": \"This scene is a chaotic intersection with cars and pedestrians moving around. No one seems to be following the traffic rules.\",\n",
    "    \"uav0000182_00000_v\": \"This scene shows a drone flying over a road with cars moving in both directions. The road is surrounded by trees.\",\n",
    "    \"uav0000268_05773_v\": \"This scene depicts a highway with cars moving in both directions. The highway is surrounded by trees and buildings.\",\n",
    "    \"uav0000305_00000_v\": \"This scene is a direct overhead shot of an intersection with cars and pedestrians moving around. Traffic seems to be orderly.\",\n",
    "    \"uav0000339_00001_v\": \"This scene is a drone shot of an intersection at dusk with cars, motorcycles, and pedestrians moving around. The scene is well lit.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's inspect a few lines from one of the annotation files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102,0,38,666,71,88,1,1,1,0\n",
      "103,0,45,662,71,91,1,1,1,0\n",
      "104,0,52,658,72,95,1,1,1,0\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 VisDrone2019-MOT-val/annotations/uav0000086_00000_v.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what each element in the annotation represents:\n",
    "\n",
    "- `frame_index`: The index of the frame where the object is detected.\n",
    "\n",
    "- `target_id`: A unique identifier assigned to each tracked object across frames.\n",
    "\n",
    "- `bbox_left`: The x-coordinate of the left corner of the bounding box.\n",
    "\n",
    "- `bbox_top`: The y-coordinate of the left corner of the bounding box.\n",
    "\n",
    "- `bbox_width`: The width of the bounding box.\n",
    "\n",
    "- `bbox_height`: The height of the bounding box.\n",
    "\n",
    "- `score`: The confidence score of the detection.\n",
    "\n",
    "- `object_category`: The category of the detected object (e.g., person, car, bicycle).\n",
    "\n",
    "- `truncation`: Indicates if the object is partially outside the image frame.\n",
    "\n",
    "- `occlusion`: Indicates if the object is partially occluded by another object.\n",
    "\n",
    "The mapping of object category from integer to a human readable format is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0: 'ignored_region', \n",
    "    1: 'pedestrian', \n",
    "    2: 'people', \n",
    "    3: 'bicycle', \n",
    "    4: 'car', \n",
    "    5: 'van', \n",
    "    6: 'truck', \n",
    "    7: 'tricycle', \n",
    "    8: 'awning-tricycle', \n",
    "    9: 'bus', \n",
    "    10: 'motor', \n",
    "    11: 'others'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code processes the VisDrone MOT dataset into a [FiftyOne Dataset](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html). Here's what we're working with:\n",
    "\n",
    "1. **Directory Structure**\n",
    "   - Sequences directory: Contains image frames for each scene\n",
    "   - Annotations directory: Contains tracking data in text files\n",
    "   - Each scene has its own sequence folder and matching annotation file\n",
    "\n",
    "2. **Data Organization**\n",
    "   - Scene Level: Attributes (scene type, time of day, etc.) and language descriptions\n",
    "   - Frame Level: Individual images from each sequence\n",
    "   - Object Level: Bounding boxes with tracking IDs and classifications\n",
    "\n",
    "3. **Processing Pipeline**\n",
    "   - Reads each sequence directory\n",
    "   - Loads corresponding annotation file as pandas DataFrame\n",
    "   - For each frame:\n",
    "     - Creates [FiftyOne Sample](https://beta-docs.voxel51.com/api/fiftyone.core.sample.Sample.html) with image path\n",
    "     - Adds scene metadata and attributes\n",
    "     - Converts annotations to [FiftyOne Detections](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html)\n",
    "     - Normalizes bounding box coordinates\n",
    "     - Maps class IDs to readable names\n",
    "\n",
    "4. **Key Features**\n",
    "   - Maintains object identity across frames (tracking IDs)\n",
    "   - Preserves scene context through attributes\n",
    "   - Includes object properties (occlusion, visibility)\n",
    "   - Normalizes coordinates for consistent representation\n",
    "\n",
    "The result is a structured [FiftyOne Dataset](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html) that maintains the hierarchical relationship between scenes, frames, and tracked objects while adding rich metadata and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 2846/2846 [25.1s elapsed, 0s remaining, 318.4 samples/s]      \n",
      "Computing metadata...\n",
      " 100% |███████████████| 2846/2846 [25.3s elapsed, 0s remaining, 112.4 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import fiftyone as fo\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = fo.Dataset(\n",
    "    name=\"visdrone-mot\",\n",
    "    overwrite=True,\n",
    "    persistent=True\n",
    "    )\n",
    "\n",
    "# Base directories\n",
    "sequences_dir = \"VisDrone2019-MOT-val/sequences/\"\n",
    "annotations_dir = \"VisDrone2019-MOT-val/annotations/\"\n",
    "\n",
    "# List to store all samples\n",
    "samples = []\n",
    "\n",
    "# Process each sequence\n",
    "for sequence_name in os.listdir(sequences_dir):\n",
    "    sequence_path = os.path.join(sequences_dir, sequence_name)\n",
    "    if not os.path.isdir(sequence_path):\n",
    "        continue\n",
    "        \n",
    "    # Get scene_id from sequence name\n",
    "    scene_id = sequence_name  # e.g., \"uav0000086_00000_v\"\n",
    "    \n",
    "    # Load annotations\n",
    "    anno_file = os.path.join(annotations_dir, f\"{sequence_name}.txt\")\n",
    "    \n",
    "    df = pd.read_csv(anno_file, names=[\n",
    "        'frame_index', 'target_id', 'bbox_left', 'bbox_top', 'bbox_width', \n",
    "        'bbox_height', 'score', 'object_category', 'truncation', 'occlusion'\n",
    "    ])\n",
    "    \n",
    "    # Process each image\n",
    "    for img_name in sorted(os.listdir(sequence_path)):\n",
    "        img_path = os.path.join(sequence_path, img_name)\n",
    "        frame_no = int(os.path.splitext(img_name)[0])\n",
    "        \n",
    "        # Get image dimensions\n",
    "        with Image.open(img_path) as img:\n",
    "            width, height = img.size\n",
    "        \n",
    "        # Create sample\n",
    "        sample = fo.Sample(filepath=img_path)\n",
    "        \n",
    "        # Add scene-level information\n",
    "        sample[\"scene_id\"] = scene_id\n",
    "        sample[\"language\"] = scene_language[scene_id]\n",
    "        sample[\"frame_number\"] = frame_no\n",
    "        \n",
    "        # Add scene attributes as Classifications\n",
    "        for attr_name, attr_value in scene_attributes[scene_id].items():\n",
    "            sample[attr_name] = fo.Classification(label=attr_value)\n",
    "        \n",
    "        # Get detections for this frame\n",
    "        frame_dets = df[df.frame_index == frame_no]\n",
    "        \n",
    "        # Create detections list\n",
    "        dets = []\n",
    "        for _, row in frame_dets.iterrows():\n",
    "            bbox = [\n",
    "                row.bbox_left / width,\n",
    "                row.bbox_top / height,\n",
    "                row.bbox_width / width,\n",
    "                row.bbox_height / height\n",
    "            ]\n",
    "            \n",
    "            # Create label with class name and target ID\n",
    "            class_name = class_names[row.object_category] #grab the class name from the dictionary\n",
    "            \n",
    "            det = fo.Detection(\n",
    "                bounding_box=bbox, #bounding box for the detection\n",
    "                index=row.target_id, #unique identifier for the detection\n",
    "                confidence=row.score, #confidence score for the detection\n",
    "                label=class_name, #label for the detection\n",
    "                visibility=1 if row.truncation == 0 else 0,  # 0=visible, 1=no visibility\n",
    "                occlusion=1 if row.occlusion == 0 else 0     # 0=fully visible, 1=occluded\n",
    "            )\n",
    "\n",
    "            dets.append(det)\n",
    "            \n",
    "        sample[\"detections\"] = fo.Detections(detections=dets)\n",
    "        samples.append(sample)\n",
    "\n",
    "# Add all samples at once\n",
    "dataset.add_samples(samples)\n",
    "dataset.compute_metadata() # compute dataset stats, you can comment this out if you don't want to compute metadata\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the Dataset and inspect the fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        visdrone-mot\n",
       "Media type:  image\n",
       "Num samples: 2846\n",
       "Persistent:  True\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:                 fiftyone.core.fields.ObjectIdField\n",
       "    filepath:           fiftyone.core.fields.StringField\n",
       "    tags:               fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:           fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:         fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at:   fiftyone.core.fields.DateTimeField\n",
       "    scene_id:           fiftyone.core.fields.StringField\n",
       "    language:           fiftyone.core.fields.StringField\n",
       "    frame_number:       fiftyone.core.fields.IntField\n",
       "    scene_type:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    time_of_day:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    pedestrian_density: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    detections:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And inspect the first Sample in the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sample: {\n",
       "    'id': '67d074618cd2b7342de36ae4',\n",
       "    'media_type': 'image',\n",
       "    'filepath': '/home/harpreet/workspace/getting-started-fo-experiences/object-tracking/VisDrone2019-MOT-val/sequences/uav0000086_00000_v/0000001.jpg',\n",
       "    'tags': [],\n",
       "    'metadata': <ImageMetadata: {\n",
       "        'size_bytes': 165707,\n",
       "        'mime_type': 'image/jpeg',\n",
       "        'width': 1344,\n",
       "        'height': 756,\n",
       "        'num_channels': 3,\n",
       "    }>,\n",
       "    'created_at': datetime.datetime(2025, 3, 11, 17, 35, 29, 56000),\n",
       "    'last_modified_at': datetime.datetime(2025, 3, 11, 17, 35, 54, 484000),\n",
       "    'scene_id': 'uav0000086_00000_v',\n",
       "    'language': 'A drone flies over a large crowd of people at a sporting complex where people are playing basketball.',\n",
       "    'frame_number': 1,\n",
       "    'scene_type': <Classification: {\n",
       "        'id': '67d0744b8cd2b7342de17c1b',\n",
       "        'tags': [],\n",
       "        'label': 'sporting event',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "    'time_of_day': <Classification: {\n",
       "        'id': '67d0744b8cd2b7342de17c1c',\n",
       "        'tags': [],\n",
       "        'label': 'daytime',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "    'pedestrian_density': <Classification: {\n",
       "        'id': '67d0744b8cd2b7342de17c1d',\n",
       "        'tags': [],\n",
       "        'label': 'high',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "    'detections': <Detections: {\n",
       "        'detections': [\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c1e',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.19122023809523808,\n",
       "                    0.38095238095238093,\n",
       "                    0.01636904761904762,\n",
       "                    0.05952380952380952,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 1,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c1f',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.21502976190476192,\n",
       "                    0.3941798941798942,\n",
       "                    0.011160714285714286,\n",
       "                    0.047619047619047616,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 2,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c20',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'people',\n",
       "                'bounding_box': [\n",
       "                    0.22991071428571427,\n",
       "                    0.3835978835978836,\n",
       "                    0.014136904761904762,\n",
       "                    0.047619047619047616,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 3,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c21',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'people',\n",
       "                'bounding_box': [\n",
       "                    0.32217261904761907,\n",
       "                    0.2526455026455027,\n",
       "                    0.017857142857142856,\n",
       "                    0.05291005291005291,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 4,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c22',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'motor',\n",
       "                'bounding_box': [\n",
       "                    0.48586309523809523,\n",
       "                    0.2857142857142857,\n",
       "                    0.03422619047619048,\n",
       "                    0.0582010582010582,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 5,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c23',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.23809523809523808,\n",
       "                    0.37433862433862436,\n",
       "                    0.015625,\n",
       "                    0.062169312169312166,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 8,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c24',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'people',\n",
       "                'bounding_box': [\n",
       "                    0.22023809523809523,\n",
       "                    0.36772486772486773,\n",
       "                    0.017857142857142856,\n",
       "                    0.05423280423280423,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 9,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c25',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.49181547619047616,\n",
       "                    0.4074074074074074,\n",
       "                    0.023065476190476192,\n",
       "                    0.10052910052910052,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 11,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c26',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.47544642857142855,\n",
       "                    0.5,\n",
       "                    0.028273809523809524,\n",
       "                    0.1164021164021164,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 12,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c27',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.5029761904761905,\n",
       "                    0.455026455026455,\n",
       "                    0.023809523809523808,\n",
       "                    0.10978835978835978,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 13,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c28',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.5498511904761905,\n",
       "                    0.5568783068783069,\n",
       "                    0.03943452380952381,\n",
       "                    0.12301587301587301,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 14,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c29',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.4538690476190476,\n",
       "                    0.46296296296296297,\n",
       "                    0.026041666666666668,\n",
       "                    0.11772486772486772,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 15,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c2a',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.4724702380952381,\n",
       "                    0.4007936507936508,\n",
       "                    0.020089285714285716,\n",
       "                    0.09391534391534391,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 16,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c2b',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.6160714285714286,\n",
       "                    0.4298941798941799,\n",
       "                    0.023809523809523808,\n",
       "                    0.10846560846560846,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 17,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c2c',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.6555059523809523,\n",
       "                    0.39021164021164023,\n",
       "                    0.017857142857142856,\n",
       "                    0.10714285714285714,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 18,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c2d',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.375,\n",
       "                    0.6349206349206349,\n",
       "                    0.02976190476190476,\n",
       "                    0.16534391534391535,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 22,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c2e',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'people',\n",
       "                'bounding_box': [\n",
       "                    0.38839285714285715,\n",
       "                    0.36772486772486773,\n",
       "                    0.01711309523809524,\n",
       "                    0.06878306878306878,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 23,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c2f',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'people',\n",
       "                'bounding_box': [\n",
       "                    0.40327380952380953,\n",
       "                    0.3558201058201058,\n",
       "                    0.023065476190476192,\n",
       "                    0.06613756613756613,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 24,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c30',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'people',\n",
       "                'bounding_box': [\n",
       "                    0.40401785714285715,\n",
       "                    0.36507936507936506,\n",
       "                    0.020089285714285716,\n",
       "                    0.06746031746031746,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 25,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c31',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.26339285714285715,\n",
       "                    0.6349206349206349,\n",
       "                    0.04017857142857143,\n",
       "                    0.14814814814814814,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 26,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c32',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'people',\n",
       "                'bounding_box': [\n",
       "                    0.5550595238095238,\n",
       "                    0.36904761904761907,\n",
       "                    0.022321428571428572,\n",
       "                    0.05423280423280423,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 27,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c33',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.1793154761904762,\n",
       "                    0.38756613756613756,\n",
       "                    0.020089285714285716,\n",
       "                    0.10185185185185185,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 28,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c34',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.23363095238095238,\n",
       "                    0.23677248677248677,\n",
       "                    0.01711309523809524,\n",
       "                    0.0701058201058201,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 29,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c35',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.20982142857142858,\n",
       "                    0.27645502645502645,\n",
       "                    0.01488095238095238,\n",
       "                    0.07407407407407407,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 30,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c36',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'motor',\n",
       "                'bounding_box': [\n",
       "                    0.2864583333333333,\n",
       "                    0.25396825396825395,\n",
       "                    0.04017857142857143,\n",
       "                    0.051587301587301584,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 32,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c37',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'people',\n",
       "                'bounding_box': [\n",
       "                    0.12276785714285714,\n",
       "                    0.7235449735449735,\n",
       "                    0.041666666666666664,\n",
       "                    0.11772486772486772,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 33,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c38',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.08630952380952381,\n",
       "                    0.5145502645502645,\n",
       "                    0.022321428571428572,\n",
       "                    0.10714285714285714,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 34,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c39',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.22172619047619047,\n",
       "                    0.6415343915343915,\n",
       "                    0.036458333333333336,\n",
       "                    0.15211640211640212,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 37,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c3a',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.4025297619047619,\n",
       "                    0.8756613756613757,\n",
       "                    0.06845238095238096,\n",
       "                    0.12169312169312169,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 38,\n",
       "                'visibility': 0,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c3b',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.36681547619047616,\n",
       "                    0.876984126984127,\n",
       "                    0.041666666666666664,\n",
       "                    0.12169312169312169,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 39,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c3c',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'people',\n",
       "                'bounding_box': [\n",
       "                    0.23288690476190477,\n",
       "                    0.7777777777777778,\n",
       "                    0.052083333333333336,\n",
       "                    0.11772486772486772,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 41,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c3d',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'people',\n",
       "                'bounding_box': [\n",
       "                    0.10639880952380952,\n",
       "                    0.5,\n",
       "                    0.041666666666666664,\n",
       "                    0.07407407407407407,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 44,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c3e',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.02976190476190476,\n",
       "                    0.3955026455026455,\n",
       "                    0.020089285714285716,\n",
       "                    0.06481481481481481,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 45,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c3f',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.2775297619047619,\n",
       "                    0.32936507936507936,\n",
       "                    0.012648809523809524,\n",
       "                    0.05952380952380952,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 51,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 1,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c40',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.5840773809523809,\n",
       "                    0.21693121693121692,\n",
       "                    0.017857142857142856,\n",
       "                    0.062169312169312166,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 53,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67d0744b8cd2b7342de17c41',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'pedestrian',\n",
       "                'bounding_box': [\n",
       "                    0.46205357142857145,\n",
       "                    0.27645502645502645,\n",
       "                    0.018601190476190476,\n",
       "                    0.07804232804232804,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': 55,\n",
       "                'visibility': 1,\n",
       "                'occlusion': 0,\n",
       "            }>,\n",
       "        ],\n",
       "    }>,\n",
       "}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, each scene in this dataset is sequences of frames. Thus they can be parsed as videos.  However, converting frame sequences to MP4 videos is inefficient because:\n",
    "\n",
    "1. The conversion process is time-consuming\n",
    "\n",
    "2. High-resolution videos consume excessive storage space\n",
    "\n",
    "3. Machine learning tasks typically process individual frames anyway, making video conversion unnecessary\n",
    "\n",
    "Instead, you can use [`group_by()`](https://beta-docs.voxel51.com/fiftyone_concepts/using_views/#grouping) to create a view that groups the data by scene, ordered by frame number/timestamp. When you load a [dynamic](https://beta-docs.voxel51.com/fiftyone_concepts/using_datasets/#dynamic-attributes) grouped view in the App, you'll have the same experience as video datasets:\n",
    "\n",
    "• You can hover over tiles in the grid to animate scenes' frame data\n",
    "\n",
    "• When you click on a tile, you'll have familiar video player controls in the modal to navigate the scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "view = dataset.group_by(\n",
    "    \"scene_id\",\n",
    "    order_by=\"frame_number\"\n",
    ")\n",
    "\n",
    "# Save the view for easy loading in the App \n",
    "dataset.save_view(\"scenes\", view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now you can view the scenes in the app:\n",
    "\n",
    "```python\n",
    "fo.launch_app(dataset)\n",
    "```\n",
    "\n",
    "<img src=\"assets/visdrone-explore.gif\" width=\"80%\">\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this guide, we explored how to work with multi-object tracking data in FiftyOne, using the VisDrone dataset as an example. Here's what we covered:\n",
    "\n",
    "1. **Dataset Structure**: We learned how tracking datasets typically organize their data:\n",
    "   - Frame sequences for each scene\n",
    "   - Annotation files with bounding boxes and tracking IDs\n",
    "   - Optional scene-level attributes and descriptions\n",
    "\n",
    "2. **Data Loading**: We walked through a complete pipeline for:\n",
    "   - Loading frame sequences and annotations\n",
    "   - Converting coordinates to normalized format\n",
    "   - Adding scene metadata and attributes\n",
    "   - Creating structured FiftyOne samples with detections\n",
    "\n",
    "3. **Efficient Visualization**: Instead of converting sequences to videos, we learned how to:\n",
    "   - Use [`group_by()`](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#group_by) to organize frames by scene\n",
    "   - Create [dynamic](https://beta-docs.voxel51.com/fiftyone_concepts/using_datasets/#dynamic-attributes) views for video-like playback\n",
    "   - Leverage FiftyOne's built-in visualization capabilities\n",
    "\n",
    "\n",
    "\n",
    "### Next steps\n",
    "\n",
    "* If your starting point is a native video, refer to the [docs for how to work with video data](https://beta-docs.voxel51.com/api/fiftyone.core.video.html).\n",
    "\n",
    "* Check out [this blog](https://voxel51.com/blog/tracking-datasets-in-fiftyone/) for an end-to-end walk through of loading, predicting, and evaluating tracking results with the MOT17 dataset.\n",
    "\n",
    "* Join the [Discord community](https://community.voxel51.com/)\n",
    "\n",
    "* Follow us on [LinkedIn](https://www.linkedin.com/company/voxel51/)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
