{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Object Tracking: Loading Object Tracking Data\n",
    "\n",
    "The directory structure of an multi-object tracking (MOT) dataset varies depending on the dataset. However, most datasets follow a similar organization. Generally, here's what a typical object tracking dataset structure might entail:\n",
    "\n",
    "- **Image Data:** This is typically sequential images corresponding to frames in a video\n",
    "\n",
    "- **Annotations:** These come in various formats such as JSON, XML, or text files. Annotations typically include bounding box coordinates, object IDs, and sometimes additional metadata like occlusion or truncation levels.\n",
    "\n",
    "- **Attributes Files:** While not universally included, some datasets might provide additional attributes or metadata at the scene level. \n",
    "\n",
    "- **Language:**  As part of a shift towards integrating tasks like Vision-Language Multi-Object Tracking, an emerging trend is including natural language descriptions for each scene in the datasets. This is useful for models that are designed to track objects based on human language commands or descriptions.\n",
    "\n",
    "#### Parsing the VisDrone dataset into FiftyOne\n",
    "\n",
    "In this guide, we will work with the VisDrone dataset, which was introduced in the 2020 paper [*Detection and Tracking Meet Drones Challenge*](https://arxiv.org/abs/2001.06303). This dataset contains object detection and multi-object tracking data from drone-captured imagery. Refer to dataset's [GitHub repo](https://github.com/VisDrone/VisDrone-Dataset) for more information.\n",
    "\n",
    "Start by downloading the validation set of the VisDrone for multi-object tracking. The dataset is located in a Google drive folder, which you can download from [here](https://drive.google.com/file/d/1rqnKe9IgU_crMaxRoel9_nuUsMEBBVQu/view?usp=sharing).\n",
    "\n",
    "Alternatively, you can download using `gdown` and extract the folder:\n",
    "\n",
    "```bash\n",
    "\n",
    "> pip install gdown\n",
    "> gdown 1rqnKe9IgU_crMaxRoel9_nuUsMEBBVQu\n",
    "> unzip VisDrone2019-MOT-val.zip\n",
    "```\n",
    "\n",
    "This datset contains **sequences of frames and annotations for each frame**, it does not contain scene level attributes. \n",
    "\n",
    "To demonstrate how we can parse an attributes or language as part of a MOT dataset, I'll generate dictionaries for attributes and language for each scene in the validation set. In a \"real-world\" scenario you might have these in `attributes` or `language` directories as part of the dataset. Whatever the case may be, it's just a matter of writing some logic to parse those files.  \n",
    "\n",
    "What matters for this guide is how those values are parsed as part of a FiftyOne dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_attributes = {\n",
    "    \"uav0000086_00000_v\": {\n",
    "        \"scene_type\": \"sporting event\",\n",
    "        \"time_of_day\": \"daytime\",\n",
    "        \"pedestrian_density\": \"high\"\n",
    "    },\n",
    "    \"uav0000117_02622_v\": {\n",
    "        \"scene_type\": \"intersection\",\n",
    "        \"time_of_day\": \"night\",\n",
    "        \"pedestrian_density\": \"medium\"\n",
    "    },\n",
    "    \"uav0000137_00458_v\": {\n",
    "        \"scene_type\": \"intersection\",\n",
    "        \"time_of_day\": \"daytime\",\n",
    "        \"pedestrian_density\": \"high\"\n",
    "    },\n",
    "    \"uav0000182_00000_v\": {\n",
    "        \"scene_type\": \"road\",\n",
    "        \"time_of_day\": \"daytime\",\n",
    "        \"pedestrian_density\": \"low\"\n",
    "    },\n",
    "    \"uav0000268_05773_v\": {\n",
    "        \"scene_type\": \"road\",\n",
    "        \"time_of_day\": \"daytime\",\n",
    "        \"pedestrian_density\": \"low\"\n",
    "    },\n",
    "    \"uav0000305_00000_v\": {\n",
    "        \"scene_type\": \"intersection\",\n",
    "        \"time_of_day\": \"daytime\",\n",
    "        \"pedestrian_density\": \"low\"\n",
    "    },\n",
    "    \"uav0000339_00001_v\": {\n",
    "        \"scene_type\": \"intersection\",\n",
    "        \"time_of_day\": \"dusk\",\n",
    "        \"pedestrian_density\": \"low\"\n",
    "    }\n",
    "}\n",
    "\n",
    "scene_language = {\n",
    "    \"uav0000086_00000_v\": \"A drone flies over a large crowd of people at a sporting complex where people are playing basketball.\",\n",
    "    \"uav0000117_02622_v\": \"This scene shows a busy intersection at night with cars and pedestrians moving around. There seems to be a festial going on.\",\n",
    "    \"uav0000137_00458_v\": \"This scene is a chaotic intersection with cars and pedestrians moving around. No one seems to be following the traffic rules.\",\n",
    "    \"uav0000182_00000_v\": \"This scene shows a drone flying over a road with cars moving in both directions. The road is surrounded by trees.\",\n",
    "    \"uav0000268_05773_v\": \"This scene depicts a highway with cars moving in both directions. The highway is surrounded by trees and buildings.\",\n",
    "    \"uav0000305_00000_v\": \"This scene is a direct overhead shot of an intersection with cars and pedestrians moving around. Traffic seems to be orderly.\",\n",
    "    \"uav0000339_00001_v\": \"This scene is a drone shot of an intersection at dusk with cars, motorcycles, and pedestrians moving around. The scene is well lit.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's inspect a few lines from one of the annotation files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102,0,38,666,71,88,1,1,1,0\n",
      "103,0,45,662,71,91,1,1,1,0\n",
      "104,0,52,658,72,95,1,1,1,0\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 VisDrone2019-MOT-val/annotations/uav0000086_00000_v.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what each element in the annotation represents:\n",
    "\n",
    "- `frame_index`: The index of the frame where the object is detected.\n",
    "\n",
    "- `target_id`: A unique identifier assigned to each tracked object across frames.\n",
    "\n",
    "- `bbox_left`: The x-coordinate of the left corner of the bounding box.\n",
    "\n",
    "- `bbox_top`: The y-coordinate of the left corner of the bounding box.\n",
    "\n",
    "- `bbox_width`: The width of the bounding box.\n",
    "\n",
    "- `bbox_height`: The height of the bounding box.\n",
    "\n",
    "- `score`: The confidence score of the detection.\n",
    "\n",
    "- `object_category`: The category of the detected object (e.g., person, car, bicycle).\n",
    "\n",
    "- `truncation`: Indicates if the object is partially outside the image frame.\n",
    "\n",
    "- `occlusion`: Indicates if the object is partially occluded by another object.\n",
    "\n",
    "The mapping of object category from integer to a human readable format is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0: 'ignored_region', \n",
    "    1: 'pedestrian', \n",
    "    2: 'people', \n",
    "    3: 'bicycle', \n",
    "    4: 'car', \n",
    "    5: 'van', \n",
    "    6: 'truck', \n",
    "    7: 'tricycle', \n",
    "    8: 'awning-tricycle', \n",
    "    9: 'bus', \n",
    "    10: 'motor', \n",
    "    11: 'others'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code processes the VisDrone MOT dataset into FiftyOne format. Here's what we're working with:\n",
    "\n",
    "1. **Directory Structure**\n",
    "   - Sequences directory: Contains image frames for each scene\n",
    "   - Annotations directory: Contains tracking data in text files\n",
    "   - Each scene has its own sequence folder and matching annotation file\n",
    "\n",
    "2. **Data Organization**\n",
    "   - Scene Level: Attributes (scene type, time of day, etc.) and language descriptions\n",
    "   - Frame Level: Individual images from each sequence\n",
    "   - Object Level: Bounding boxes with tracking IDs and classifications\n",
    "\n",
    "3. **Processing Pipeline**\n",
    "   - Reads each sequence directory\n",
    "   - Loads corresponding annotation file as DataFrame\n",
    "   - For each frame:\n",
    "     - Creates FiftyOne Sample with image path\n",
    "     - Adds scene metadata and attributes\n",
    "     - Converts annotations to FiftyOne Detections\n",
    "     - Normalizes bounding box coordinates\n",
    "     - Maps class IDs to readable names\n",
    "\n",
    "4. **Key Features**\n",
    "   - Maintains object identity across frames (tracking IDs)\n",
    "   - Preserves scene context through attributes\n",
    "   - Includes object properties (occlusion, visibility)\n",
    "   - Normalizes coordinates for consistent representation\n",
    "\n",
    "The result is a structured FiftyOne dataset that maintains the hierarchical relationship between scenes, frames, and tracked objects while adding rich metadata and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import fiftyone as fo\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = fo.Dataset(\n",
    "    name=\"visdrone-mot\",\n",
    "    overwrite=True,\n",
    "    persistent=True\n",
    "    )\n",
    "\n",
    "# Base directories\n",
    "sequences_dir = \"VisDrone2019-MOT-val/sequences/\"\n",
    "annotations_dir = \"VisDrone2019-MOT-val/annotations/\"\n",
    "\n",
    "# List to store all samples\n",
    "samples = []\n",
    "\n",
    "# Process each sequence\n",
    "for sequence_name in os.listdir(sequences_dir):\n",
    "    sequence_path = os.path.join(sequences_dir, sequence_name)\n",
    "    if not os.path.isdir(sequence_path):\n",
    "        continue\n",
    "        \n",
    "    # Get scene_id from sequence name\n",
    "    scene_id = sequence_name  # e.g., \"uav0000086_00000_v\"\n",
    "    \n",
    "    # Load annotations\n",
    "    anno_file = os.path.join(annotations_dir, f\"{sequence_name}.txt\")\n",
    "    \n",
    "    df = pd.read_csv(anno_file, names=[\n",
    "        'frame_index', 'target_id', 'bbox_left', 'bbox_top', 'bbox_width', \n",
    "        'bbox_height', 'score', 'object_category', 'truncation', 'occlusion'\n",
    "    ])\n",
    "    \n",
    "    # Process each image\n",
    "    for img_name in sorted(os.listdir(sequence_path)):\n",
    "        img_path = os.path.join(sequence_path, img_name)\n",
    "        frame_no = int(os.path.splitext(img_name)[0])\n",
    "        \n",
    "        # Get image dimensions\n",
    "        with Image.open(img_path) as img:\n",
    "            width, height = img.size\n",
    "        \n",
    "        # Create sample\n",
    "        sample = fo.Sample(filepath=img_path)\n",
    "        \n",
    "        # Add scene-level information\n",
    "        sample[\"scene_id\"] = scene_id\n",
    "        sample[\"language\"] = scene_language[scene_id]\n",
    "        sample[\"frame_number\"] = frame_no\n",
    "        \n",
    "        # Add scene attributes as Classifications\n",
    "        for attr_name, attr_value in scene_attributes[scene_id].items():\n",
    "            sample[attr_name] = fo.Classification(label=attr_value)\n",
    "        \n",
    "        # Get detections for this frame\n",
    "        frame_dets = df[df.frame_index == frame_no]\n",
    "        \n",
    "        # Create detections list\n",
    "        dets = []\n",
    "        for _, row in frame_dets.iterrows():\n",
    "            bbox = [\n",
    "                row.bbox_left / width,\n",
    "                row.bbox_top / height,\n",
    "                row.bbox_width / width,\n",
    "                row.bbox_height / height\n",
    "            ]\n",
    "            \n",
    "            # Create label with class name and target ID\n",
    "            class_name = class_names[row.object_category] #grab the class name from the dictionary\n",
    "            \n",
    "            det = fo.Detection(\n",
    "                bounding_box=bbox, #bounding box for the detection\n",
    "                index=row.target_id, #unique identifier for the detection\n",
    "                confidence=row.score, #confidence score for the detection\n",
    "                label=class_name, #label for the detection\n",
    "                visibility=1 if row.truncation == 0 else 0,  # 0=visible, 1=no visibility\n",
    "                occlusion=1 if row.occlusion == 0 else 0     # 0=fully visible, 1=occluded\n",
    "            )\n",
    "\n",
    "            dets.append(det)\n",
    "            \n",
    "        sample[\"detections\"] = fo.Detections(detections=dets)\n",
    "        samples.append(sample)\n",
    "\n",
    "# Add all samples at once\n",
    "dataset.add_samples(samples)\n",
    "dataset.compute_metadata() # compute dataset stats, you can comment this out if you don't want to compute metadata\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the Dataset and inspect the fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And inspect the first Sample in the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, each scene in this dataset is sequences of frames. Thus they can be parsed as videos.  However, converting frame sequences to MP4 videos is inefficient because:\n",
    "\n",
    "1. The conversion process is time-consuming\n",
    "\n",
    "2. High-resolution videos consume excessive storage space\n",
    "\n",
    "3. Machine learning tasks typically process individual frames anyway, making video conversion unnecessary\n",
    "\n",
    "Instead, you can use `group_by()` to create a view that groups the data by scene, ordered by frame number/timestamp. When you load a dynamic grouped view in the App, you'll have the same experience as video datasets:\n",
    "\n",
    "• You can hover over tiles in the grid to animate scenes' frame data\n",
    "\n",
    "• When you click on a tile, you'll have familiar video player controls in the modal to navigate the scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "view = dataset.group_by(\n",
    "    \"scene_id\",\n",
    "    order_by=\"frame_number\"\n",
    ")\n",
    "\n",
    "# Save the view for easy loading in the App \n",
    "dataset.save_view(\"scenes\", view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now you can view the scenes in the app:\n",
    "\n",
    "```python\n",
    "fo.launch_app(dataset)\n",
    "```\n",
    "\n",
    "<img src=\"assets/visdrone-explore.gif\" width=\"80%\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
