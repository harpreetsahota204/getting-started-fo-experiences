{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Classification in FiftyOne\n",
    "\n",
    "\n",
    "\n",
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "\n",
    "dataset = fouh.load_from_hub(\"Voxel51/ImageNet-O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the classes from this dataset using the [`distinct` method](beta-docs.voxel51.com/api/fiftyone.core.aggregations.distinct.html) of the [Dataset](https://beta-docs.voxel51.com/getting_started/basic/datasets_samples_fields/). These will be the classes we use for zero-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_classes = dataset.distinct(\"ground_truth.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FiftyOne Model Zoo\n",
    "\n",
    "The [FiftyOne Model Zoo](https://beta-docs.voxel51.com/models/model_zoo/) provides a powerful interface for downloading models and applying them to your FiftyOne datasets. It provides native access to hundreds of pre-trained models, and it also supports downloading arbitrary public or private models whose definitions are provided via GitHub repositories or URLs.\n",
    "\n",
    "\n",
    "all of these models accept a text_prompt keyword argument, which allows you to override the prompt template used to embed the class names. Zero-shot classification results can vary based on this text!\n",
    "\n",
    "https://beta-docs.voxel51.com/api/fiftyone.zoo.models.html#load_zoo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "clip_zoo_model = foz.load_zoo_model(\n",
    "    model_name=\"clip-vit-base-patch32\",\n",
    "    text_prompt=\"A photo of a \",\n",
    "    classes=dataset_classes,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # install_requirements=True # uncomment this line if you are running this code for the first time\n",
    ")\n",
    "\n",
    "dataset.apply_model(clip_zoo_model, label_field=\"clip_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open CLIP Integration\n",
    "\n",
    "\n",
    "https://beta-docs.voxel51.com/integrations/openclip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "open_clip_model = foz.load_zoo_model(\n",
    "    \"open-clip-torch\",\n",
    "    text_prompt=\"A photo of a\",\n",
    "    classes=dataset_classes,\n",
    "    install_requirements=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # install_requirements=True # uncomment this line if you are running this code for the first time\n",
    ")\n",
    "\n",
    "dataset.apply_model(open_clip_model, label_field=\"open_clip_classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify different model architectures and pretrained weights by passing in optional parameters. Pretrained models can be loaded directly from OpenCLIP with the following syntax:\n",
    "\n",
    "\n",
    "```python\n",
    "meta_clip = foz.load_zoo_model(\n",
    "    \"open-clip-torch\",\n",
    "    clip_model=\"ViT-B-32-quickgelu\",\n",
    "    pretrained=\"metaclip_400m\",\n",
    "    text_prompt=\"A photo of a\",\n",
    "    classes=dataset_classes,\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "Alternatively you can also load a model from Hugging Faceâ€™s Model Hub with the following syntax:\n",
    "\n",
    "\n",
    "```python\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "open_clip_model = foz.load_zoo_model(\n",
    "    \"open-clip-torch\",\n",
    "    clip_model=\"hf-hub:repo-name/model-name\",\n",
    "    pretrained=\"\",\n",
    ")\n",
    "```\n",
    "\n",
    "As a concrete example, if you were interested in the [StreetCLIP model](https://huggingface.co/geolocal/StreetCLIP) you would use:\n",
    "\n",
    "```python\n",
    "street_clip_model = foz.load_zoo_model(\n",
    "    \"open-clip-torch\",\n",
    "    pretrained=\"\",\n",
    "    clip_model=\"hf-hub:geolocal/StreetCLIP\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Integration\n",
    "\n",
    "\n",
    "\"zero-shot-classification-transformer-torch\" specifies that you want to a zero-shot image classification model from the Hugging Face Transformers library. You can then specify the model via the name_or_path argument, which should be the repository name or model identifier of the model you want to load.\n",
    "\n",
    "https://beta-docs.voxel51.com/integrations/huggingface/#zero-shot-classification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "siglip_model = foz.load_zoo_model(\n",
    "    \"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"google/siglip2-base-patch32-256\",\n",
    "    classes=dataset_classes,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # install_requirements=True # uncomment this line if you are running this code for the first time\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbitrary Model\n",
    "\n",
    "Any model that can be run in a Hugging Face pipeline for the `zero-shot-image-classification` task can be loaded as a Zoo model.\n",
    "\n",
    "A good first entry point is to just do it and pass the model name into `name_or_path` in the [`load_zoo_model`](https://beta-docs.voxel51.com/api/fiftyone.zoo.models.html#fiftyone.zoo.models.load_zoo_model) method of the dataset. If a Hugging Face model is not compatible with the integration, you'll see an error to the effect of: \n",
    "\n",
    "```python\n",
    "ValueError: Unrecognized model in <whatever-model-name>\n",
    "```\n",
    "\n",
    "In this case, you will need to run the model manually. All this means is that you need to instantiate the model, it's  processor, and write some logic to parse it the model output a FiftyOne Classification. \n",
    "\n",
    "Here's an example of how you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Next Steps\n",
    "https://beta-docs.voxel51.com/tutorials/zero_shot_classification/\n",
    "\n",
    "https://voxel51.com/blog/a-history-of-clip-model-training-data-advances/\n",
    "\n",
    "https://voxel51.com/blog/this-visual-illusions-benchmark-makes-me-question-the-power-of-vlms/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
