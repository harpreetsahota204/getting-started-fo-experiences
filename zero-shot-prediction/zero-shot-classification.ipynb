{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who this is for\n",
    "\n",
    "This tutorial is designed for:\n",
    "\n",
    "- **FiftyOne Experience**: Beginners who have basic familiarity with FiftyOne's core concepts like [Datasets and Samples](https://beta-docs.voxel51.com/getting_started/basic/datasets_samples_fields/)\n",
    "\n",
    "- **Expertise Level**: Machine learning practitioners with basic understanding of computer vision and classification tasks\n",
    "\n",
    "- **Goals**: Users looking to implement classification for new or changing categories without retraining models, or those wanting to quickly label datasets with flexible categories\n",
    "\n",
    "## Assumed Knowledge\n",
    "\n",
    "### Computer Vision Concepts\n",
    "- Basic understanding of image classification\n",
    "- Familiarity with model inference and confidence scores\n",
    "- Understanding of zero-shot learning (though not required)\n",
    "\n",
    "### Technical Prerequisites\n",
    "- Python programming fundamentals\n",
    "- Basic understanding of PyTorch\n",
    "- Experience working with Jupyter notebooks\n",
    "\n",
    "### FiftyOne Concepts\n",
    "You should be familiar with:\n",
    "- [Datasets and Samples](https://beta-docs.voxel51.com/getting_started/basic/datasets_samples_fields)\n",
    "- [Working with Labels](https://beta-docs.voxel51.com/fiftyone_concepts/using_datasets/#labels)\n",
    "- [Model Zoo](https://beta-docs.voxel51.com/models/model_zoo/)\n",
    "\n",
    "## Time to Complete\n",
    "- Approximately 30-45 minutes \n",
    "\n",
    "## Required Packages\n",
    "\n",
    "It's recommended to use a virtual environment with [FiftyOne already installed](https://beta-docs.voxel51.com/getting_started/basic/install/). You'll need these additional packages:\n",
    "\n",
    "```bash\n",
    "# Install required packages\n",
    "pip install fiftyone\n",
    "pip install torch torchvision\n",
    "pip install open_clip_torch\n",
    "pip install transformers\n",
    "```\n",
    "\n",
    "## Content Overview\n",
    "\n",
    "The notebook covers:\n",
    "\n",
    "1. **Dataset Download**: Loading the [ImageNet-O dataset](https://huggingface.co/datasets/Voxel51/ImageNet-O) from Hugging Face\n",
    "\n",
    "2. **FiftyOne Model Zoo**: Using CLIP models from FiftyOne's[ built-in model zoo](https://beta-docs.voxel51.com/models/model_zoo/models/) for zero-shot classification\n",
    "\n",
    "3. **OpenCLIP Integration**: Implementing zero-shot classification [using OpenCLIP models](https://beta-docs.voxel51.com/integrations/openclip/) with various architectures\n",
    "\n",
    "4. **Hugging Face Integration**: Running zero-shot classification using models from [Hugging Face's](https://beta-docs.voxel51.com/integrations/huggingface/model hub \n",
    "\n",
    "\n",
    "# Zero-Shot Classification in FiftyOne\n",
    "\n",
    "Traditionally, computer vision models are trained to predict a fixed set of categories. For image classification, for instance, many standard models are trained on the ImageNet dataset, which contains 1,000 categories. All images must be assigned to one of these 1,000 categories, and the model is trained to predict the correct category for each image.\n",
    "\n",
    "Thanks to the recent advances in multimodal models, [it is now possible to perform zero-shot learning](https://beta-docs.voxel51.com/tutorials/zero_shot_classification/), which allows us to predict categories that were not seen during training. This can be especially useful when:\n",
    "\n",
    "• We want to roughly pre-label images with a new set of categories\n",
    "\n",
    "• Obtaining labeled data for all categories is impractical or impossible.\n",
    "\n",
    "•  The categories are changing over time, and we want to predict new categories without retraining the model.\n",
    "\n",
    "# Download Dataset\n",
    "\n",
    "In this tutorial we will use the ImageNet-O dataset.\n",
    "\n",
    "The [ImageNet-O dataset](https://huggingface.co/datasets/Voxel51/ImageNet-O) consists of images from classes not found in the standard ImageNet-1k dataset. It tests the robustness and out-of-distribution detection capabilities of computer vision models trained on ImageNet-1k.\n",
    "\n",
    "Let's [load the dataset](https://beta-docs.voxel51.com/integrations/huggingface/#loading-datasets-from-the-hub) from [Voxel51's Hugging Face Org](https://huggingface.co/datasets/Voxel51/ImageNet-O):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "\n",
    "dataset = fouh.load_from_hub(\"Voxel51/ImageNet-O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the classes from this dataset using the [`distinct` method](beta-docs.voxel51.com/api/fiftyone.core.aggregations.distinct.html) of the [Dataset](https://beta-docs.voxel51.com/getting_started/basic/datasets_samples_fields/). These will be the classes we use for zero-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_classes = dataset.distinct(\"ground_truth.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FiftyOne Model Zoo\n",
    "\n",
    "The [FiftyOne Model Zoo](https://beta-docs.voxel51.com/models/model_zoo/) provides a powerful interface for downloading models and applying them to your FiftyOne datasets. It provides native access to hundreds of pre-trained models, and it also supports downloading arbitrary public or private models whose definitions are provided via GitHub repositories or URLs.\n",
    "\n",
    "All of these models accept a `text_prompt` keyword argument, which allows you to override the prompt template used to embed the class names. Zero-shot classification results can vary based on this text!\n",
    "\n",
    "You can load a model from the Model Zoo using the [`load_zoo_model` method](\n",
    "https://beta-docs.voxel51.com/api/fiftyone.zoo.models.html#load_zoo_model), in this example we will use [CLIP](https://beta-docs.voxel51.com/models/model_zoo/models/#clip-vit-base32-torch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "clip_zoo_model = foz.load_zoo_model(\n",
    "    name_or_url=\"clip-vit-base32-torch\",\n",
    "    text_prompt=\"A photo of a \",\n",
    "    classes=dataset_classes,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # install_requirements=True # uncomment this line if you are running this code for the first time\n",
    ")\n",
    "\n",
    "dataset.apply_model(clip_zoo_model, label_field=\"clip_classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the results on the [first Sample](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#first) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Classification: {\n",
       "    'id': '67d9ddda99e7fb132baf9334',\n",
       "    'tags': [],\n",
       "    'label': 'mousetrap',\n",
       "    'confidence': 0.34010758996009827,\n",
       "    'logits': None,\n",
       "}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.first()['clip_classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open CLIP Integration\n",
    "\n",
    "FiftyOne [integrates natively with the OpenCLIP library](https://beta-docs.voxel51.com/integrations/openclip/), an open source implementation of OpenAI’s CLIP (Contrastive Language-Image Pre-training) model that you can use to run inference on your FiftyOne datasets with a few lines of code!\n",
    "\n",
    "To get started with OpenCLIP, install the `open_clip_torch` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install open_clip_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running inference with [OpenCLIP](https://github.com/mlfoundations/open_clip), you can specify a text prompt to help guide the model towards a solution as well as only specify a certain number of classes to output during zero shot classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "open_clip_model = foz.load_zoo_model(\n",
    "    name_or_url=\"open-clip-torch\",\n",
    "    text_prompt=\"A photo of a\",\n",
    "    classes=dataset_classes,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # install_requirements=True # uncomment this line if you are running this code for the first time\n",
    ")\n",
    "\n",
    "dataset.apply_model(open_clip_model, label_field=\"open_clip_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Classification: {\n",
       "    'id': '67d9de8e99e7fb132bafa2d4',\n",
       "    'tags': [],\n",
       "    'label': 'mousetrap',\n",
       "    'confidence': nan,\n",
       "    'logits': None,\n",
       "}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.first()['open_clip_classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify different model architectures and pretrained weights by passing in optional parameters. Pretrained models can be loaded directly from OpenCLIP with the following syntax:\n",
    "\n",
    "\n",
    "```python\n",
    "meta_clip = foz.load_zoo_model(\n",
    "    name_or_url=\"open-clip-torch\",\n",
    "    clip_model=\"ViT-B-32-quickgelu\",\n",
    "    pretrained=\"metaclip_400m\",\n",
    "    text_prompt=\"A photo of a\",\n",
    "    classes=dataset_classes,\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "Alternatively you can also load a model from Hugging Face’s Model Hub with the following syntax:\n",
    "\n",
    "\n",
    "```python\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "open_clip_model = foz.load_zoo_model(\n",
    "    name_or_url=\"open-clip-torch\",\n",
    "    clip_model=\"hf-hub:repo-name/model-name\",\n",
    "    pretrained=\"\",\n",
    ")\n",
    "```\n",
    "\n",
    "As a concrete example, if you were interested in the [StreetCLIP model](https://huggingface.co/geolocal/StreetCLIP) you would use:\n",
    "\n",
    "```python\n",
    "street_clip_model = foz.load_zoo_model(\n",
    "    name_or_url=\"open-clip-torch\",\n",
    "    pretrained=\"\",\n",
    "    clip_model=\"hf-hub:geolocal/StreetCLIP\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Integration\n",
    "\n",
    "\n",
    "You can also run models from Hugging Face as a Zoo Model with [FiftyOne's Hugging Face Integration](https://beta-docs.voxel51.com/integrations/huggingface/#zero-shot-classification). Note: These models must be fully integrated in the [Hugging Face transformers](github.com/huggingface/transformers) library, some model weights may be available via Hugging Face but not fully integrated into the transformers library.\n",
    "\n",
    "To load a model from the Hugging Face Hub, set `name_or_url=zero-shot-classification-transformer-torch`. This specifies that you want to a zero-shot image classification model from the Hugging Face Transformers library. You can then specify the model via the `name_or_path` argument. This should be the repository name or model identifier of the model you want to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "siglip_model = foz.load_zoo_model(\n",
    "    name_or_url=\"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"google/siglip2-so400m-patch14-384\",\n",
    "    classes=dataset_classes,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # install_requirements=True # uncomment this line if you are running this code for the first time\n",
    "    )\n",
    "\n",
    "dataset.apply_model(siglip_model, label_field=\"siglip2_classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the output for the first Sample as shown below. Note that not all models will output a value for `confidence` or `logits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Classification: {\n",
       "    'id': '67d9dff499e7fb132bafb274',\n",
       "    'tags': [],\n",
       "    'label': 'frying pan',\n",
       "    'confidence': 0.10734604299068451,\n",
       "    'logits': array([-14.63465  , -16.027546 , -14.841102 , -15.603795 , -15.634769 ,\n",
       "           -26.755354 , -28.286345 , -28.364231 , -27.1351   , -26.7131   ,\n",
       "           -27.507458 , -26.499924 , -21.994976 , -13.691553 , -28.55092  ,\n",
       "           -25.564491 , -14.591154 , -26.281277 , -26.274792 , -13.6278   ,\n",
       "           -14.481212 , -24.684128 , -24.921196 , -28.01366  , -24.428066 ,\n",
       "           -26.221783 , -24.934784 , -27.947395 , -28.367542 , -13.191751 ,\n",
       "           -26.12117  , -27.978542 , -21.961689 , -13.579732 , -26.58063  ,\n",
       "           -13.924773 , -27.960869 , -13.0043545, -26.122686 , -20.667011 ,\n",
       "           -22.799128 , -20.430523 , -15.019728 , -27.692225 , -27.954025 ,\n",
       "           -12.913255 , -27.146004 , -27.823196 , -28.423857 , -23.00042  ,\n",
       "           -23.592287 , -25.413513 , -26.26255  , -15.5457735, -25.749825 ,\n",
       "           -24.848305 , -28.531633 , -26.165901 , -23.151388 , -19.266499 ,\n",
       "           -24.093252 , -14.020183 , -26.386292 , -28.239845 , -22.07525  ,\n",
       "           -27.582024 , -25.223106 , -19.29158  , -12.56849  , -25.734182 ,\n",
       "           -22.642479 , -24.389975 , -24.530643 , -23.746538 , -27.28984  ,\n",
       "           -25.155281 , -25.74003  , -19.188692 , -24.167593 , -24.387491 ,\n",
       "           -26.841904 , -23.65986  , -13.781616 , -26.865953 , -28.562445 ,\n",
       "           -14.650488 , -27.812248 , -23.470142 , -28.144627 , -28.123253 ,\n",
       "           -30.08646  , -24.829483 , -15.422783 , -26.38512  , -22.980677 ,\n",
       "           -12.935882 , -24.881578 , -26.172321 , -25.142155 , -27.739046 ,\n",
       "           -28.699215 , -28.724304 , -28.418411 , -27.454609 , -22.694475 ,\n",
       "           -25.072674 , -28.214672 , -14.173061 , -13.863817 , -19.484104 ,\n",
       "           -28.667864 , -25.375149 , -26.225508 , -24.78184  , -26.520378 ,\n",
       "           -23.774696 , -27.95382  , -28.486088 , -21.505005 , -22.355341 ,\n",
       "           -23.349148 , -24.033804 , -23.604559 , -13.987801 , -13.772884 ,\n",
       "           -28.77172  , -23.77694  , -27.695286 , -13.899179 , -24.19553  ,\n",
       "           -28.618416 , -23.60842  , -16.364819 , -13.776959 , -13.612553 ,\n",
       "           -28.31878  , -20.912731 , -24.29652  , -25.350151 , -28.747776 ,\n",
       "           -14.814828 , -13.303535 , -28.47046  , -24.823593 , -28.176216 ,\n",
       "           -22.470306 , -20.222847 , -28.176281 , -29.118187 , -20.819706 ,\n",
       "           -23.281315 , -28.769344 , -27.104895 , -27.658867 , -25.080599 ,\n",
       "           -22.988195 , -24.258585 , -20.518099 , -28.228722 , -22.119791 ,\n",
       "           -23.093803 , -25.818127 , -19.867298 , -16.562128 , -22.246336 ,\n",
       "           -25.318657 , -24.268326 , -26.903309 , -26.76782  , -24.200016 ,\n",
       "           -28.868828 , -26.636438 , -24.618382 , -18.01135  , -22.788385 ,\n",
       "           -29.595188 , -29.231174 , -26.795033 , -22.59452  , -14.442874 ,\n",
       "           -14.756289 , -25.36583  , -20.68103  , -27.915737 , -19.589752 ,\n",
       "           -27.01284  , -24.279789 , -29.063992 , -27.855495 , -15.463411 ,\n",
       "           -25.507193 , -24.51913  , -22.107254 , -22.242912 , -23.621586 ,\n",
       "           -15.402868 , -24.746723 , -23.633163 , -28.656672 , -23.97926  ],\n",
       "          dtype=float32),\n",
       "}>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.first()['siglip2_classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any model that can be run in a Hugging Face pipeline for the `zero-shot-image-classification` task can be loaded as a Zoo model.\n",
    "\n",
    "A good first entry point is to just do it and pass the model name into `name_or_path` in the [`load_zoo_model`](https://beta-docs.voxel51.com/api/fiftyone.zoo.models.html#fiftyone.zoo.models.load_zoo_model) method of the dataset. If a Hugging Face model is not compatible with the integration, you'll see an error to the effect of: \n",
    "\n",
    "```python\n",
    "ValueError: Unrecognized model in <whatever-model-name>\n",
    "```\n",
    "\n",
    "In this case, you will need to run the model manually. All this means is that you need to instantiate the model, it's  processor, and write some logic to parse it the model output a [FiftyOne Classification](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Classification.html). \n",
    "\n",
    "Refer to [this documentation](https://beta-docs.voxel51.com/how_do_i/recipes/adding_classifications/) for details on how to manually parse model outputs as a [FiftyOne Classification](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Classification.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you've learned how to:\n",
    "- Implement zero-shot classification using multiple model architectures without needing to retrain models\n",
    "- Use three different approaches for zero-shot classification:\n",
    "  - FiftyOne's Model Zoo CLIP models\n",
    "  - OpenCLIP models with custom architectures\n",
    "  - Hugging Face Transformers integration\n",
    "- Customize text prompts to improve classification results\n",
    "- Apply models to a FiftyOne dataset and access the classification results\n",
    "\n",
    "### Key Takeaways\n",
    "- Zero-shot classification enables prediction of new categories without model retraining\n",
    "- Different model architectures and text prompts can significantly impact results\n",
    "- FiftyOne provides flexible integrations with popular model frameworks\n",
    "- Classification results are stored as standard FiftyOne Classifications, making them easy to analyze and evaluate\n",
    "\n",
    "\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Check out this [in-depth end-to-end tutorial](https://beta-docs.voxel51.com/tutorials/zero_shot_classification/) for Zero-Shot Classification which includes details on how to evaluate your results\n",
    "\n",
    "- Learn how to [evaluate classification](https://beta-docs.voxel51.com/fiftyone_concepts/evaluation/#classifications) results\n",
    "\n",
    "You might also be interested in reading these blogs:\n",
    "\n",
    "- [*This Visual Illusions Benchmark Makes Me Question the Power of VLMs*](https://voxel51.com/blog/this-visual-illusions-benchmark-makes-me-question-the-power-of-vlms/)\n",
    "\n",
    "- [*AIMv2 Outperforms CLIP on Synthetic Dataset ImageNet-D*](https://voxel51.com/blog/aimv2-outperforms-clip-on-synthetic-dataset-imagenet-d/)\n",
    "\n",
    "- [*A History of CLIP Model Training Data Advances*](https://voxel51.com/blog/a-history-of-clip-model-training-data-advances/)\n",
    "\n",
    "For more resources and updates, follow us on [LinkedIn](https://www.linkedin.com/company/voxel51/) or join our [Discord Community](https://community.voxel51.com/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
