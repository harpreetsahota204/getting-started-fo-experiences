{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who this is for\n",
    "This notebook is designed for:\n",
    "\n",
    "- Computer vision engineers with basic FiftyOne experience (can load datasets and use the App)\n",
    "\n",
    "- Practitioners interested in zero-shot computer vision approaches who may be new to segmentation tasks\n",
    "\n",
    "- Users looking to implement quick segmentation solutions without training custom models or creating labeled datasets\n",
    "\n",
    "## Assumed Knowledge\n",
    "\n",
    "### Computer Vision Concepts\n",
    "\n",
    "- Basic understanding of image segmentation (semantic, instance)\n",
    "\n",
    "- Familiarity with vision-language models and prompting\n",
    "\n",
    "- Understanding of coordinate systems in images\n",
    "\n",
    "### Technical Requirements\n",
    "\n",
    "- Intermediate Python programming skills\n",
    "\n",
    "- Experience with Jupyter notebooks\n",
    "\n",
    "- Basic understanding of PyTorch (for model usage)\n",
    "\n",
    "### FiftyOne Concepts\n",
    "You should be familiar with:\n",
    "- [Datasets and Samples](https://beta-docs.voxel51.com/getting_started/basics.html)\n",
    "- [The FiftyOne App](https://beta-docs.voxel51.com/getting_started/basic/application_tour/)\n",
    "- [Working with Labels](https://beta-docs.voxel51.com/user_guide/using_datasets.html#labels)\n",
    "- [The Model Zoo](https://beta-docs.voxel51.com/user_guide/model_zoo/index.html)\n",
    "\n",
    "## Time to Complete\n",
    "\n",
    "~45-60 minutes\n",
    "\n",
    "## Required Packages\n",
    "\n",
    "It's recommended to use a virtual environment with FiftyOne already installed. Additionally, you'll need:\n",
    "\n",
    "```bash\n",
    "# Install Florence2 plugin requirements\n",
    "fiftyone plugins download https://github.com/jacobmarks/fiftyone_florence2_plugin\n",
    "fiftyone plugins requirements @jacobmarks/florence2 --install\n",
    "\n",
    "# Install Moondream2 plugin requirements\n",
    "fiftyone plugins download https://github.com/harpreetsahota204/moondream2-plugin\n",
    "fiftyone plugins requirements @harpreetsahota/moondream2 --install\n",
    "\n",
    "# Install SAM2\n",
    "pip install \"git+https://github.com/facebookresearch/sam2.git#egg=sam-2\"\n",
    "\n",
    "# Install FastSAM\n",
    "pip install ultralytics\n",
    "```\n",
    "\n",
    "## Content Overview\n",
    "\n",
    "- **Zero-Shot Segmentation Introduction**: Understanding the basics of zero-shot segmentation and its applications\n",
    "\n",
    "- **Phrase Grounding Segmentation**: Using Florence2 to segment images based on text descriptions\n",
    "\n",
    "- **Moondream + SAM2 Integration**: Combining automatic keypoint detection with advanced segmentation\n",
    "\n",
    "- **FastSAM Implementation**: Using point-based prompts for quick and efficient segmentation\n",
    "\n",
    "\n",
    "# Zero-Shot Segmentation\n",
    "\n",
    "Zero-shot segmentation is a computer vision task that aims to segment objects or regions in images without any training samples for those specific categories. It enables models to perform instance, semantic, or panoptic segmentation for novel categories by transferring visual knowledge learned from seen categories to unseen ones. Prompt-based zero-shot segmentation uses prompts to guide the segmentation process at test time without requiring retraining for new categories. This approach allows a single trained model to handle various segmentation tasks dynamically.\n",
    "\n",
    "### Types of Prompts\n",
    "\n",
    "**Text Prompts**\n",
    "- Free-text descriptions that specify what to segment in an image\n",
    "- The model uses pre-trained knowledge of text-image relationships to identify and segment the described objects\n",
    "\n",
    "**Image Prompts**\n",
    "- Visual examples that show what to segment\n",
    "- Particularly useful when the target is difficult to describe in words\n",
    "- Can be a reference image containing the object of interest\n",
    "- The model compares the visual features between the prompt image and the target image to identify similar regions\n",
    "\n",
    "**Hybrid Approaches**\n",
    "- Some systems can accept either text or image prompts for the same model\n",
    "- CLIPSeg is an example of a model that works with both text and image prompts by adding a decoder to CLIP\n",
    "\n",
    "#### Let's begin by downloading a dataset from the FiftyOne [Dataset Zoo](https://beta-docs.voxel51.com/data/dataset_zoo/datasets/). \n",
    "\n",
    "You'll notice I have passed several arguments to the [`load_zoo_dataset`](https://beta-docs.voxel51.com/api/fiftyone.zoo.datasets.html#load_zoo_dataset) function:\n",
    "\n",
    "- `max_samples`: The [Dataset](https://beta-docs.voxel51.com/fiftyone_concepts/using_datasets/) will only be comprised of, at most, 25 [Samples](https://beta-docs.voxel51.com/getting_started/basic/datasets_samples_fields/)\n",
    "\n",
    "- `shuffle`: Randomize the [Samples](https://beta-docs.voxel51.com/api/fiftyone.core.sample.Sample.html) that are selected \n",
    "\n",
    "- `dataset_name`: Assign a [name](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#name) to the [Dataset](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"quickstart\", \n",
    "    max_samples=25,\n",
    "    shuffle=True,\n",
    "    dataset_name=\"mini_quickstart\",\n",
    "    overwrite=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need [metadata](https://beta-docs.voxel51.com/fiftyone_concepts/using_datasets/#storing-field-metadata), such as each Sample's height and width later, so we can use the [`compute_metadata`](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#compute_metadata) method of the Dataset to accomplish this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the metadata for the first Sample looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first().metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase Grounding Segmentation\n",
    "\n",
    "Phrase grounding segmentation extends traditional phrase grounding by not only localizing objects mentioned in text but also generating pixel-level segmentation masks for those objects. While phrase grounding typically produces bounding boxes around regions corresponding to textual phrases, phrase grounding segmentation aims to create fine-grained segmentation masks that precisely delineate the boundaries of the referenced objects.\n",
    "\n",
    "This approach enables more precise visual understanding by:\n",
    "- Associating specific words or phrases with their corresponding image regions\n",
    "- Generating pixel-accurate segmentation masks rather than just bounding boxes\n",
    "- Creating a more detailed alignment between language and visual content\n",
    "\n",
    "\n",
    "## Plugins\n",
    "\n",
    "[FiftyOne plugins](https://beta-docs.voxel51.com/plugins/) are powerful extensions that allow users to customize and enhance the functionality of the [FiftyOne App](https://beta-docs.voxel51.com/getting_started/basic/application_tour/). \n",
    "\n",
    "[Plugins can be written in Python, JavaScript, or a combination of both](https://beta-docs.voxel51.com/plugins/developing_plugins/), enabling users to add new features, create integrations with other tools and APIs, render custom panels, and add custom actions to menus. They are composed of panels, operators, and components, which together allow for building full-featured interactive data applications tailored to specific use cases. Plugins can range from simple actions like adding a checkbox to complex workflows such as requesting annotations from a configurable backend. This extensibility makes FiftyOne highly adaptable to various computer vision tasks and workflows, limited only by the user's imagination.\n",
    "\n",
    "We'll use the Plugin framework via the FiftyOne SDK, and you can [refer to the docs on using the Plugin Frame in the FiftyOne App](https://beta-docs.voxel51.com/plugins/using_plugins/)\n",
    "\n",
    "### Florence2 Plugin\n",
    "\n",
    "The [Florence2 Plugin](https://github.com/jacobmarks/fiftyone_florence2_plugin) integrates Microsoft's Florence2 Vision-Language Model with FiftyOne datasets, offering several powerful computer vision capabilities.\n",
    "\n",
    "One of these tasks is referring segmentation, which allows you to segment specific regions in an image based on natural language descriptions. This is particularly useful when you need to segment specific parts of an image based on textual descriptions, allowing for region identification using natural language. It can be used in two ways:\n",
    "\n",
    "• Using a direct expression through the `expression` parameter\n",
    "\n",
    "• Using an existing expression field in your dataset via the `expression_field` parameter\n",
    "\n",
    "Note: Referring segmentation is a hard task in Visual AI, and as powerful as the Florence2 model is, the results are not always the best. It's a good idea to be precise with your open vocabulary prompt and use the shortest caption possible for each Sample.\n",
    "\n",
    "Let's start by setting an enviornment variable, [downloading the plugin, and installing it's requirements](https://beta-docs.voxel51.com/plugins/using_plugins/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment variable\n",
    "import os\n",
    "os.environ['FIFTYONE_ALLOW_LEGACY_ORCHESTRATORS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the plugin\n",
    "!fiftyone plugins download https://github.com/jacobmarks/fiftyone_florence2_plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requirements for the plugin\n",
    "!fiftyone plugins requirements @jacobmarks/florence2 --install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the plugin installed, you can instantiate the [Operator](https://beta-docs.voxel51.com/plugins/using_plugins/#using-operators) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "MODEL_PATH =\"microsoft/Florence-2-large-ft\"\n",
    "\n",
    "florence2_referring_segmentation = foo.get_operator(\"@jacobmarks/florence2/referring_expression_segmentation_with_florence2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the operator, you will need to start a [Delegated service](https://beta-docs.voxel51.com/plugins/developing_plugins/#delegated-execution_1) by opening your terminal and running the following command:\n",
    "\n",
    "```bash\n",
    "fiftyone delegated launch\n",
    "```\n",
    "\n",
    "Then, you can [call the Operator](https://beta-docs.voxel51.com/plugins/using_plugins/#calling-operators) by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await florence2_referring_segmentation(\n",
    "    dataset,\n",
    "    model_path=MODEL_PATH,\n",
    "    expression=\"human\",\n",
    "    output_field=\"open_expression_segmentations\",\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the results of the first Sample like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['open_expression_segmentations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use [the Florence2 Plugin](https://github.com/jacobmarks/fiftyone_florence2_plugin) when you have existing captions on your dataset. We don't have those here, so let's generate these captions and then use that for segmentation. Start by instantiating the Operator for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "florence2_captioning = foo.get_operator(\"@jacobmarks/florence2/caption_with_florence2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And calling the operator, as you've done previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await florence2_captioning(\n",
    "    dataset,\n",
    "    model_path=MODEL_PATH,\n",
    "    detail_level=\"basic\",\n",
    "    output_field=\"basic_caption\",\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['basic_caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await florence2_referring_segmentation(\n",
    "    dataset,\n",
    "    model_path=MODEL_PATH,\n",
    "    expression_field=\"basic_caption\", #must be a field on your dataset\n",
    "    output_field=\"expression_field_segmentations\",\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['expression_field_segmentations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moondream + SAM2\n",
    "\n",
    "This next workflow will show you how to leverage the [Moondream2 plugin](https://github.com/harpreetsahota204/moondream2-plugin) for FiftyOne alongside [SAM2 from the FiftyOne Model Zoo](https://voxel51.com/blog/sam-2-is-now-available-in-fiftyone/) for zero-shot segmentation. \n",
    "\n",
    "The process works by first using Moondream2 to automatically analyze your images and generate [Keypoints](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) of interest in a zero-shot fashion, requiring no training data or manual annotation. These points then serve as a prompt for SAM2, which uses them to generate segmentation masks around the detected objects or regions. \n",
    "\n",
    "First, install the Moondream2 plugin and it's requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the plugin from the github repository\n",
    "!fiftyone plugins download https://github.com/harpreetsahota204/moondream2-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requirements for the plugin\n",
    "!fiftyone plugins requirements @harpreetsahota/moondream2 --install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the plugin installed, you can instantiate the [Operator](https://beta-docs.voxel51.com/plugins/using_plugins/#using-operators) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "moondream_operator = foo.get_operator(\"@harpreetsahota/moondream2/moondream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the operator, you will need to start a [Delegated service](https://beta-docs.voxel51.com/plugins/developing_plugins/#delegated-execution_1) by opening your terminal and running the following command:\n",
    "\n",
    "```bash\n",
    "fiftyone delegated launch\n",
    "```\n",
    "\n",
    "Then, you can [call the Operator](https://beta-docs.voxel51.com/plugins/using_plugins/#calling-operators) by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await moondream_operator(\n",
    "    dataset,\n",
    "    revision=\"2025-01-09\",\n",
    "    operation=\"point\",\n",
    "    output_field=\"moondream_point\",\n",
    "    delegate=True,\n",
    "    object_type=\"person\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [`reload` method](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#reload) of the Dataset to reload any in-memory samples from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use SAM2 from the FiftyOne Model Zoo you need to first install its dependencies. You can do so by running the following command:\n",
    "\n",
    "`pip install \"git+https://github.com/facebookresearch/sam2.git#egg=sam-2`\n",
    "\n",
    "\n",
    "Please [refer to the SAM2 GitHub repo](https://github.com/facebookresearch/sam2) for details and any troubleshooting. You can [refer to the Model Zoo documentation](https://beta-docs.voxel51.com/models/model_zoo/models/#med-sam-2-video-torch_1) for more information about which checkpoints are available in the FiftyOne Model Zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam2_model = foz.load_zoo_model(\"segment-anything-2.1-hiera-base-plus-image-torch\")\n",
    "\n",
    "dataset.apply_model(\n",
    "    sam2_model,\n",
    "    label_field=\"sam_segmentations\",\n",
    "    prompt_field=\"moondream_point\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting with Keypoints\n",
    "\n",
    "When working with keypoints in FiftyOne and Hugging Face segmentation models, you need to perform some conversion. \n",
    "\n",
    "FiftyOne's [Keypoint class](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) stores points in normalized coordinates within the [0,1] x [0,1] range, regardless of image dimensions. This normalization enables consistent representation across images of different sizes.\n",
    "\n",
    "When a model requires absolute pixel coordinates to generate segmentation masks, you'll need to perform coordinate conversion when moving between these systems. Feeding those points to a segmentation model requires transforming the normalized coordinates back to absolute pixel values using the image's actual dimensions from its metadata.\n",
    "\n",
    "### How to parse segmentation masks that are point coordinates\n",
    "#### FastSAM from Ultralytics \n",
    "\n",
    "\n",
    "[FastSAM](https://docs.ultralytics.com/models/fast-sam) outputs segmentation masks as normalized coordinate arrays. Each mask is represented as an array of (x,y) coordinates defining the boundary of a detected object. These coordinates are normalized to [0,1] range and stored in NumPy arrays.\n",
    "\n",
    "When working with segmentation models that output point coordinates (boundary points of objects), here's what you need to know to display them in FiftyOne:\n",
    "\n",
    "* Model outputs (often NumPy arrays or tensors) need to be converted to standard Python lists of coordinates.\n",
    "\n",
    "* Ensure coordinates are normalized to [0,1] range if they aren't already.\n",
    "\n",
    "* FiftyOne's Polyline expects a specific nesting structure - your points must be organized as a list of shapes, where each shape is a list of points.\n",
    "\n",
    "* Specify that your polylines should be closed (connecting last point to first) and filled to properly represent segmentation masks.\n",
    "\n",
    "* Store your polylines in a Polylines field to enable proper visualization in the FiftyOne UI.\n",
    "\n",
    "Note: FastSAM can accept text input and boundin boxes as prompts. Refer to the [Ultralytics documentation](https://docs.ultralytics.com/models/fast-sam) to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import FastSAM\n",
    "import fiftyone as fo\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load FastSAM model\n",
    "model = FastSAM(\"FastSAM-s.pt\")\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Process samples in dataset\n",
    "for sample in dataset.iter_samples(progress=True):\n",
    "    # Skip samples without keypoints\n",
    "    if not hasattr(sample, \"moondream_point\") or not sample.moondream_point.keypoints:\n",
    "        continue\n",
    "        \n",
    "    # Get image path and dimensions\n",
    "    image_path = sample.filepath\n",
    "    image_width = sample.metadata.width\n",
    "    image_height = sample.metadata.height\n",
    "    \n",
    "    # Process all keypoints in the sample\n",
    "    all_keypoints = sample.moondream_point.keypoints\n",
    "    \n",
    "    # Collect all points from all keypoint objects\n",
    "    all_pixel_points = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for keypoint_obj in all_keypoints:\n",
    "        points = keypoint_obj.points\n",
    "        # Convert to pixel coordinates\n",
    "        pixel_points = [\n",
    "            [int(point[0] * image_width), int(point[1] * image_height)]\n",
    "            for point in points\n",
    "        ]\n",
    "        all_pixel_points.extend(pixel_points)\n",
    "        all_labels.extend([1] * len(pixel_points))  # 1 for foreground\n",
    "    \n",
    "    # Run inference with all points\n",
    "    results = model(image_path, \n",
    "                    device=device, \n",
    "                    retina_masks=True,\n",
    "                    points=all_pixel_points, \n",
    "                    labels=all_labels,\n",
    "                    conf=0.51,\n",
    "                    iou=0.51\n",
    "                    )\n",
    "    \n",
    "    result = results[0]\n",
    "    \n",
    "    # Check if masks were generated\n",
    "    if hasattr(result, 'masks') and result.masks:\n",
    "        masks = result.masks.xyn\n",
    "        \n",
    "        # Create polyline objects for each mask\n",
    "        polylines = []\n",
    "        \n",
    "        for mask in masks:\n",
    "            # Convert NumPy arrays to plain Python lists of floats with list comprehension\n",
    "            points_list = [[float(point[0]), float(point[1])] for point in mask]\n",
    "            \n",
    "            # Create polyline with correct nesting\n",
    "            polyline = fo.Polyline(\n",
    "                points=[points_list],  # Each mask is one shape\n",
    "                closed=True,\n",
    "                filled=True\n",
    "            )\n",
    "            \n",
    "            polylines.append(polyline)\n",
    "        \n",
    "        # Save to sample if we have valid polylines\n",
    "        if polylines:\n",
    "            polylines_field = fo.Polylines(polylines=polylines)\n",
    "            sample[\"fastsam_segmentation\"] = polylines_field\n",
    "            sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, [we are calling `sample.save()` after adding predictions to each Sample](https://beta-docs.voxel51.com/faq/#why-didnt-changes-to-my-dataset-save). This method persists your changes to the FiftyOne database, ensuring that your generated segmentation masks are stored and accessible for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reload()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['fastsam_segmentation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you've learned how to:\n",
    "\n",
    "- Implement zero-shot segmentation using different approaches without training custom models\n",
    "\n",
    "- Use text prompts with Florence2 for phrase grounding segmentation\n",
    "\n",
    "- Combine Moondream2's automatic keypoint detection with SAM2 for efficient segmentation\n",
    "\n",
    "- Leverage FastSAM for point-based segmentation tasks\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Zero-shot segmentation enables quick implementation of segmentation tasks without labeled training data\n",
    "\n",
    "- Different prompting methods (text, points, hybrid) offer flexibility for various use cases\n",
    "\n",
    "- FiftyOne plugins can significantly extend your computer vision capabilities\n",
    "\n",
    "- Combining multiple models (like Moondream2 + SAM2) can create powerful workflows\n",
    "\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Check out some more [FiftyOne Plugins](https://beta-docs.voxel51.com/plugins/#getting-started)\n",
    "\n",
    "- Check out [SAM2](https://voxel51.com/blog/sam-2-is-now-available-in-fiftyone/) in the FiftyOne Model Zoo\n",
    "\n",
    "- Check out [MedSAM2](https://beta-docs.voxel51.com/models/model_zoo/models/#med-sam-2-video-torch_1) in the FiftyOne Model Zoo\n",
    "\n",
    "- Learn how to [evaluate segmentations](https://beta-docs.voxel51.com/fiftyone_concepts/evaluation/#semantic-segmentations) with the FiftyOne [Evaluation API](https://beta-docs.voxel51.com/fiftyone_concepts/evaluation/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
