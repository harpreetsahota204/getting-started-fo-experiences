{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Segmentation\n",
    "\n",
    "Zero-shot segmentation is a computer vision task that aims to segment objects or regions in images without any training samples for those specific categories. It enables models to perform instance, semantic, or panoptic segmentation for novel categories by transferring visual knowledge learned from seen categories to unseen ones. Prompt-based zero-shot segmentation uses prompts to guide the segmentation process at test time without requiring retraining for new categories. This approach allows a single trained model to handle various segmentation tasks dynamically.\n",
    "\n",
    "### Types of Prompts\n",
    "\n",
    "**Text Prompts**\n",
    "- Free-text descriptions that specify what to segment in an image\n",
    "- The model uses pre-trained knowledge of text-image relationships to identify and segment the described objects\n",
    "\n",
    "**Image Prompts**\n",
    "- Visual examples that show what to segment\n",
    "- Particularly useful when the target is difficult to describe in words\n",
    "- Can be a reference image containing the object of interest\n",
    "- The model compares the visual features between the prompt image and the target image to identify similar regions\n",
    "\n",
    "**Hybrid Approaches**\n",
    "- Some systems can accept either text or image prompts for the same model\n",
    "- CLIPSeg is an example of a model that works with both text and image prompts by adding a decoder to CLIP\n",
    "\n",
    "Let's begin by downloading a dataset from the FiftyOne [Dataset Zoo](https://beta-docs.voxel51.com/data/dataset_zoo/datasets/). \n",
    "\n",
    "You'll notice I have passed several arguments to the [`load_zoo_dataset`](https://beta-docs.voxel51.com/api/fiftyone.zoo.datasets.html#load_zoo_dataset) function:\n",
    "\n",
    "- `max_samples`: The [Dataset](https://beta-docs.voxel51.com/fiftyone_concepts/using_datasets/) will only be comprised of, at most, 25 [Samples](https://beta-docs.voxel51.com/getting_started/basic/datasets_samples_fields/)\n",
    "\n",
    "- `shuffle`: Randomize the [Samples](https://beta-docs.voxel51.com/api/fiftyone.core.sample.Sample.html) that are selected \n",
    "\n",
    "- `classes`: Load only Samples containing at least one instance of a specified class\n",
    "\n",
    "- `dataset_name`: Assign a [name](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#name) to the [Dataset](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing directory '/home/harpreet/fiftyone/quickstart'\n",
      "Downloading dataset to '/home/harpreet/fiftyone/quickstart'\n",
      "Downloading dataset...\n",
      " 100% |████|  187.5Mb/187.5Mb [882.5ms elapsed, 0s remaining, 212.5Mb/s]      \n",
      "Extracting dataset...\n",
      "Parsing dataset metadata\n",
      "Found 200 samples\n",
      "Dataset info written to '/home/harpreet/fiftyone/quickstart/info.json'\n",
      "Ignoring unsupported parameter 'classes' for importer type <class 'fiftyone.utils.data.importers.FiftyOneDatasetImporter'>\n",
      "Loading 'quickstart'\n",
      " 100% |███████████████████| 25/25 [241.3ms elapsed, 0s remaining, 104.3 samples/s]    \n",
      "Dataset 'mini_quickstart' created\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"quickstart\", \n",
    "    max_samples=25,\n",
    "    shuffle=True,\n",
    "    classes=['person', 'car', 'train'],\n",
    "    dataset_name=\"mini_quickstart\",\n",
    "    overwrite=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need [metadata](https://beta-docs.voxel51.com/fiftyone_concepts/using_datasets/#storing-field-metadata), such as each Sample's height and width later, so we can use the [`compute_metadata`](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#compute_metadata) method of the Dataset to accomplish this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metadata...\n",
      " 100% |███████████████████| 25/25 [12.9ms elapsed, 0s remaining, 1.9K samples/s] \n"
     ]
    }
   ],
   "source": [
    "dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the metadata for the first Sample looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ImageMetadata: {\n",
       "    'size_bytes': 108669,\n",
       "    'mime_type': 'image/jpeg',\n",
       "    'width': 600,\n",
       "    'height': 400,\n",
       "    'num_channels': 3,\n",
       "}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.first().metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase Grounding Segmentation\n",
    "\n",
    "Phrase grounding segmentation extends traditional phrase grounding by not only localizing objects mentioned in text but also generating pixel-level segmentation masks for those objects. While phrase grounding typically produces bounding boxes around regions corresponding to textual phrases, phrase grounding segmentation aims to create fine-grained segmentation masks that precisely delineate the boundaries of the referenced objects.\n",
    "\n",
    "This approach enables more precise visual understanding by:\n",
    "- Associating specific words or phrases with their corresponding image regions\n",
    "- Generating pixel-accurate segmentation masks rather than just bounding boxes\n",
    "- Creating a more detailed alignment between language and visual content\n",
    "\n",
    "\n",
    "## Plugins\n",
    "\n",
    "[FiftyOne plugins](https://beta-docs.voxel51.com/plugins/) are powerful extensions that allow users to customize and enhance the functionality of the [FiftyOne App](https://beta-docs.voxel51.com/getting_started/basic/application_tour/). \n",
    "\n",
    "[Plugins can be written in Python, JavaScript, or a combination of both](https://beta-docs.voxel51.com/plugins/developing_plugins/), enabling users to add new features, create integrations with other tools and APIs, render custom panels, and add custom actions to menus. They are composed of panels, operators, and components, which together allow for building full-featured interactive data applications tailored to specific use cases. Plugins can range from simple actions like adding a checkbox to complex workflows such as requesting annotations from a configurable backend. This extensibility makes FiftyOne highly adaptable to various computer vision tasks and workflows, limited only by the user's imagination.\n",
    "\n",
    "We'll use the Plugin framework via the FiftyOne SDK, and you can [refer to the docs on using the Plugin Frame in the FiftyOne App](https://beta-docs.voxel51.com/plugins/using_plugins/)\n",
    "\n",
    "### Florence2 Plugin\n",
    "\n",
    "The [Florence2 Plugin](https://github.com/jacobmarks/fiftyone_florence2_plugin) integrates Microsoft's Florence2 Vision-Language Model with FiftyOne datasets, offering several powerful computer vision capabilities.\n",
    "\n",
    "One of these tasks is referring segmentation, which allows you to segment specific regions in an image based on natural language descriptions. This is particularly useful when you need to segment specific parts of an image based on textual descriptions, allowing for region identification using natural language. It can be used in two ways:\n",
    "\n",
    "• Using a direct expression through the `expression` parameter\n",
    "\n",
    "• Using an existing expression field in your dataset via the `expression_field` parameter\n",
    "\n",
    "Note: Referring segmentation is a hard task in Visual AI, and as powerful as the Florence2 model is, the results are not always the best. It's a good idea to be precise with your open vocabulary prompt and use the shortest caption possible for each Sample.\n",
    "\n",
    "Let's start by setting an enviornment variable, [downloading the plugin, and installing it's requirements](https://beta-docs.voxel51.com/plugins/using_plugins/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment variable\n",
    "import os\n",
    "os.environ['FIFTYONE_ALLOW_LEGACY_ORCHESTRATORS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the plugin\n",
    "!fiftyone plugins download https://github.com/jacobmarks/fiftyone_florence2_plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requirements for the plugin\n",
    "!fiftyone plugins requirements @jacobmarks/florence2 --install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the plugin installed, you can instantiate the [Operator](https://beta-docs.voxel51.com/plugins/using_plugins/#using-operators) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "MODEL_PATH =\"microsoft/Florence-2-large-ft\"\n",
    "\n",
    "florence2_referring_segmentation = foo.get_operator(\"@jacobmarks/florence2/referring_expression_segmentation_with_florence2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the operator, you will need to start a [Delegated service](https://beta-docs.voxel51.com/plugins/developing_plugins/#delegated-execution_1) by opening your terminal and running the following command:\n",
    "\n",
    "```bash\n",
    "fiftyone delegated launch\n",
    "```\n",
    "\n",
    "Then, you can [call the Operator](https://beta-docs.voxel51.com/plugins/using_plugins/#calling-operators) by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await florence2_referring_segmentation(\n",
    "    dataset,\n",
    "    model_path=MODEL_PATH,\n",
    "    expression=\"human\",\n",
    "    output_field=\"open_expression_segmentations\",\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the results of the first Sample like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['open_expression_segmentations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use [the Florence2 Plugin](https://github.com/jacobmarks/fiftyone_florence2_plugin) when you have existing captions on your dataset. We don't have those here, so let's generate these captions and then use that for segmentation. Start by instantiating the Operator for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "florence2_captioning = foo.get_operator(\"@jacobmarks/florence2/caption_with_florence2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And calling the operator, as you've done previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await florence2_captioning(\n",
    "    dataset,\n",
    "    model_path=MODEL_PATH,\n",
    "    detail_level=\"basic\",\n",
    "    output_field=\"basic_caption\",\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['basic_caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await florence2_referring_segmentation(\n",
    "    dataset,\n",
    "    model_path=MODEL_PATH,\n",
    "    expression_field=\"basic_caption\", #must be a field on your dataset\n",
    "    output_field=\"expression_field_segmentations\",\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['expression_field_segmentations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moondream + SAM2\n",
    "\n",
    "This next workflow will show you how to leverage the [Moondream2 plugin](https://github.com/harpreetsahota204/moondream2-plugin) for FiftyOne alongside [SAM2 from the FiftyOne Model Zoo](https://voxel51.com/blog/sam-2-is-now-available-in-fiftyone/) for zero-shot segmentation. \n",
    "\n",
    "The process works by first using Moondream2 to automatically analyze your images and generate [Keypoints](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Keypoint.html) of interest in a zero-shot fashion, requiring no training data or manual annotation. These points then serve as a prompt for SAM2, which uses them to generate segmentation masks around the detected objects or regions. \n",
    "\n",
    "First, install the Moondream2 plugin and it's requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the plugin from the github repository\n",
    "!fiftyone plugins download https://github.com/harpreetsahota204/moondream2-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requirements for the plugin\n",
    "!fiftyone plugins requirements @harpreetsahota/moondream2 --install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the plugin installed, you can instantiate the [Operator](https://beta-docs.voxel51.com/plugins/using_plugins/#using-operators) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "moondream_operator = foo.get_operator(\"@harpreetsahota/moondream2/moondream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the operator, you will need to start a [Delegated service](https://beta-docs.voxel51.com/plugins/developing_plugins/#delegated-execution_1) by opening your terminal and running the following command:\n",
    "\n",
    "```bash\n",
    "fiftyone delegated launch\n",
    "```\n",
    "\n",
    "Then, you can [call the Operator](https://beta-docs.voxel51.com/plugins/using_plugins/#calling-operators) by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await moondream_operator(\n",
    "    dataset,\n",
    "    revision=\"2025-01-09\",\n",
    "    operation=\"point\",\n",
    "    output_field=\"moondream_point\",\n",
    "    delegate=True,\n",
    "    object_type=\"people\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use SAM2 from the FiftyOne Model Zoo you need to first install its dependencies. You can do so by running the following command:\n",
    "\n",
    "`pip install \"git+https://github.com/facebookresearch/sam2.git#egg=sam-2`\n",
    "\n",
    "\n",
    "Please [refer to the SAM2 GitHub repo](https://github.com/facebookresearch/sam2) for details and any troubleshooting. You can [refer to the Model Zoo documentation](https://beta-docs.voxel51.com/models/model_zoo/models/#med-sam-2-video-torch_1) for more information about which checkpoints are available in the FiftyOne Model Zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = foz.load_zoo_model(\"segment-anything-2.1-hiera-base-plus-image-torch\")\n",
    "\n",
    "dataset.apply_model(\n",
    "    model,\n",
    "    label_field=\"sam_segmentations\",\n",
    "    prompt_field=\"moondream_point\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Hugging Face Pipeline\n",
    "\n",
    "\n",
    "#### With a KeyPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import SamModel, SamProcessor\n",
    "import fiftyone.utils.transformers as fout\n",
    "\n",
    "# Set device based on availability\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# Initialize model and processor\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\", device_map=device)\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# Process samples in dataset\n",
    "for sample in dataset.iter_samples(progress=True):\n",
    "    # Load image\n",
    "    image = Image.open(sample.filepath)\n",
    "    \n",
    "    # Convert normalized FiftyOne keypoints [0,1] to pixel coordinates\n",
    "    image_width = sample.metadata[\"width\"]\n",
    "    image_height = sample.metadata[\"height\"]\n",
    "    \n",
    "    # Access points from the Keypoint object\n",
    "    keypoint_obj = sample.moondream_point  # or whatever field name contains the Keypoint\n",
    "    \n",
    "    # Convert all points to pixel coordinates\n",
    "    pixel_points = [\n",
    "        [\n",
    "            int(point[0] * image_width),  # x coordinate\n",
    "            int(point[1] * image_height)  # y coordinate\n",
    "        ]\n",
    "        for point in keypoint_obj.points\n",
    "    ]\n",
    "    \n",
    "    # Format points for SAM (needs list of lists of points)\n",
    "    input_points = [pixel_points]  # Single list of multiple points\n",
    "    \n",
    "    # Prepare inputs and move to appropriate device\n",
    "    inputs = processor(\n",
    "        image, \n",
    "        input_points=input_points, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Post-process masks\n",
    "    masks = processor.image_processor.post_process_masks(\n",
    "        outputs.pred_masks.cpu(),  # Move tensors back to CPU for processing\n",
    "        inputs[\"original_sizes\"].cpu(),\n",
    "        inputs[\"reshaped_input_sizes\"].cpu()\n",
    "    )\n",
    "    \n",
    "    # Convert masks to FiftyOne format and store predictions\n",
    "    sample[\"seg_predictions\"] = fout.to_segmentation(masks)\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggging Face Pipeline \n",
    "\n",
    "#### Without a KeyPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"mask-generation\", model=\"Zigeng/SlimSAM-uniform-50\", points_per_batch=64, device=\"cuda\")\n",
    "image_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n",
    "outputs = generator(image_url)\n",
    "masks = outputs[\"masks\"]\n",
    "# array of multiple binary masks returned for each generated mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.transformers as fout\n",
    "\n",
    "fout.to_segmentation(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbitrary Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPSegConfig, CLIPSegModel\n",
    "\n",
    "# Initializing a CLIPSegConfig with CIDAS/clipseg-rd64 style configuration\n",
    "configuration = CLIPSegConfig()\n",
    "\n",
    "# Initializing a CLIPSegModel (with random weights) from the CIDAS/clipseg-rd64 style configuration\n",
    "model = CLIPSegModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "\n",
    "# We can also initialize a CLIPSegConfig from a CLIPSegTextConfig and a CLIPSegVisionConfig\n",
    "\n",
    "# Initializing a CLIPSegText and CLIPSegVision configuration\n",
    "config_text = CLIPSegTextConfig()\n",
    "config_vision = CLIPSegVisionConfig()\n",
    "\n",
    "config = CLIPSegConfig.from_text_vision_configs(config_text, config_vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, CLIPSegTextModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "model = CLIPSegTextModel.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "\n",
    "inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output  # pooled (EOS token) states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Check out some more [FiftyOne Plugins](https://beta-docs.voxel51.com/plugins/#getting-started)\n",
    "\n",
    "- Check out SAM2 in the FiftyOne Model Zoo\n",
    "\n",
    "- Learn how to evaluate segmentations with the FiftyOne Evaluation API\n",
    "\n",
    "- https://beta-docs.voxel51.com/models/model_zoo/models/#med-sam-2-video-torch_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
