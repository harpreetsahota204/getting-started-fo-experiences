{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\"quickstart-geo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_classes = [\n",
    "    \"yellow cab\",\n",
    "    \"sedan\",\n",
    "    \"coupe\",\n",
    "    \"hatchback\",\n",
    "    \"SUV\",\n",
    "    \"pickup truck\",\n",
    "    \"station wagon\",\n",
    "    \"crossover\",\n",
    "    \"minivan\",\n",
    "    \"green light\",\n",
    "    \"red light\",\n",
    "    \"illuminated tail lights\",\n",
    "    \"illuminated head lights\",\n",
    "    \"tow truck\",\n",
    "    \"parking meter\",\n",
    "    \"traffic barrier\",\n",
    "    \"traffic cone\",\n",
    "    \"bus stop\",\n",
    "    \"storefront\",\n",
    "    \"construction vehicle\",\n",
    "    \"municipal bus\",\n",
    "    \"charter bus\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo\n",
    "\n",
    "The FiftyOne Model Zoo provides a powerful interface for downloading models and applying them to your FiftyOne datasets.\n",
    "\n",
    "It provides native access to hundreds of pre-trained models, and it also supports downloading arbitrary public or private models whose definitions are provided via GitHub repositories or URLs.\n",
    "\n",
    "In fact, the [Model Zoo](https://beta-docs.voxel51.com/models/model_zoo/) is so flexible that you can natively load certain Hugging Face Transformers models and Ultralytics models for zero-shot object detection as a Zoo model via the [`load_zoo_model`](https://beta-docs.voxel51.com/api/fiftyone.zoo.models.html#load_zoo_model) method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Integration\n",
    "\n",
    "FiftyOne integrates with [Hugging Face's Transformers](https://beta-docs.voxel51.com/integrations/huggingface/#zero-shot-object-detection) library for Zero Shot Detection models. This allows you to load a Transformers model as a [Zoo Model](https://beta-docs.voxel51.com/models/model_zoo/).\n",
    "\n",
    "\n",
    "To load a model from the Hugging Face Hub, set `name_or_url=zero-shot-detection-transformer-torch`. This specifies that you want to a zero-shot object detection model from the Hugging Face Transformers library. You can then specify the model via the `name_or_path` argument. This should be the repository name or model identifier of the model you want to load.\n",
    "\n",
    "\n",
    "Note: the `confidence_thresh` parameter is optional and can be used to filter out predictions with confidence scores below the specified threshold. You may need to adjust this value based on the model and dataset you are working. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "owlvit = foz.load_zoo_model(\n",
    "    \"zero-shot-detection-transformer-torch\",\n",
    "    text_prompt=\"a photo of a \", # per the model card\n",
    "    name_or_path=\"google/owlvit-base-patch32\",  # HF model name or path\n",
    "    classes=detection_classes,\n",
    "    device=device,\n",
    "    confidence_thresh=0.2\n",
    "    # install_requirements=True # uncomment to install the necessary requirements\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply_model(\n",
    "    owlvit, \n",
    "    label_field=\"owlvit_detections\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any model that can be run in a Hugging Face pipeline for the `zero-shot-object-detection` task can be loaded as a Zoo model.\n",
    "\n",
    "A good first entry point is to just do it and pass the model name into `name_or_path` in the [`load_zoo_model`](https://beta-docs.voxel51.com/api/fiftyone.zoo.models.html#fiftyone.zoo.models.load_zoo_model) method of the dataset. If a Hugging Face model is not compatible with the integration, you'll see an error to the effect of: \n",
    "\n",
    "```python\n",
    "ValueError: Unrecognized model in <whatever-model-name>\n",
    "```\n",
    "\n",
    "In this case, you will need to run the model manually. All this means is that you need to instantiate the model, it's  processor, and write some logic to parse the model output a [FiftyOne Detection](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html). I'll show you how to this later on in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultralytics\n",
    "\n",
    "FiftyOne [integrates natively with Ultralytics](https://beta-docs.voxel51.com/integrations/ultralytics/), so you can load, fine-tune, and run inference with your favorite Ultralytics models on your FiftyOne datasets with just a few lines of code.\n",
    "\n",
    "Check out the [documention for our Ultralytics integration](https://docs.voxel51.com/integrations/ultralytics.html#open-vocabulary-detection) if you're interested in manually using an Ultralytics model rather than as a Zoo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "yolo_world = foz.load_zoo_model(\n",
    "    \"yolov8s-world-torch\", \n",
    "    classes=detection_classes,\n",
    "    device=device,\n",
    "    confidence_thresh=0.2\n",
    "    # install_requirements=True # uncomment to install the necessary requirements\n",
    "    )\n",
    "\n",
    "dataset.apply_model(yolo_world, label_field=\"yolow_dets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbitrary Models\n",
    "\n",
    "Regardless of which zero-shot object detection model you use, the process of converting predictions to FiftyOne format follows the same general pattern:\n",
    "\n",
    "1. **Standardize Bounding Box Format**\n",
    "   - FiftyOne [Detection labels](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html) expects bounding boxes in relative coordinates [0,1]\n",
    "   - Format must be [top-left-x, top-left-y, width, height]\n",
    "   - Most models output absolute coordinates or different formats, so conversion is usually needed\n",
    "\n",
    "2. **Create Detection Objects**\n",
    "   - Each [Detection](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html) object needs three main components:\n",
    "     - `label`: the class name\n",
    "     - `bounding_box`: the normalized coordinates\n",
    "     - `confidence`: the detection score\n",
    "   - The individual Detection objects must be grouped into a [Detections](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detections.html) [Field for each Sample](https://beta-docs.voxel51.com/getting_started/basic/datasets_samples_fields/). \n",
    "\n",
    "3. **Batch Processing Strategy**\n",
    "   - Instead of updating samples one by one, collect all detections\n",
    "   - Use [`dataset.set_values()`](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#set_values) for efficient batch updates\n",
    "   - This is much faster than individual [`sample.save()`](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#save) calls\n",
    "\n",
    "The core workflow is:\n",
    "- Get model predictions\n",
    "- Convert coordinates to FiftyOne's expected format\n",
    "- Create [Detection](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html) objects\n",
    "- Group them into [Detections](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detections.html) objects (one per [Sample](https://beta-docs.voxel51.com/api/fiftyone.core.sample.Sample.html))\n",
    "- Batch update the [Dataset](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html)\n",
    "\n",
    "This pattern remains the same regardless of the model you're using, whether it's from the Hugging Face Hub, Torch Hub, or some brand new SOTA model that you can only use via it's GitHub Repo. The only part that changes is how you extract and convert the specific model's output into FiftyOne [Detection](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html) format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import fiftyone as fo\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, OmDetTurboForObjectDetection\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize model and processor\n",
    "processor = AutoProcessor.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n",
    "model = OmDetTurboForObjectDetection.from_pretrained(\n",
    "    \"omlab/omdet-turbo-swin-tiny-hf\",\n",
    "    device_map=device)\n",
    "\n",
    "filepaths = dataset.values(\"filepath\")\n",
    "\n",
    "all_detections = []\n",
    "for filepath in filepaths:\n",
    "    # Load and process image\n",
    "    image = Image.open(filepath)\n",
    "    height, width = image.size[::-1]  # Get dimensions in same format as target_sizes\n",
    "    \n",
    "    inputs = processor(image, text=detection_classes, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        text_labels=detection_classes,\n",
    "        target_sizes=[image.size[::-1]],  # Keep model's expected format\n",
    "        threshold=0.3,\n",
    "        nms_threshold=0.3,\n",
    "    )[0]\n",
    "    \n",
    "    scores = results[\"scores\"].cpu().numpy()\n",
    "    boxes = results[\"boxes\"].cpu().numpy()\n",
    "    text_labels = results[\"text_labels\"]\n",
    "    \n",
    "    detections = []\n",
    "    for score, class_name, box in zip(scores, text_labels, boxes):\n",
    "        x1, y1, x2, y2 = box\n",
    "        \n",
    "        # First normalize all coordinates by their respective dimensions (x/width, y/height)\n",
    "        x1 = x1 / width\n",
    "        y1 = y1 / height\n",
    "        x2 = x2 / width\n",
    "        y2 = y2 / height\n",
    "    \n",
    "        # Then calculate width and height as differences of normalized coordinates\n",
    "        w = x2 - x1  # width is right_x - left_x\n",
    "        h = y2 - y1  # height is bottom_y - top_y\n",
    "        \n",
    "        detection = fo.Detection(\n",
    "            label=class_name,\n",
    "            bounding_box=[x1, y1, w, h],\n",
    "            confidence=float(score)\n",
    "        )\n",
    "        detections.append(detection)\n",
    "    \n",
    "    all_detections.append(fo.Detections(detections=detections))\n",
    "\n",
    "dataset.set_values(\"omdet_predictions\", all_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "• Learn more about our [integration with Hugging Face](https://beta-docs.voxel51.com/integrations/huggingface/)\n",
    "\n",
    "• Check out the [Zero-Shot Detection Plugin](https://github.com/jacobmarks/zero-shot-prediction-plugin)\n",
    "\n",
    "• Learn more about [adding object detections to a Dataset](https://beta-docs.voxel51.com/how_do_i/recipes/adding_detections/)\n",
    "\n",
    "• Learn how to [evaluate object detections with FiftyOne](https://beta-docs.voxel51.com/tutorials/evaluate_detections/)\n",
    "\n",
    "• Learn more in our blog, [_Zero-Shot Image Classification with Multimodal Models and FiftyOne_](https://beta-docs.voxel51.com/tutorials/zero_shot_classification/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
