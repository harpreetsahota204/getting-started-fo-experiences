{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who this is for\n",
    "\n",
    "This tutorial is designed for machine learning practitioners who:\n",
    "- Have basic familiarity with FiftyOne (used it at least once before)\n",
    "- Are interested in exploring zero-shot object detection without training models\n",
    "- Want to quickly test different zero-shot detection models on their datasets\n",
    "\n",
    "## Assumed Knowledge\n",
    "\n",
    "**Computer Vision Concepts:**\n",
    "- Understanding of object detection and bounding boxes\n",
    "- Familiarity with confidence scores and model predictions\n",
    "- Basic knowledge of zero-shot learning concepts\n",
    "\n",
    "**Technical Requirements:**\n",
    "- Intermediate Python programming skills\n",
    "- Experience with PyTorch and/or Hugging Face\n",
    "- Ability to work with image datasets and common formats (jpg, png)\n",
    "\n",
    "**FiftyOne Concepts:**\n",
    "You should be familiar with:\n",
    "- [Datasets and Samples](https://beta-docs.voxel51.com/getting_started/basics.html)\n",
    "- [Working with Labels](https://beta-docs.voxel51.com/user_guide/using_datasets.html#labels)\n",
    "- [Model Zoo](https://beta-docs.voxel51.com/user_guide/model_zoo/index.html)\n",
    "- [Dataset Zoo](https://beta-docs.voxel51.com/user_guide/dataset_zoo/index.html)\n",
    "\n",
    "## Time to complete\n",
    "\n",
    "Estimated time: 30-45 minutes\n",
    "- Setup: 5-10 minutes\n",
    "- Tutorial: 20-25 minutes\n",
    "- Experimentation: 10+ minutes\n",
    "\n",
    "## Required packages\n",
    "\n",
    "Make sure you have a virtual environment with FiftyOne already installed. Then install the following packages:\n",
    "\n",
    "```bash\n",
    "\n",
    "# Install required packages\n",
    "pip install fiftyone\n",
    "pip install torch torchvision\n",
    "pip install transformers<=4.49\n",
    "pip install ultralytics\n",
    "pip install pillow\n",
    "```\n",
    "\n",
    "## What's covered in this tutorial\n",
    "\n",
    "This tutorial covers:\n",
    "1. **Dataset Loading** - Loading a street scene dataset from FiftyOne's Dataset Zoo\n",
    "\n",
    "2. **Hugging Face Integration** - Using OWL-ViT for zero-shot detection through FiftyOne's Hugging Face integration\n",
    "\n",
    "3. **Ultralytics Integration** - Implementing YOLO-World for zero-shot detection\n",
    "\n",
    "4. **Plugin Usage** - Exploring the Florence2 plugin for additional zero-shot capabilities\n",
    "\n",
    "5. **Custom Implementation** - Understanding how to implement arbitrary zero-shot detection models in FiftyOne\n",
    "\n",
    "Each section builds upon the previous ones, demonstrating different approaches to zero-shot detection while highlighting FiftyOne's flexibility in working with various model frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "\n",
    "Let's load a [Dataset](https://beta-docs.voxel51.com/getting_started/basic/datasets_samples_fields/) from the FiftyOne [Dataset Zoo](https://beta-docs.voxel51.com/data/dataset_zoo/datasets/). In this tutorial, we'll use the [Quickstart Geo](https://beta-docs.voxel51.com/data/dataset_zoo/datasets/#dataset-zoo-quickstart-geo) dataset. This is a a small Dataset which consists of 500 images from the validation split of the [BDD100K dataset](https://beta-docs.voxel51.com/data/dataset_zoo/datasets/#dataset-zoo-bdd100k) in the New York City area with object detections and GPS timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\"quickstart-geo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make up a list of classes to detect. Since the Dataset we're working with is from New York City streets, we'll focus on vehicles, traffic infrastructure, and urban elements that we'd expect to see in NYC traffic scenes. \n",
    "\n",
    "This includes various car types, traffic signals, street furniture, and public transportation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_classes = [\n",
    "    \"yellow cab\",\n",
    "    \"sedan\",\n",
    "    \"coupe\",\n",
    "    \"hatchback\",\n",
    "    \"SUV\",\n",
    "    \"pickup truck\",\n",
    "    \"station wagon\",\n",
    "    \"crossover\",\n",
    "    \"minivan\",\n",
    "    \"green light\",\n",
    "    \"red light\",\n",
    "    \"illuminated tail lights\",\n",
    "    \"illuminated head lights\",\n",
    "    \"tow truck\",\n",
    "    \"parking meter\",\n",
    "    \"traffic barrier\",\n",
    "    \"traffic cone\",\n",
    "    \"bus stop\",\n",
    "    \"storefront\",\n",
    "    \"construction vehicle\",\n",
    "    \"municipal bus\",\n",
    "    \"charter bus\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo\n",
    "\n",
    "The FiftyOne Model Zoo provides a powerful interface for downloading models and applying them to your FiftyOne datasets.\n",
    "\n",
    "It provides native access to hundreds of pre-trained models, and it also supports downloading arbitrary public or private models whose definitions are provided via GitHub repositories or URLs.\n",
    "\n",
    "In fact, the [Model Zoo](https://beta-docs.voxel51.com/models/model_zoo/) is so flexible that you can natively load certain Hugging Face Transformers models and Ultralytics models for zero-shot object detection as a Zoo model via the [`load_zoo_model`](https://beta-docs.voxel51.com/api/fiftyone.zoo.models.html#load_zoo_model) method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Integration\n",
    "\n",
    "FiftyOne integrates with [Hugging Face's Transformers](https://beta-docs.voxel51.com/integrations/huggingface/#zero-shot-object-detection) library for Zero Shot Detection models. This allows you to load a Transformers model as a [Zoo Model](https://beta-docs.voxel51.com/models/model_zoo/).\n",
    "\n",
    "\n",
    "To load a model from the Hugging Face Hub, set `name_or_url=zero-shot-detection-transformer-torch`. This specifies that you want to a zero-shot object detection model from the Hugging Face Transformers library. You can then specify the model via the `name_or_path` argument. This should be the repository name or model identifier of the model you want to load.\n",
    "\n",
    "\n",
    "Note: the `confidence_thresh` parameter is optional and can be used to filter out predictions with confidence scores below the specified threshold. You may need to adjust this value based on the model and dataset you are working. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 500/500 [29.5s elapsed, 0s remaining, 17.7 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "owlvit = foz.load_zoo_model(\n",
    "    \"zero-shot-detection-transformer-torch\",\n",
    "    text_prompt=\"a photo of a \", # per the model card\n",
    "    name_or_path=\"google/owlvit-base-patch32\",  # HF model name or path\n",
    "    classes=detection_classes,\n",
    "    device=device,\n",
    "    confidence_thresh=0.1 #setting aribtrarily low threshold\n",
    "    # install_requirements=True # uncomment to install the necessary requirements\n",
    ")\n",
    "\n",
    "dataset.apply_model(\n",
    "    owlvit, \n",
    "    label_field=\"owlvit_detections\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the results on by [skipping to a random Sample](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#skip) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Detections: {\n",
       "    'detections': [\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef665',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'storefront',\n",
       "            'bounding_box': [\n",
       "                0.0014444444444444446,\n",
       "                -0.0026875,\n",
       "                0.17194444444444443,\n",
       "                0.47613281250000006,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.13197840750217438,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef666',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'storefront',\n",
       "            'bounding_box': [\n",
       "                0.13490277777777776,\n",
       "                0.26649218750000003,\n",
       "                0.10397222222222224,\n",
       "                0.19666406250000001,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.15454693138599396,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef667',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'storefront',\n",
       "            'bounding_box': [\n",
       "                0.10261111111111111,\n",
       "                0.091953125,\n",
       "                0.14166666666666666,\n",
       "                0.379328125,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.11891092360019684,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef668',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'municipal bus',\n",
       "            'bounding_box': [\n",
       "                0.5685138888888889,\n",
       "                0.31115624999999997,\n",
       "                0.15155555555555564,\n",
       "                0.1060546875,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.13214434683322906,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef669',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'SUV',\n",
       "            'bounding_box': [\n",
       "                0.4620833333333333,\n",
       "                0.3072578125,\n",
       "                0.11956944444444449,\n",
       "                0.15671093749999998,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.29390838742256165,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef66a',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'SUV',\n",
       "            'bounding_box': [\n",
       "                0.5698333333333333,\n",
       "                0.33166406249999997,\n",
       "                0.11266666666666668,\n",
       "                0.166296875,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.17897413671016693,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef66b',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'sedan',\n",
       "            'bounding_box': [\n",
       "                0.5687222222222222,\n",
       "                0.3703671875,\n",
       "                0.08163888888888884,\n",
       "                0.14845312500000002,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.2534674406051636,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef66c',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'SUV',\n",
       "            'bounding_box': [\n",
       "                0.6298055555555555,\n",
       "                0.34385937499999997,\n",
       "                0.19693055555555558,\n",
       "                0.2575,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.3990810811519623,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef66d',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'sedan',\n",
       "            'bounding_box': [\n",
       "                0.5770555555555555,\n",
       "                0.37800781250000004,\n",
       "                0.07523611111111106,\n",
       "                0.14806249999999999,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.22299253940582275,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef66e',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'sedan',\n",
       "            'bounding_box': [\n",
       "                0.5954861111111112,\n",
       "                0.38992187500000003,\n",
       "                0.06872222222222225,\n",
       "                0.15815624999999994,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.10626339912414551,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef66f',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'minivan',\n",
       "            'bounding_box': [\n",
       "                0.7587499999999999,\n",
       "                0.24249218749999998,\n",
       "                0.2410277777777779,\n",
       "                0.41490625000000003,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.3947419822216034,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef670',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'illuminated tail lights',\n",
       "            'bounding_box': [\n",
       "                0.2708333333333333,\n",
       "                0.477,\n",
       "                0.07577777777777778,\n",
       "                0.051179687500000084,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.12947803735733032,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef671',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'yellow cab',\n",
       "            'bounding_box': [\n",
       "                0.2574722222222222,\n",
       "                0.2663515625,\n",
       "                0.3298055555555555,\n",
       "                0.360625,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.2435181438922882,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef672',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'yellow cab',\n",
       "            'bounding_box': [\n",
       "                0.2605972222222222,\n",
       "                0.220921875,\n",
       "                0.32504166666666673,\n",
       "                0.4124921875,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.3468368649482727,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef673',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'illuminated tail lights',\n",
       "            'bounding_box': [\n",
       "                0.2707361111111111,\n",
       "                0.5041015625,\n",
       "                0.03811111111111111,\n",
       "                0.04823437500000001,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.14190329611301422,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef674',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'illuminated tail lights',\n",
       "            'bounding_box': [\n",
       "                0.4896388888888889,\n",
       "                0.4840703125,\n",
       "                0.063625,\n",
       "                0.037249999999999964,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.1208077222108841,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef675',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'illuminated tail lights',\n",
       "            'bounding_box': [\n",
       "                0.5343333333333333,\n",
       "                0.5111484374999999,\n",
       "                0.030236111111111085,\n",
       "                0.03962500000000002,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.16234591603279114,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef676',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'illuminated tail lights',\n",
       "            'bounding_box': [\n",
       "                0.5202361111111111,\n",
       "                0.5022734375,\n",
       "                0.05220833333333338,\n",
       "                0.061531249999999996,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.12089472264051437,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef677',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'parking meter',\n",
       "            'bounding_box': [\n",
       "                0.004472222222222223,\n",
       "                0.18684375,\n",
       "                0.9988055555555555,\n",
       "                0.534171875,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.11007577180862427,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef678',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'station wagon',\n",
       "            'bounding_box': [\n",
       "                0.008263888888888888,\n",
       "                0.1459609375,\n",
       "                0.9990833333333332,\n",
       "                0.6118125,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.12653888761997223,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef679',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'tow truck',\n",
       "            'bounding_box': [\n",
       "                0.6108055555555555,\n",
       "                0.23802343750000002,\n",
       "                0.38940277777777776,\n",
       "                0.412671875,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.1311836540699005,\n",
       "            'index': None,\n",
       "        }>,\n",
       "        <Detection: {\n",
       "            'id': '67e1dae20b9d9cbc6d0ef67a',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'station wagon',\n",
       "            'bounding_box': [\n",
       "                -0.00175,\n",
       "                0.5913906250000001,\n",
       "                0.9975972222222222,\n",
       "                0.4046484375,\n",
       "            ],\n",
       "            'mask': None,\n",
       "            'mask_path': None,\n",
       "            'confidence': 0.14577694237232208,\n",
       "            'index': None,\n",
       "        }>,\n",
       "    ],\n",
       "}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.skip(42).first()['owlvit_detections']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any model that can be run in a Hugging Face pipeline for the `zero-shot-object-detection` task can be loaded as a Zoo model.\n",
    "\n",
    "A good first entry point is to just do it and pass the model name into `name_or_path` in the [`load_zoo_model`](https://beta-docs.voxel51.com/api/fiftyone.zoo.models.html#fiftyone.zoo.models.load_zoo_model) method of the dataset. If a Hugging Face model is not compatible with the integration, you'll see an error to the effect of: \n",
    "\n",
    "```python\n",
    "ValueError: Unrecognized model in <whatever-model-name>\n",
    "```\n",
    "\n",
    "In this case, you will need to run the model manually. All this means is that you need to instantiate the model, it's  processor, and write some logic to parse the model output a [FiftyOne Detection](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html). I'll show you how to this later on in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultralytics\n",
    "\n",
    "FiftyOne [integrates natively with Ultralytics](https://beta-docs.voxel51.com/integrations/ultralytics/), so you can load, fine-tune, and run inference with your favorite Ultralytics models on your FiftyOne datasets with just a few lines of code.\n",
    "\n",
    "Check out the [documention for our Ultralytics integration](https://docs.voxel51.com/integrations/ultralytics.html#open-vocabulary-detection) if you're interested in manually using an Ultralytics model rather than as a Zoo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  72% |████████████\\----| 361/500 [10.1s elapsed, 3.8s remaining, 36.8 samples/s]    "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "yolo_world = foz.load_zoo_model(\n",
    "    \"yolov8s-world-torch\", \n",
    "    classes=detection_classes,\n",
    "    device=device,\n",
    "    confidence_thresh=0.2\n",
    "    # install_requirements=True # uncomment to install the necessary requirements\n",
    "    )\n",
    "\n",
    "dataset.apply_model(yolo_world, label_field=\"yolow_detections\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.skip(42).first()['yolow_detections']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plugins\n",
    "\n",
    "You can also run zero-shot detection via FiftyOne Plugins. The following code will show you how to use the [Florence2](https://github.com/jacobmarks/fiftyone_florence2_plugin).\n",
    "\n",
    "The example below will show you how to use the Florence2 plugin for zero-shot object detection and zero-shot open vocabulary detection. Begin by downloading the plugin and installing requirements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/jacobmarks/fiftyone_florence2_plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins requirements @jacbobmarks/florence2 --install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, instantiate the operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "MODEL_PATH =\"microsoft/Florence-2-base-ft\"\n",
    "\n",
    "florence2_detection = foo.get_operator(\"@jacobmarks/florence2/detect_with_florence2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should start a [delegated service](https://beta-docs.voxel51.com/plugins/developing_plugins/#delegated-execution_1) for this [Operator](https://beta-docs.voxel51.com/plugins/using_plugins/#calling-operators), you can do that by opening your terminal and executing the following command:\n",
    "\n",
    "```shell\n",
    "fiftyone delegated launch\n",
    "```\n",
    "\n",
    "You'll use the `await` syntax and pass the `delegate=True` argument when running this plugin via notebook. Here's how you can use the plugin for zero-shot object detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await florence2_detection(\n",
    "    dataset,\n",
    "    model_path=MODEL_PATH,\n",
    "    detection_type=\"detection\",\n",
    "    output_field=\"zero_shot_detections\",\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['zero_shot_detections']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use Florence2 for zero-shot open vocabulary detection. Note that the model only supports passing one candidate label for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await florence2_detection(\n",
    "    dataset,\n",
    "    model_path=MODEL_PATH,\n",
    "    detection_type=\"open_vocabulary_detection\",\n",
    "    text_prompt = \"pedestrain in intersection\", # the object you want to detect\n",
    "    output_field=\"open_detection\",\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['open_detection']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit the [Florence2 Plugin's GitHub Repo](https://github.com/jacobmarks/fiftyone_florence2_plugin) for more detail about using this plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbitrary Models\n",
    "\n",
    "Regardless of which zero-shot object detection model you use, the process of converting predictions to FiftyOne format follows the same general pattern:\n",
    "\n",
    "1. **Standardize Bounding Box Format**\n",
    "   - FiftyOne [Detection labels](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html) expects bounding boxes in relative coordinates [0,1]\n",
    "   - Format must be [top-left-x, top-left-y, width, height]\n",
    "   - Most models output absolute coordinates or different formats, so conversion is usually needed\n",
    "\n",
    "2. **Create Detection Objects**\n",
    "   - Each [Detection](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html) object needs three main components:\n",
    "     - `label`: the class name\n",
    "     - `bounding_box`: the normalized coordinates\n",
    "     - `confidence`: the detection score\n",
    "   - The individual Detection objects must be grouped into a [Detections](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detections.html) [Field for each Sample](https://beta-docs.voxel51.com/getting_started/basic/datasets_samples_fields/). \n",
    "\n",
    "3. **Batch Processing Strategy**\n",
    "   - Instead of updating samples one by one, collect all detections\n",
    "   - Use [`dataset.set_values()`](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#set_values) for efficient batch updates\n",
    "   - This is much faster than individual [`sample.save()`](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#save) calls\n",
    "\n",
    "The core workflow is:\n",
    "- Get model predictions\n",
    "- Convert coordinates to FiftyOne's expected format\n",
    "- Create [Detection](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html) objects\n",
    "- Group them into [Detections](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detections.html) objects (one per [Sample](https://beta-docs.voxel51.com/api/fiftyone.core.sample.Sample.html))\n",
    "- Batch update the [Dataset](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html)\n",
    "\n",
    "This pattern remains the same regardless of the model you're using, whether it's from the Hugging Face Hub, Torch Hub, or some brand new SOTA model that you can only use via it's GitHub Repo. The only part that changes is how you extract and convert the specific model's output into FiftyOne [Detection](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html) format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import fiftyone as fo\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, OmDetTurboForObjectDetection\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize model and processor\n",
    "processor = AutoProcessor.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n",
    "model = OmDetTurboForObjectDetection.from_pretrained(\n",
    "    \"omlab/omdet-turbo-swin-tiny-hf\",\n",
    "    device_map=device)\n",
    "\n",
    "filepaths = dataset.values(\"filepath\")\n",
    "\n",
    "all_detections = []\n",
    "for filepath in filepaths:\n",
    "    # Load and process image\n",
    "    image = Image.open(filepath)\n",
    "    height, width = image.size[::-1]  # Get dimensions in same format as target_sizes\n",
    "    \n",
    "    inputs = processor(image, text=detection_classes, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        text_labels=detection_classes,\n",
    "        target_sizes=[image.size[::-1]],  # Keep model's expected format\n",
    "        threshold=0.3,\n",
    "        nms_threshold=0.3,\n",
    "    )[0]\n",
    "    \n",
    "    scores = results[\"scores\"].cpu().numpy()\n",
    "    boxes = results[\"boxes\"].cpu().numpy()\n",
    "    text_labels = results[\"text_labels\"]\n",
    "    \n",
    "    detections = []\n",
    "    for score, class_name, box in zip(scores, text_labels, boxes):\n",
    "        x1, y1, x2, y2 = box\n",
    "        \n",
    "        # First normalize all coordinates by their respective dimensions (x/width, y/height)\n",
    "        x1 = x1 / width\n",
    "        y1 = y1 / height\n",
    "        x2 = x2 / width\n",
    "        y2 = y2 / height\n",
    "    \n",
    "        # Then calculate width and height as differences of normalized coordinates\n",
    "        w = x2 - x1  # width is right_x - left_x\n",
    "        h = y2 - y1  # height is bottom_y - top_y\n",
    "        \n",
    "        detection = fo.Detection(\n",
    "            label=class_name,\n",
    "            bounding_box=[x1, y1, w, h],\n",
    "            confidence=float(score)\n",
    "        )\n",
    "        detections.append(detection)\n",
    "    \n",
    "    all_detections.append(fo.Detections(detections=detections))\n",
    "\n",
    "dataset.set_values(\"omdet_predictions\", all_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial has introduced you to several approaches for performing zero-shot object detection using FiftyOne:\n",
    "\n",
    "- Using pre-trained models through the Hugging Face integration\n",
    "- Leveraging Ultralytics' YOLO-World model\n",
    "- Exploring plugin-based solutions like Florence2\n",
    "- Implementing custom zero-shot detection models\n",
    "\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To continue learning, you can:\n",
    "\n",
    "• Learn more about our [integration with Hugging Face](https://beta-docs.voxel51.com/integrations/huggingface/)\n",
    "\n",
    "• Check out the [Zero-Shot Detection Plugin](https://github.com/jacobmarks/zero-shot-prediction-plugin) and learn [more about Plugins](https://beta-docs.voxel51.com/plugins/using_plugins/) in general\n",
    "\n",
    "• Learn more about [adding object detections to a Dataset](https://beta-docs.voxel51.com/how_do_i/recipes/adding_detections/)\n",
    "\n",
    "• Use the [Moondream2 Plugin](https://github.com/harpreetsahota204/moondream2-plugin) for zero-shot object detection\n",
    "\n",
    "• Learn how to [evaluate object detections with FiftyOne](https://beta-docs.voxel51.com/tutorials/evaluate_detections/)\n",
    "\n",
    "• Learn more in our blog, [_Zero-Shot Image Classification with Multimodal Models and FiftyOne_](https://beta-docs.voxel51.com/tutorials/zero_shot_classification/)\n",
    "\n",
    "\n",
    "Remember that zero-shot detection is a rapidly evolving field - the approaches shown here are just the beginning. FiftyOne's flexible architecture allows you to easily incorporate new models and techniques as they become available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
