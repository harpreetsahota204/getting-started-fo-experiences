{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Depth Estimation: Using Depth Estimation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset\n",
    "\n",
    "Note, we've created an in-depth tutorial that discusses the methods for loading depth data into Fiftyone. As discussed in that tutorial FiftyOne's `Heatmap` class is ideal for representing depth data:\n",
    "\n",
    "```python\n",
    "fo.Heatmap(\n",
    "    map=None,           # 2D numpy array containing the data\n",
    "    map_path=None,      # OR path to the heatmap image on disk\n",
    "    range=None          # Optional [min, max] range for proper visualization\n",
    ")\n",
    "```\n",
    "\n",
    "Let's start by loading a dataset from the Hugging Face Hub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "clver_depth = load_dataset(\n",
    "    \"erkam/clevr-with-depth\",\n",
    "    split=\"train\",\n",
    "    cache_dir=\"clevr_with_depth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how this dataset is saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clver_depth[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code takes a Hugging Face dataset containing image-depth pairs and converts it into a FiftyOne dataset for visualization and analysis. \n",
    "\n",
    "For each sample, it saves the RGB image to disk (since FiftyOne requires file paths) and extracts the depth information from the first channel of the RGBA depth map. Each sample in the resulting FiftyOne dataset contains the path to the RGB image, the original prompt, and the depth map stored as a heatmap visualization. \n",
    "\n",
    "The depth values are scaled between 0 and 198, which represents the range of depth values in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1400/1400 [2.2s elapsed, 0s remaining, 626.8 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def convert_dataset_to_fiftyone(hf_dataset, save_dir=\"./clver_depth_data\"):\n",
    "    \"\"\"\n",
    "    Converts a Hugging Face dataset containing image-depth pairs into a FiftyOne dataset.\n",
    "\n",
    "    This function takes a dataset from Hugging Face that contains RGB images and their corresponding\n",
    "    depth maps, saves the images to disk, and creates a FiftyOne dataset with the images and depth\n",
    "    information stored as heatmaps.\n",
    "\n",
    "    Args:\n",
    "        hf_dataset: A Hugging Face dataset containing 'image', 'depth', and 'prompt' fields\n",
    "        save_dir (str): Directory path where images and depth maps will be saved.\n",
    "                       Defaults to \"./clver_depth_data\"\n",
    "\n",
    "    Returns:\n",
    "        fo.Dataset: A FiftyOne dataset containing:\n",
    "            - RGB images stored on disk\n",
    "            - Depth maps as FiftyOne Heatmap objects (scaled 0-198)\n",
    "            - Original prompts from the dataset\n",
    "\n",
    "    Note:\n",
    "        The depth maps are extracted from the first channel of the RGBA depth images\n",
    "        since all channels are identical in this dataset.\n",
    "    \"\"\"\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(os.path.join(save_dir, \"images\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir, \"depth\"), exist_ok=True)\n",
    "    \n",
    "    samples = []\n",
    "    # Create a FiftyOne dataset\n",
    "    dataset = fo.Dataset(\"clver_depth\", overwrite=True, persistent=True)\n",
    "    \n",
    "    for idx, item in enumerate(hf_dataset):\n",
    "        # Generate filenames\n",
    "        image_filename = f\"image_{idx:06d}.png\"\n",
    "        depth_filename = f\"depth_{idx:06d}.png\"\n",
    "        \n",
    "        image_path = os.path.join(save_dir, \"images\", image_filename)\n",
    "        depth_path = os.path.join(save_dir, \"depth\", depth_filename)\n",
    "        \n",
    "        # Save images to disk\n",
    "        item['image'].save(image_path)\n",
    "        \n",
    "        # Extract depth map from first channel (since all channels are identical in this dataset)\n",
    "        depth_np = np.array(item['depth'])[:, :, 0]  # Taking channel 0\n",
    "\n",
    "        # Create a FiftyOne sample\n",
    "        sample = fo.Sample(\n",
    "            filepath=image_path,\n",
    "            prompt=item['prompt']\n",
    "        )\n",
    "        \n",
    "        # Add depth as Heatmap with proper range\n",
    "        sample[\"depth\"] = fo.Heatmap(\n",
    "            map=depth_np,\n",
    "            range=[0, 198] # if you know the range of your dataset, use those values\n",
    "        )\n",
    "        # Add the sample to the dataset\n",
    "        samples.append(sample)\n",
    "\n",
    "    dataset.add_samples(samples)\n",
    "    dataset.compute_metadata()\n",
    "    return dataset\n",
    "# Usage:\n",
    "fo_dataset = convert_dataset_to_fiftyone(clver_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the depth map was parsed by calling the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        clver_depth\n",
       "Media type:  image\n",
       "Num samples: 1400\n",
       "Persistent:  True\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:               fiftyone.core.fields.ObjectIdField\n",
       "    filepath:         fiftyone.core.fields.StringField\n",
       "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:       fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
       "    prompt:           fiftyone.core.fields.StringField\n",
       "    depth:            fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Heatmap)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And inspect the values of the first map like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Heatmap: {\n",
       "    'id': '67cb64d2f7d612329d38ecd6',\n",
       "    'tags': [],\n",
       "    'map': array([[  0,   0,   0, ...,   0,   0,   0],\n",
       "           [  0,   0,   0, ...,   0,   0,   0],\n",
       "           [  0,   0,   0, ...,   0,   0,   0],\n",
       "           ...,\n",
       "           [191, 191, 191, ..., 191, 191, 191],\n",
       "           [191, 191, 192, ..., 192, 191, 191],\n",
       "           [192, 192, 192, ..., 192, 192, 192]], dtype=uint8),\n",
       "    'map_path': None,\n",
       "    'range': [0.0, 198.0],\n",
       "}>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo_dataset.first()['depth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to our guide for loading depth data for other examples and more detail. Once the dataset has been parsed to Fiftyone format you can launch the app and inspect it's contents\n",
    "\n",
    "```python\n",
    "fo.launch_app(fo_dataset)\n",
    "```\n",
    "\n",
    "<img src=\"/home/harpreet/workspace/getting-started-fo-experiences/depth-estimation/assets/clevr-dataset.gif\" width= \"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using depth estimation models in FiftyOne\n",
    "\n",
    "### As a Zoo model\n",
    "\n",
    "You can load `transformers` depth estimation models directly from the FiftyOne Model Zoo! \n",
    "\n",
    "To load a transformers depth estimation model from the zoo, specify `depth-estimation-transformer-torch` as the first argument, and pass in the modelâ€™s name or path as a keyword argument:\n",
    "\n",
    "```python\n",
    "model = foz.load_zoo_model(\n",
    "    \"depth-estimation-transformer-torch\",\n",
    "    name_or_path=\"path/to-model\",\n",
    ")\n",
    "```\n",
    "\n",
    "Any model that can be run in a Hugging Face pipeline for the `depth-estimation` task can be loaded as a Zoo model. A non-exhaustive list of such models includes:\n",
    "\n",
    "* [`Intel/dpt-large`](Intel/dpt-large)\n",
    "\n",
    "* [`Intel/dpt-hybrid-midas`](https://huggingface.co/Intel/dpt-hybrid-midas)\n",
    "\n",
    "* [`Intel/zoedepth-nyu-kitti`](https://huggingface.co/Intel/zoedepth-nyu-kitti)\n",
    "\n",
    "\n",
    "* [`vinvino02/glpn-kitti`](https://huggingface.co/vinvino02/glpn-kitti)\n",
    "\n",
    "* [`LiheYoung/depth-anything-small-hf`](https://huggingface.co/LiheYoung/depth-anything-small-hf)\n",
    "\n",
    "* [`depth-anything/Depth-Anything-V2-Small-hf`](https://huggingface.co/depth-anything/Depth-Anything-V2-Small-hf)\n",
    "\n",
    "* [`depth-anything/Depth-Anything-V2-Base-hf`](https://huggingface.co/depth-anything/Depth-Anything-V2-Base-hf)\n",
    "\n",
    "* [`depth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf`](https://huggingface.co/depth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf)\n",
    "\n",
    "\n",
    "Refer to the Hugging Face documentation on [*Monocular depth estimation*](https://huggingface.co/docs/transformers/tasks/monocular_depth_estimation) to stay up to date on which models can be run in a pipeline.  \n",
    "\n",
    "**Note:** When selecting a model, it's advisable to refer to it's model card and determine whether its suitable for your dataset and use case.\n",
    "\n",
    "Below is an example of using the `depth-anything/Depth-Anything-V2-Small-hf` on the dataset we parsed earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1400/1400 [36.4s elapsed, 0s remaining, 38.7 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dav2_model = foz.load_zoo_model(\n",
    "    \"depth-estimation-transformer-torch\",\n",
    "    name_or_path=\"depth-anything/Depth-Anything-V2-Small-hf\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "fo_dataset.apply_model(\n",
    "    dav2_model, \n",
    "    label_field=\"dav2_small\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Heatmap: {\n",
       "    'id': '67cb6554f7d612329d38f7c9',\n",
       "    'tags': [],\n",
       "    'map': array([[0.0884279 , 0.08933415, 0.08994453, ..., 0.11456049, 0.11414974,\n",
       "            0.1075917 ],\n",
       "           [0.08988198, 0.08931533, 0.08934323, ..., 0.11374313, 0.11474685,\n",
       "            0.1143341 ],\n",
       "           [0.08958292, 0.0893321 , 0.08897983, ..., 0.11302507, 0.11365069,\n",
       "            0.1142491 ],\n",
       "           ...,\n",
       "           [0.98185605, 0.9817354 , 0.9805786 , ..., 0.9762832 , 0.97647375,\n",
       "            0.9771001 ],\n",
       "           [0.98547536, 0.98751265, 0.9860172 , ..., 0.9807068 , 0.98140776,\n",
       "            0.98098916],\n",
       "           [0.9837377 , 0.98384845, 0.99339867, ..., 0.98687685, 0.9861701 ,\n",
       "            0.9877786 ]], dtype=float32),\n",
       "    'map_path': None,\n",
       "    'range': None,\n",
       "}>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo_dataset.first()[\"dav2_small\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face model that's not compatible with integration\n",
    "\n",
    "Admittedly, it's not always clear which Hugging Face model can be run as part of a pipeline. \n",
    "\n",
    "A good first entry point is to just do it and pass the model name into `name_or_path` in the `load_zoo_model` method of the dataset. If a Hugging Face model is not compatible with the integration, you'll see an error to the effect of: \n",
    "\n",
    "```python\n",
    "ValueError: Unrecognized model in <whatever-model-name>\n",
    "```\n",
    "\n",
    "In this case, you will need to run the model manually. All this means is that you need to instantiate the model, it's  processor, and write some logic to parse it the model output a FiftyOne Heatmap. \n",
    "\n",
    "Here's an example of how you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dpt_processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-beit-large-512\")\n",
    "\n",
    "dpt_model = DPTForDepthEstimation.from_pretrained(\n",
    "    \"Intel/dpt-beit-large-512\",\n",
    "    device_map=device\n",
    "    )\n",
    "\n",
    "dpt_model.eval()\n",
    "\n",
    "file_paths = fo_dataset.values(\"filepath\") # a list of all filepaths in Dataset\n",
    "\n",
    "dpt_depth_maps = [] # to store the depth maps\n",
    "\n",
    "for img in file_paths:\n",
    "\n",
    "    image = Image.open(img).convert(\"RGB\")\n",
    "    \n",
    "    inputs = dpt_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = dpt_model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "    \n",
    "    # interpolate to original size\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "        )\n",
    "    \n",
    "    output = prediction.squeeze().cpu().numpy()\n",
    "\n",
    "    formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "\n",
    "    fo_depth_map = fo.Heatmap(map=formatted)\n",
    "\n",
    "    dpt_depth_maps.append(fo_depth_map)\n",
    "\n",
    "\n",
    "fo_dataset.set_values(\"dpt_beit_maps\", dpt_depth_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Heatmap: {\n",
       "    'id': '67cb6ec8f7d612329d38fd41',\n",
       "    'tags': [],\n",
       "    'map': array([[  6,   6,   6, ...,   5,   5,   5],\n",
       "           [  7,   6,   6, ...,   5,   5,   5],\n",
       "           [  7,   7,   7, ...,   6,   6,   6],\n",
       "           ...,\n",
       "           [246, 246, 246, ..., 251, 251, 252],\n",
       "           [247, 247, 247, ..., 252, 252, 255],\n",
       "           [244, 248, 248, ..., 253, 252, 253]], dtype=uint8),\n",
       "    'map_path': None,\n",
       "    'range': None,\n",
       "}>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo_dataset.first()[\"dpt_beit_maps\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the logic in the code above comes directly from the [model card](https://huggingface.co/Intel/dpt-beit-large-512).\n",
    "\n",
    "The only FiftyOne specific aspects are just grabbing the filepaths for the Samples, parsing the model output as numpy arrays, loading it as a FiftyOne Heatmap, and adding it as a Field to the Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plugin\n",
    "\n",
    "The FiftyOne community contributes plugins which can make it easy to run a depth estimation model on your Dataset. For example, there is a plugin for [DepthPro](https://github.com/harpreetsahota204/depthpro-plugin).\n",
    "\n",
    "To use this plugin, download it and install the requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/depthpro-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins requirements @harpreetsahota/depth_pro_plugin --install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then instantiate the operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "depthpro = foo.get_operator(\"@harpreetsahota/depth_pro_plugin/depth_pro_estimator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to start a delegated service, which you can do by opening your terimal and executing the following command:\n",
    "\n",
    "```bash\n",
    "fiftyone delegated launch\n",
    "```\n",
    "\n",
    "And then run the plugin on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await depthpro(\n",
    "    fo_dataset,\n",
    "    depth_field=\"depthpro_map\",\n",
    "    depth_type=\"inverse\", # or \"regular\" see the plugin repo for more details\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have to call the `reload` method of the dataset if you don't see your field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo_dataset.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Heatmap: {\n",
       "    'id': '67cb741c3d77d8418052ff01',\n",
       "    'tags': [],\n",
       "    'map': array([[  0,   0,   0, ...,  12,  11,  11],\n",
       "           [  1,   1,   1, ...,  13,  13,  13],\n",
       "           [  1,   1,   2, ...,  13,  13,  13],\n",
       "           ...,\n",
       "           [246, 246, 247, ..., 251, 250, 250],\n",
       "           [247, 248, 249, ..., 252, 251, 250],\n",
       "           [246, 249, 250, ..., 252, 252, 251]], dtype=uint8),\n",
       "    'map_path': None,\n",
       "    'range': None,\n",
       "}>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo_dataset.first()[\"depthpro_map_depth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§¨ Diffusers Depth Estimation \n",
    "\n",
    "You can also use the `Diffusers` library for zero-shot prediction of heatmaps.\n",
    "\n",
    "Start by installing the library and instantiating the model, in this case we'll use [Marigold Depth model](https://huggingface.co/prs-eth/marigold-depth-v1-0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "marigold_pipe = diffusers.MarigoldDepthPipeline.from_pretrained(\n",
    "    \"prs-eth/marigold-depth-v1-0\", \n",
    "    variant=\"fp16\", \n",
    "    torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "\n",
    "marigold_pipe.set_progress_bar_config(disable=True) # disable progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model instantiated, we can iterate through the filepaths of our Dataset and run inference. This is an example of a model that outputs a `png` depth map. We'll save the depth map to disk and point to the filepath of the png via the `map_path` argument of `Heatmap`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = fo_dataset.values(\"filepath\") # a list of all filepaths in Dataset\n",
    "\n",
    "marigold_depth_maps = [] # to store the depth maps\n",
    "\n",
    "for img in file_paths:\n",
    "\n",
    "    # Create new filename with _marigold_map suffix, save wherever you want\n",
    "    base_path = os.path.splitext(img)[0]  # Remove extension\n",
    "    depth_map_path = f\"{base_path}_marigold_map.png\"\n",
    "\n",
    "    image = diffusers.utils.load_image(img)\n",
    "\n",
    "    depth_estimate = marigold_pipe(image)\n",
    "\n",
    "    depth_map = marigold_pipe.image_processor.visualize_depth(depth_estimate.prediction)\n",
    "    depth_map[0].save(depth_map_path)\n",
    "    \n",
    "    # alternatively, you can extract a 16 bit depthmap\n",
    "    # depth_16bit = marigold_pipe.image_processor.export_depth_to_16bit_png(depth_estimate.prediction)\n",
    "    \n",
    "    fo_depth_map = fo.Heatmap(map_path=depth_map_path)\n",
    "\n",
    "    marigold_depth_maps.append(fo_depth_map)\n",
    "\n",
    "fo_dataset.set_values(\"marigold_depth\", marigold_depth_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Heatmap: {\n",
       "    'id': '67cf2c80f7d612329d39083d',\n",
       "    'tags': [],\n",
       "    'map': None,\n",
       "    'map_path': '/home/harpreet/workspace/getting-started-fo-experiences/depth-estimation/clver_depth_data/images/image_000000_marigold_map.png',\n",
       "    'range': None,\n",
       "}>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo_dataset.first()['marigold_depth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's launch the FiftyOne app and inspect all the depth maps we created. \n",
    "\n",
    "```python\n",
    "fo.launch_app(fo_dataset)\n",
    "```\n",
    "\n",
    "<img src=\"assets/all_predicted_depths.gif\" width=\"70%\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
