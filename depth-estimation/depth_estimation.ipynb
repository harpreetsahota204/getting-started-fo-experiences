{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Depth Estimation: Using Depth Estimation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset\n",
    "\n",
    "Note, we've created an indepth tutorial that discusses the methods for loading depth data into Fiftyone. As discussed i in that tutorial FiftyOne's `Heatmap` class is ideal for representing depth data:\n",
    "\n",
    "```python\n",
    "fo.Heatmap(\n",
    "    map=None,           # 2D numpy array containing the data\n",
    "    map_path=None,      # OR path to the heatmap image on disk\n",
    "    range=None          # Optional [min, max] range for proper visualization\n",
    ")\n",
    "```\n",
    "\n",
    "Let's start by loading a dataset from the Hugging Face Hub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "clver_depth = load_dataset(\n",
    "    \"erkam/clevr-with-depth\",\n",
    "    split=\"train\",\n",
    "    cache_dir=\"clevr_with_depth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how this dataset is saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clver_depth[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code takes a Hugging Face dataset containing image-depth pairs and converts it into a FiftyOne dataset for visualization and analysis. \n",
    "\n",
    "For each sample, it saves the RGB image to disk (since FiftyOne requires file paths) and extracts the depth information from the first channel of the RGBA depth map. Each sample in the resulting FiftyOne dataset contains the path to the RGB image, the original prompt, and the depth map stored as a heatmap visualization. \n",
    "\n",
    "The depth values are scaled between 0 and 198, which represents the range of depth values in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1400/1400 [2.2s elapsed, 0s remaining, 626.8 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def convert_dataset_to_fiftyone(hf_dataset, save_dir=\"./clver_depth_data\"):\n",
    "    \"\"\"\n",
    "    Converts a Hugging Face dataset containing image-depth pairs into a FiftyOne dataset.\n",
    "\n",
    "    This function takes a dataset from Hugging Face that contains RGB images and their corresponding\n",
    "    depth maps, saves the images to disk, and creates a FiftyOne dataset with the images and depth\n",
    "    information stored as heatmaps.\n",
    "\n",
    "    Args:\n",
    "        hf_dataset: A Hugging Face dataset containing 'image', 'depth', and 'prompt' fields\n",
    "        save_dir (str): Directory path where images and depth maps will be saved.\n",
    "                       Defaults to \"./clver_depth_data\"\n",
    "\n",
    "    Returns:\n",
    "        fo.Dataset: A FiftyOne dataset containing:\n",
    "            - RGB images stored on disk\n",
    "            - Depth maps as FiftyOne Heatmap objects (scaled 0-198)\n",
    "            - Original prompts from the dataset\n",
    "\n",
    "    Note:\n",
    "        The depth maps are extracted from the first channel of the RGBA depth images\n",
    "        since all channels are identical in this dataset.\n",
    "    \"\"\"\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(os.path.join(save_dir, \"images\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir, \"depth\"), exist_ok=True)\n",
    "    \n",
    "    samples = []\n",
    "    # Create a FiftyOne dataset\n",
    "    dataset = fo.Dataset(\"clver_depth\", overwrite=True, persistent=True)\n",
    "    \n",
    "    for idx, item in enumerate(hf_dataset):\n",
    "        # Generate filenames\n",
    "        image_filename = f\"image_{idx:06d}.png\"\n",
    "        depth_filename = f\"depth_{idx:06d}.png\"\n",
    "        \n",
    "        image_path = os.path.join(save_dir, \"images\", image_filename)\n",
    "        depth_path = os.path.join(save_dir, \"depth\", depth_filename)\n",
    "        \n",
    "        # Save images to disk\n",
    "        item['image'].save(image_path)\n",
    "        \n",
    "        # Extract depth map from first channel (since all channels are identical in this dataset)\n",
    "        depth_np = np.array(item['depth'])[:, :, 0]  # Taking channel 0\n",
    "\n",
    "        # Create a FiftyOne sample\n",
    "        sample = fo.Sample(\n",
    "            filepath=image_path,\n",
    "            prompt=item['prompt']\n",
    "        )\n",
    "        \n",
    "        # Add depth as Heatmap with proper range\n",
    "        sample[\"depth\"] = fo.Heatmap(\n",
    "            map=depth_np,\n",
    "            range=[0, 198] # if you know the range of your dataset, use those values\n",
    "        )\n",
    "        # Add the sample to the dataset\n",
    "        samples.append(sample)\n",
    "        \n",
    "    dataset.add_samples(samples)\n",
    "    return dataset\n",
    "# Usage:\n",
    "fo_dataset = convert_dataset_to_fiftyone(clver_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=82d07ca6-3f03-4803-9783-3a8301f8a0ef\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7151c6d93210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset:          clver_depth\n",
       "Media type:       image\n",
       "Num samples:      1400\n",
       "Selected samples: 0\n",
       "Selected labels:  0\n",
       "Session URL:      http://localhost:5151/"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo.launch_app(fo_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the depth map was parsed by calling the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        clver_depth\n",
       "Media type:  image\n",
       "Num samples: 1400\n",
       "Persistent:  True\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:               fiftyone.core.fields.ObjectIdField\n",
       "    filepath:         fiftyone.core.fields.StringField\n",
       "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:       fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
       "    prompt:           fiftyone.core.fields.StringField\n",
       "    depth:            fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Heatmap)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And inspect the values of the first map like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Heatmap: {\n",
       "    'id': '67cb64d2f7d612329d38ecd6',\n",
       "    'tags': [],\n",
       "    'map': array([[  0,   0,   0, ...,   0,   0,   0],\n",
       "           [  0,   0,   0, ...,   0,   0,   0],\n",
       "           [  0,   0,   0, ...,   0,   0,   0],\n",
       "           ...,\n",
       "           [191, 191, 191, ..., 191, 191, 191],\n",
       "           [191, 191, 192, ..., 192, 191, 191],\n",
       "           [192, 192, 192, ..., 192, 192, 192]], dtype=uint8),\n",
       "    'map_path': None,\n",
       "    'range': [0.0, 198.0],\n",
       "}>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo_dataset.first()['depth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to our guide for loading depth data for other examples and more detail. Once the dataset has been parsed to Fiftyone format you can launch the app and inspect it's contents\n",
    "\n",
    "```python\n",
    "fo.launch_app(fo_dataset)\n",
    "```\n",
    "\n",
    "<img src=\"/home/harpreet/workspace/getting-started-fo-experiences/depth-estimation/assets/clevr-dataset.gif\" width= \"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using depth estimation models in FiftyOne\n",
    "\n",
    "### As a Zoo model\n",
    "\n",
    "You can load `transformers` depth estimation models directly from the FiftyOne Model Zoo! \n",
    "\n",
    "To load a transformers depth estimation model from the zoo, specify `depth-estimation-transformer-torch` as the first argument, and pass in the model’s name or path as a keyword argument:\n",
    "\n",
    "```python\n",
    "model = foz.load_zoo_model(\n",
    "    \"depth-estimation-transformer-torch\",\n",
    "    name_or_path=\"path/to-model\",\n",
    ")\n",
    "```\n",
    "\n",
    "Any model that can be run in a Hugging Face pipeline for the `depth-estimation` task can be loaded as a Zoo model. A non-exhaustive list of such models includes:\n",
    "\n",
    "* [`Intel/dpt-large`](Intel/dpt-large) \n",
    "\n",
    "* [`Intel/dpt-hybrid-midas`](https://huggingface.co/Intel/dpt-hybrid-midas)\n",
    "\n",
    "* [`vinvino02/glpn-kitti`](https://huggingface.co/vinvino02/glpn-kitti)\n",
    "\n",
    "* [`LiheYoung/depth-anything-small-hf`](https://huggingface.co/LiheYoung/depth-anything-small-hf)\n",
    "\n",
    "* [`depth-anything/Depth-Anything-V2-Small-hf`](https://huggingface.co/depth-anything/Depth-Anything-V2-Small-hf)\n",
    "\n",
    "* [`Intel/zoedepth-nyu-kitti`](https://huggingface.co/Intel/zoedepth-nyu-kitti)\n",
    "\n",
    "Refer to the Hugging Face documentation on [*Monocular depth estimation*](https://huggingface.co/docs/transformers/tasks/monocular_depth_estimation) to stay up to date on which models can be run in a pipeline.  \n",
    "\n",
    "**Note:** When selecting a model, it's advisable to refer to it's model card and determine whether its suitable for your dataset and use case.\n",
    "\n",
    "Below is an example of using the `depth-anything/Depth-Anything-V2-Small-hf` on the dataset we parsed earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  62% |█████████------|  868/1400 [22.6s elapsed, 13.8s remaining, 38.8 samples/s]   "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dav2_model = foz.load_zoo_model(\n",
    "    \"depth-estimation-transformer-torch\",\n",
    "    name_or_path=\"depth-anything/Depth-Anything-V2-Small-hf\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "fo_dataset.apply_model(\n",
    "    dav2_model, \n",
    "    label_field=\"dav2_small\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo_dataset.first()[\"dav2_small\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "model = foz.load_zoo_model(\n",
    "    \"depth-estimation-transformer-torch\",\n",
    "    name_or_path=\"Intel/dpt-hybrid-midas\",\n",
    ")\n",
    "\n",
    "dataset.apply_model(model, label_field=\"dpt_hybrid_midas\")\n",
    "\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face model that's not compatible with integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plugin\n",
    "\n",
    "https://github.com/harpreetsahota204/depthpro-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiftyone plugins download https://github.com/harpreetsahota204/depthpro-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiftyone plugins requirements @harpreetsahota/depth_pro_plugin --install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "depthpro = foo.get_operator(\"@harpreetsahota/depth_pro_plugin/depth_pro_estimator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compute the depth map directly through the FiftyOne App:\n",
    "\n",
    "Launch the FiftyOne App with your dataset\n",
    "Open the \"Operators Browser\" by clicking on the Operator Browser icon above the sample grid or by typing backtick (`)\n",
    "Type \"depth_pro_estimator\"\n",
    "Configure the following parameters:\n",
    "Depth Type: Choose between:\n",
    "inverse - Reciprocal of depth (1/distance)\n",
    "regular - Direct physical distance measurement in meters.\n",
    "Field Name: Enter the name for the heatmap field (e.g., \"depth_map\")\n",
    "Click \"Execute\" to compute depth estimation for your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await depthpro(\n",
    "    dataset,\n",
    "    depth_field=\"depth_map\",\n",
    "    depth_type=\"inverse\",\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusers Depth Estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers\n",
    "import torch\n",
    "\n",
    "pipe = diffusers.MarigoldDepthPipeline.from_pretrained(\n",
    "    \"prs-eth/marigold-depth-lcm-v1-0\", variant=\"fp16\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "image = diffusers.utils.load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n",
    "depth = pipe(image)\n",
    "\n",
    "vis = pipe.image_processor.visualize_depth(depth.prediction)\n",
    "vis[0].save(\"einstein_depth.png\")\n",
    "\n",
    "depth_16bit = pipe.image_processor.export_depth_to_16bit_png(depth.prediction)\n",
    "depth_16bit[0].save(\"einstein_depth_16bit.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrary Depth Estimation Model\n",
    "\n",
    "https://github.com/DepthAnything/Depth-Anything-V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
